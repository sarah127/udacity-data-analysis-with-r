{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarah127/udacity-data-analysis-with-r/blob/master/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgzROkNIH4do",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAV8exRXypkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanizeii\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh5NCoc8fsSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1R6gDJMND6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP8zG65mYk86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD03caqDhFhA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "from  pyspark import SparkConf\n",
        "#conf = SparkConf().setMaster(\"local\").setAppName(\"My app\").set(\"spark.executor.memory\", \"1g\")\n",
        "#sc = SparkContext()\n",
        "# ===============================read from csv==============================================\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "from pyspark.sql.types import StructType,StructField,StringType,TimestampType,IntegerType,DoubleType,LongType,FloatType\n",
        "Df5_Schema = StructType([StructField(\"Airline\", StringType(),True),StructField(\"Flight\", DoubleType()),\n",
        "StructField(\"AirportFrom\",StringType()),StructField(\"AirportTo\", StringType()),StructField(\"DayOfWeek\", IntegerType()),\n",
        "StructField(\"Time\",IntegerType()), StructField(\"Length\", IntegerType()),StructField(\"codrna_X1\",DoubleType()),\n",
        "StructField(\"codrna_X2\",DoubleType()),StructField(\"codrna_X3\",DoubleType()),StructField(\"codrna_X4\", DoubleType()),\n",
        "StructField(\"codrna_X5\", DoubleType()),StructField(\"codrna_X6\", DoubleType()),StructField(\"codrna_X7\", DoubleType()),\n",
        "StructField(\"codrna_X8\", DoubleType()),StructField(\"age\", IntegerType()),StructField(\"workclass\", StringType()),\n",
        "StructField(\"fnlwgt\", IntegerType()),StructField(\"education\", StringType()),StructField(\"education-num\", IntegerType()),\n",
        "StructField(\"marital-status\", StringType()),StructField(\"occupation\", StringType()),StructField(\"relationship\", StringType()),\n",
        "StructField(\"race\", StringType()),StructField(\"sex\", StringType()),StructField(\"capitalgain\", IntegerType()),\n",
        "StructField(\"capitalloss\", IntegerType()),StructField(\"hoursperweek\", IntegerType()),StructField(\"native-country\", StringType()),\n",
        "StructField(\"Delay\", IntegerType())])\n",
        "CSV_2007= '/content/drive/My Drive/AirlinesCodrnaAdult.csv'\n",
        "df5 =spark.read.schema(Df5_Schema).option(\"header\",\"true\").option(\"mode\", \"DROPMALFORMED\").csv(CSV_2007)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQvods40rPBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "names = [f.name for f in df5.schema.fields]\n",
        "dataTypes = [f.dataType for f in df5.schema.fields]\n",
        "ls=list(zip(names,dataTypes))\n",
        "str_cols=[]\n",
        "for i in range(len(ls)):\n",
        "     if type(ls[i][1])==StringType and len(df5.select(df5[ls[i][0]]).distinct().collect())<=20:\n",
        "         str_cols.append([i,ls[i][0]]) \n",
        "strings=[]\n",
        "for i in range(len(str_cols)):\n",
        "     strings.append(str_cols[i][1])  \n",
        "\n",
        "#the process of convertiong catogerical columns into numeric is not included in the machine learning operation \n",
        "#it will be left for future improvements !!!ان شاء الله      \n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df5) for column in strings]\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "df6 = pipeline.fit(df5).transform(df5)\n",
        "df6=df6.drop(*strings)\n",
        "df6_pd=df6.toPandas()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIM7xkiDsRJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install category_encoders"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB96hBgnrflo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import f1_score\n",
        "import category_encoders as ce\n",
        "from sklearn.utils import resample\n",
        "\n",
        "encoder_list = [ce.backward_difference.BackwardDifferenceEncoder,ce.basen.BaseNEncoder,ce.binary.BinaryEncoder,\n",
        "                ce.cat_boost.CatBoostEncoder,ce.helmert.HelmertEncoder,\n",
        "                ce.james_stein.JamesSteinEncoder,ce.one_hot.OneHotEncoder,ce.leave_one_out.LeaveOneOutEncoder,\n",
        "                ce.m_estimate.MEstimateEncoder,ce.ordinal.OrdinalEncoder,\n",
        "                ce.sum_coding.SumEncoder,ce.target_encoder.TargetEncoder,ce.woe.WOEEncoder]\n",
        "#numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "#categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X = df6_pd.drop('Delay', axis=1)\n",
        "y = df6_pd['Delay']\n",
        "#le = preprocessing.LabelEncoder()\n",
        "#label_encoder = le.fit(y)\n",
        "#y = label_encoder.transform(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "block_size=int(round(X_train.shape[0]/len(encoder_list)))\n",
        "n = block_size  #chunk row size\n",
        "list_X_train = [X_train[i:i+n] for i in range(0,X_train.shape[0],n)]\n",
        "list_y_train = [y_train[i:i+n] for i in range(0,y_train.shape[0],n)]\n",
        "list_X_test = [X_test[i:i+n] for i in range(0,X_test.shape[0],n)]\n",
        "list_y_test = [y_test[i:i+n] for i in range(0,y_test.shape[0],n)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "486lmPRrsNS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_numeric_dataset=[]\n",
        "intermidate=[]\n",
        "testing_numeric_dataset=[]\n",
        "for i,encoder in enumerate(encoder_list):\n",
        "  #(encoder(handle_unknown='ignore',cols=['AirportFrom','AirportTo','native-country'])).fit_transform(boot1[i])\n",
        "   training_numeric_dataset.append(encoder_list[i](cols=['AirportFrom','AirportTo','native-country']).fit(list_X_train[i],list_y_train[i]))\n",
        "   intermidate.append((training_numeric_dataset[i]).transform(list_X_train[i],list_y_train[i]))\n",
        "for i in range(len(list_X_test)):\n",
        "    for j,encoder in enumerate(encoder_list):\n",
        "        testing_numeric_dataset.append((training_numeric_dataset[j]).transform(list_X_test[i])) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KMjFk6msm2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xx=[]\n",
        "for i in range(len(encoder_list)):\n",
        "    #xx.append([i,min(intermidate[i].columns)] )\n",
        "    xx.append([encoder_list[i],len(intermidate[i].columns)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XrUFnRFs2ZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import category_encoders as ce\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "block_size=int(round(X_train2.shape[0]/12))\n",
        "n = block_size  #chunk row size\n",
        "list_X_train2 = [X_train2[i:i+n] for i in range(0,X_train2.shape[0],n)]\n",
        "list_y_train2 = [y_train2[i:i+n] for i in range(0,y_train2.shape[0],n)]\n",
        "list_X_test2 = [X_test2[i:i+n] for i in range(0,X_test2.shape[0],n)]\n",
        "list_y_test2 = [y_test2[i:i+n] for i in range(0,y_test2.shape[0],n)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3BlGKV4pypi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install unittest2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n5nuzhaSUqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import category_encoders as ce\n",
        "import doctest\n",
        "import os\n",
        "import warnings\n",
        "from datetime import timedelta\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import category_encoders.tests.helpers as th\n",
        "from sklearn.utils.estimator_checks import check_transformer_general, check_transformers_unfitted\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from unittest2 import TestSuite, TextTestRunner, TestCase  # or `from unittest import ...` if on Python 3.4+\n",
        "from copy import deepcopy\n",
        "class TestEncoders(TestCase):\n",
        "  def test_handle_missing_return_nan_train(self):\n",
        "        X = X2 \n",
        "        y = y2 \n",
        "\n",
        "        for encoder_name in (set(ce.__all__) - {'HashingEncoder'}):  # HashingEncoder supports new values by design -> excluded\n",
        "            with self.subTest(encoder_name=encoder_name):\n",
        "                enc = getattr(encoders, encoder_name)(handle_missing='return_nan')\n",
        "                result = enc.fit_transform(X, y).iloc[2, :]\n",
        "\n",
        "                if len(result) == 1:\n",
        "                    self.assertTrue(result.isnull().all())\n",
        "                else:\n",
        "                    self.assertTrue(result[1:].isnull().all())\n",
        "\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YII4QCuC8vL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install packyou"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkW0XBx_9X1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import category_encoders as ce\n",
        "from packyou.github.category_encoders.tests import test_encoders as tr\n",
        "\n",
        "aa=tr.TestEncoders.test_handle_unknown_return_nan\n",
        "aa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKoLkxiSj0oC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for encoder_name in (set(ce.__all__)):\n",
        "  print(encoder_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8tdFVyTqhaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import tests as tt\n",
        "import category_encoders.tests.helpers as th\n",
        "th.verify_numeric()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pe2dD4wB0LLh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(ce.BackwardDifferenceEncoder()),ce.__all__[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az1iDyv80fCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr5MaAHkvxv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import category_encoders as ce\n",
        "names_encoders=[]\n",
        "for i, _ in enumerate(ce.__all__):\n",
        "    names_encoders.append(str(ce.__all__[i+1])+\"encoder\")  \n",
        "\n",
        "dict = {}\n",
        "for name, Accepted_val in zip(names, AcceptedR ):\n",
        "    dict[name] =make_pipeline(Accepted_val(),fiv[AcceptedR.index(Accepted_val)]['classifier'](**fiv[AcceptedR.index(Accepted_val)]['parameters']))\n",
        "\n",
        "encoder = ce.BackwardDifferenceEncoder()\n",
        "encoder = ce.BaseNEncoder()\n",
        "encoder = ce.BinaryEncoder()\n",
        "encoder = ce.CatBoostEncoder()\n",
        "encoder = ce.HashingEncoder()\n",
        "encoder = ce.HelmertEncoder()\n",
        "encoder = ce.JamesSteinEncoder()\n",
        "encoder = ce.LeaveOneOutEncoder()\n",
        "encoder = ce.MEstimateEncoder()\n",
        "encoder = ce.OneHotEncoder()\n",
        "encoder = ce.OrdinalEncoder()\n",
        "encoder = ce.SumEncoder()\n",
        "encoder = ce.PolynomialEncoder()\n",
        "encoder = ce.TargetEncoder()\n",
        "encoder = ce.WOEEncoder()\n",
        "\n",
        "encoder.fit(X, y)\n",
        "X_cleaned = encoder.transform(X_dirty)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAV2AZs2tE00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import category_encoders as ce\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from heapq import nlargest\n",
        "\n",
        "encoder_list = [ce.backward_difference.BackwardDifferenceEncoder(),ce.basen.BaseNEncoder(),\n",
        "                ce.binary.BinaryEncoder(),ce.cat_boost.CatBoostEncoder(),\n",
        "                ce.helmert.HelmertEncoder(),ce.james_stein.JamesSteinEncoder(),\n",
        "                ce.one_hot.OneHotEncoder(),ce.leave_one_out.LeaveOneOutEncoder()\n",
        "                ,ce.m_estimate.MEstimateEncoder(),ce.ordinal.OrdinalEncoder(),ce.sum_coding.SumEncoder(),\n",
        "                ce.target_encoder.TargetEncoder(),ce.woe.WOEEncoder()]\n",
        "my_dict={}\n",
        "for encoder in encoder_list:\n",
        "    model = Pipeline(steps=[('preprocessor', encoder),('classifier',RandomForestClassifier())])               \n",
        "    modelOpt = model.fit(X_train, y_train)\n",
        "    y_true,y_pred =y_test, model.predict(X_test)\n",
        "    clf2_val_score = roc_auc_score(y_test, modelOpt.predict_proba(X_test)[:, 1])\n",
        "    my_dict[type(encoder)]= [modelOpt.score(X_train, y_train),clf2_val_score]\n",
        "\n",
        "num=int(round(len(encoder_list)*0.5))    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZZ84ICV1Nu9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gg\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGPSXZo5Nftf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNP-3BVuIy8l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z={}\n",
        "for i in range(len(my_dict)):\n",
        "   z[list(my_dict.keys())[i]]=list(my_dict.items())[i][1][0] \n",
        "Highest1 = nlargest(num, z, key = z.get)  \n",
        "\n",
        "zz={}\n",
        "for i in range(len(my_dict)):\n",
        "   zz[list(my_dict.keys())[i]]=list(my_dict.items())[i][1][1] \n",
        "Highest2 = nlargest(num, zz, key = zz.get)  \n",
        "Highest3=list(set(Highest1+Highest2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBykn8gEJFqf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Highest3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_4erO4hGjKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ww=list(zz.keys())\n",
        "rr=[]\n",
        "for i in range(len(ww)):\n",
        "  rr.append(ww[i].__name__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbPC2NsLI7TC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN3Vj_8VJDhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Accepted=[]\n",
        "keys = my_dict.keys()\n",
        "for i,each in enumerate(xx):\n",
        " for j,val in enumerate(Highest3): \n",
        "   if xx[i][0]==val and  xx[i][1]==len(X_train.columns)  : \n",
        "           #  Accepted.append(\"%s :%s: %s\" % (i,type(each), my_dict.get(each))) \n",
        "           Accepted.append((i,val))\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20rpUs7-EfrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Accepted      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AWt8EE1Jl76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "pipe1 = make_pipeline(Accepted[0][1](),RandomForestClassifier())\n",
        "pipe2 = make_pipeline(Accepted[1][1](), KNeighborsClassifier())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2], meta_classifier=LogisticRegression())\n",
        "\n",
        "model2=sclf2.fit(X_train, y_train)\n",
        "y_true3, y_pred3  =y_test, model2.predict(X_test)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train, y_train))\n",
        "clf2_val_score = roc_auc_score(y_test, sclf2.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P6auG6ahBf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier())\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier())\n",
        "pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier())\n",
        "pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "pipe5 = make_pipeline(Accepted[4][1](),AdaBoostClassifier())\n",
        "pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe5,pipe6], meta_classifier=LogisticRegression())\n",
        "\n",
        "model2=sclf2.fit(X_train, y_train)\n",
        "y_true3, y_pred3  =y_test, model2.predict(X_test)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train, y_train))\n",
        "clf2_val_score = roc_auc_score(y_test, sclf2.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZZsULnlhHEM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier())\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier())\n",
        "pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier())\n",
        "pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "pipe5 = make_pipeline(Accepted[4][1](),AdaBoostClassifier())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4], meta_classifier=LogisticRegression())\n",
        "\n",
        "model2=sclf2.fit(X_train, y_train)\n",
        "y_true3, y_pred3  =y_test, model2.predict(X_test)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train, y_train))\n",
        "clf2_val_score = roc_auc_score(y_test, sclf2.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))\n",
        "res4=[]\n",
        "res4.append([model2.score(X_train, y_train),clf2_val_score])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAC_6RpSykOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuYcc4nnZkp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from hyperopt import fmin, hp, tpe, Trials, space_eval\n",
        "from hyperopt.pyll import scope as ho_scope\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.utils import shuffle\n",
        "clf1 = f_clf1(space_eval(param_hyperopt, best_clf3)).fit(X_train, y_train)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf1_val_score = roc_auc_score(y_test, clf1.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf1_val_score))\n",
        "print('Best parameters:')\n",
        "print(space_eval(param_hyperopt, best_clf3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyBAgEvxakfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_true3, y_pred3  =y_test, model2.predict(X_test)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"RandomForestClassifier score: %f\" % clf1.score(X_train, y_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GoT4rgyqbNPr",
        "colab": {}
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "import warnings\n",
        "pipe1 = make_pipeline(Accepted[0][1](),RandomForestClassifier(n_estimators= 21,\n",
        "                max_depth= 7,min_samples_split= 4,min_samples_leaf= 4,max_features= 'sqrt'))\n",
        "pipe2 = make_pipeline(Accepted[1][1](),AdaBoostClassifier(DecisionTreeClassifier(max_depth=1)\n",
        ",n_estimators=28,learning_rate=0.8051561754791255)) \n",
        "pipe3 = make_pipeline(Accepted[2][1](),XGBClassifier())\n",
        "pipe4 = make_pipeline(Accepted[3][1](),ExtraTreesClassifier(criterion= 'entropy'))\n",
        "pipe5 = make_pipeline(Accepted[4][1](),GradientBoostingClassifier())\n",
        "pipe6 = make_pipeline(Accepted[5][1](),GaussianNB())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe5,pipe6],\n",
        "                           meta_classifier=LogisticRegression())\n",
        "\n",
        "model2=sclf2.fit(X_train, y_train)\n",
        "y_true3, y_pred3  =y_test, model2.predict(X_test)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"RandomForestClassifier score: %f\" % model2.score(X_train, y_train))\n",
        "clf2_val_score = roc_auc_score(y_test, sclf2.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APRcNVp5qCWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive \n",
        "from google.colab import auth \n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7jAi0SyqJ6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGO4odcgVfWt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compare Algorithms\n",
        "import pandas\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis,QuadraticDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.pipeline import Pipeline \n",
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "# explicitly require this experimental feature\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
        "# now you can import normally from ensemble\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import xgboost\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import lightgbm as lgb\n",
        "# load dataset\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "#le = preprocessing.LabelEncoder()\n",
        "#label_encoder = le.fit(y)\n",
        "#y = label_encoder.transform(y)\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2)\n",
        "block_size=int(round(X_train2.shape[0]/13))\n",
        "n = block_size  #chunk row size\n",
        "list_X_train = [X_train2[i:i+n] for i in range(0,X_train2.shape[0],n)]\n",
        "list_y_train = [y_train2[i:i+n] for i in range(0,y_train2.shape[0],n)]\n",
        "list_X_test = [X_test2[i:i+n] for i in range(0,X_test2.shape[0],n)]\n",
        "list_y_test = [y_test2[i:i+n] for i in range(0,y_test2.shape[0],n)]\n",
        "test_data = lgb.Dataset(list_X_test[0], label=list_y_test[0])\n",
        "# Build the encoder\n",
        "# prepare configuration for cross validation test harness\n",
        "seed = 7\n",
        "# prepare models\n",
        "models = []\n",
        "parameters = {\n",
        "    'application': 'binary','objective': 'binary','metric': 'auc','is_unbalance': 'true',\n",
        "    'boosting': 'gbdt','num_leaves': 31,'feature_fraction': 0.5,'bagging_fraction': 0.5,\n",
        "    'bagging_freq': 20,'learning_rate': 0.05,'verbose': 0\n",
        "}\n",
        "fit_params={\"early_stopping_rounds\":10,\"eval_metric\" : 'auc',\"eval_set\" : [(X_test,y_test)],\n",
        "            'eval_names': ['valid'],'verbose': 100,\n",
        "            'feature_name': 'auto', # that's actually the default\n",
        "            'categorical_feature': 'auto' # that's actually the default\n",
        "           }\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                               ('RandomForestClassifier',RandomForestClassifier(n_estimators=20))])) \n",
        "                               .fit(list_X_train[0], list_y_train[0])) \n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                               ('AdaBoostClassifier',AdaBoostClassifier(n_estimators=25))]))\n",
        "                               .fit(list_X_train[1], list_y_train[1])) \n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                       ('HistGradientBoostingClassifier', HistGradientBoostingClassifier())])).fit(list_X_train[2], list_y_train[2]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                              ('XGBClassifier',XGBClassifier(max_depth=2,n_estimators=10,objective='binary:logistic'))]))\n",
        "                            .fit(list_X_train[3], list_y_train[3]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                              ('ExtraTreesClassifier',ExtraTreesClassifier(criterion= 'entropy'))]))\n",
        "                               .fit(list_X_train[4], list_y_train[4]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "              ('lightgbm',lgb.LGBMClassifier(n_estimators=5 , num_leaves= 15,\n",
        "           max_depth=-1,colsample_bytree=0.9,subsample=0.9,learning_rate=0.1))])).fit(list_X_train[5], list_y_train[5]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                     ('LDA', LinearDiscriminantAnalysis())])).fit(list_X_train[6], list_y_train[6]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                     ('MLPClassifier',\n",
        "                      MLPClassifier(hidden_layer_sizes=(50,30,30), max_iter=5, alpha=0.0001,\n",
        "                     solver='sgd', verbose=10,  random_state=1))])).fit(list_X_train[7], list_y_train[7]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                     ('LR', LogisticRegression())])).fit(list_X_train[8], list_y_train[8]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                     ('QDA', QuadraticDiscriminantAnalysis())])).fit(list_X_train[9], list_y_train[9]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                      ('CART', DecisionTreeClassifier())])).fit(list_X_train[10], list_y_train[10]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                       ('NB', GaussianNB())])).fit(list_X_train[11], list_y_train[11]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "   ('BaggingClassifier',BaggingClassifier( DecisionTreeClassifier(random_state=42), n_estimators=100,\n",
        "    max_samples=100, bootstrap=True, n_jobs=-1, random_state=42,oob_score = True))])).fit(list_X_train[12], list_y_train[12]))\n",
        "\n",
        "\n",
        "   \n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "for name, model in enumerate(models):\n",
        "    #kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
        "    print('1')\n",
        "    cv_results = model_selection.cross_val_score(model, list_X_train[name], \n",
        "                            list_y_train[name], cv=5, scoring=scoring)\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "    print(msg)\n",
        "# boxplot algorithm comparison\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Algorithm Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CseDCsH_WonM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ff=[]\n",
        "for i in range(len(results)):\n",
        "  ff.append(results[i].mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9t2n4tOLvpu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aa=list(range(0,13))\n",
        "plt.figure(figsize=(6,6))\n",
        "data = pd.DataFrame({\"xlabel_values\": gg,\n",
        "                     \"x\": aa,\n",
        "                     \"y\": ff}\n",
        "                    )\n",
        "sns.catplot(x=\"x\",  y=\"y\", data=data,kind='box' , hue=\"xlabel_values\",height=4, aspect=2)\n",
        "ax.set(ylim=(0.50, 0.85))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiQ6AuRL_Add",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set(style=\"whitegrid\")\n",
        "gg=[]\n",
        "for i in range(len(models)):\n",
        "  gg.append(type(models[i][1]).__name__)\n",
        "aa=list(range(0,13))\n",
        "plt.figure(figsize=(9,8))\n",
        "data = pd.DataFrame({\"xlabel_values\": gg,\"x\": aa,\"y\": ff} )\n",
        "ax = sns.boxplot(x=\"x\",y= \"y\",hue=\"xlabel_values\",data=data)\n",
        "ax = sns.boxplot(data=results)\n",
        "aa=list(range(0,13))\n",
        "#sns.utils.axlabel(xlabel=aa,hue=gg, ylabel=aa, fontsize=9)\n",
        "#ax.set_xticklabels(ax.get_xticklabels(),rotation=45)\n",
        "ax.set(xlabel='classifier', ylabel='Acracy', title='Initial Perfered Classifiers')\n",
        "ax.set(ylim=(0.50, 0.85))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs_JuIdQzwIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compare Algorithms\n",
        "import pandas\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis,QuadraticDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.pipeline import Pipeline \n",
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "# explicitly require this experimental feature\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
        "# now you can import normally from ensemble\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import xgboost\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import lightgbm as lgb\n",
        "# load dataset\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "#le = preprocessing.LabelEncoder()\n",
        "#label_encoder = le.fit(y)\n",
        "#y = label_encoder.transform(y)\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2)\n",
        "block_size=int(round(X_train2.shape[0]/13))\n",
        "n = block_size  #chunk row size\n",
        "list_X_train = [X_train2[i:i+n] for i in range(0,X_train2.shape[0],n)]\n",
        "list_y_train = [y_train2[i:i+n] for i in range(0,y_train2.shape[0],n)]\n",
        "list_X_test = [X_test2[i:i+n] for i in range(0,X_test2.shape[0],n)]\n",
        "list_y_test = [y_test2[i:i+n] for i in range(0,y_test2.shape[0],n)]\n",
        "test_data = lgb.Dataset(list_X_test[0], label=list_y_test[0])\n",
        "# Build the encoder\n",
        "# prepare configuration for cross validation test harness\n",
        "seed = 7\n",
        "# prepare models\n",
        "models = []\n",
        "parameters = {\n",
        "    'application': 'binary','objective': 'binary','metric': 'auc','is_unbalance': 'true',\n",
        "    'boosting': 'gbdt','num_leaves': 31,'feature_fraction': 0.5,'bagging_fraction': 0.5,\n",
        "    'bagging_freq': 20,'learning_rate': 0.05,'verbose': 0\n",
        "}\n",
        "fit_params={\"early_stopping_rounds\":10,\"eval_metric\" : 'auc',\"eval_set\" : [(X_test,y_test)],\n",
        "            'eval_names': ['valid'],'verbose': 100,\n",
        "            'feature_name': 'auto', # that's actually the default\n",
        "            'categorical_feature': 'auto' # that's actually the default\n",
        "           }\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                               ('RandomForestClassifier',RandomForestClassifier(n_estimators=20))])) \n",
        "                               .fit(list_X_train[0], list_y_train[0])) \n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                               ('AdaBoostClassifier',AdaBoostClassifier(n_estimators=25))]))\n",
        "                               .fit(list_X_train[1], list_y_train[1])) \n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                       ('HistGradientBoostingClassifier', HistGradientBoostingClassifier())])).fit(list_X_train[2], list_y_train[2]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                              ('XGBClassifier',XGBClassifier(max_depth=2,n_estimators=10,objective='binary:logistic'))]))\n",
        "                            .fit(list_X_train[3], list_y_train[3]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                              ('ExtraTreesClassifier',ExtraTreesClassifier(criterion= 'entropy'))]))\n",
        "                               .fit(list_X_train[4], list_y_train[4]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "              ('lightgbm',lgb.LGBMClassifier(n_estimators=5 , num_leaves= 15,\n",
        "           max_depth=-1,colsample_bytree=0.9,subsample=0.9,learning_rate=0.1))])).fit(list_X_train[5], list_y_train[5]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                     ('LDA', LinearDiscriminantAnalysis())])).fit(list_X_train[6], list_y_train[6]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                     ('MLPClassifier',\n",
        "                      MLPClassifier(hidden_layer_sizes=(50,30,30), max_iter=5, alpha=0.0001,\n",
        "                     solver='sgd', verbose=10,  random_state=1))])).fit(list_X_train[7], list_y_train[7]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                     ('LR', LogisticRegression())])).fit(list_X_train[8], list_y_train[8]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                     ('QDA', QuadraticDiscriminantAnalysis())])).fit(list_X_train[9], list_y_train[9]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                      ('CART', DecisionTreeClassifier())])).fit(list_X_train[10], list_y_train[10]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                       ('NB', GaussianNB())])).fit(list_X_train[11], list_y_train[11]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "   ('BaggingClassifier',BaggingClassifier( DecisionTreeClassifier(random_state=42), n_estimators=100,\n",
        "    max_samples=100, bootstrap=True, n_jobs=-1, random_state=42,oob_score = True))])).fit(list_X_train[12], list_y_train[12]))\n",
        "\n",
        "\n",
        "   \n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "for name, model in enumerate(models):\n",
        "    #kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
        "    print('1')\n",
        "    cv_results = model_selection.cross_val_score(model, list_X_train[name], \n",
        "                            list_y_train[name], cv=5, scoring=scoring)\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "    print(msg)\n",
        "# boxplot algorithm comparison\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Algorithm Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGSR_9eGDZB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "last=[]\n",
        "for name, model in enumerate(models):\n",
        "  last.append([type(model[1]),results[name].mean()])\n",
        "last\n",
        "def Sort(sub_li): \n",
        "   return(sorted(sub_li, key = lambda x: x[1], reverse=True))     \n",
        "bb=(Sort(last))\n",
        "try1=[]\n",
        "for i in range(len(bb)):\n",
        "  if bb[i][1]>=0.70:\n",
        "     try1.append(bb[i])\n",
        "try1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09XV0e_yx2rJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAd-n2vnR-TY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll import scope as ho_scope\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.utils import shuffle\n",
        "import lightgbm as lgb\n",
        "from functools import partial\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "param_hyperopt5= {\n",
        "  'DecisionTreeClassifier':\n",
        "            {\n",
        "            'max_depth': ho_scope.int(hp.quniform('c0_max_depth', 5, 50, 5)),\n",
        "            'max_features': hp.choice('c0_max_features', ['auto', 'sqrt', 'log2']),\n",
        "            'min_samples_split': ho_scope.int(hp.quniform('c0_min_samples_split', 5, 40,5)),\n",
        "            'min_samples_leaf': ho_scope.int(hp.quniform('c0_min_samples_leaf', 5, 40,5)),                                  \n",
        "             'criterion': hp.choice('c0_criterion', ['gini', 'entropy'])\n",
        "         }  \n",
        "} \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "def f_clf5(params):\n",
        "   model1 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),('DecisionTreeClassifier', \n",
        "                                                         DecisionTreeClassifier(**f_unpack_dict(params['DecisionTreeClassifier'])))])   \n",
        "   return model1  \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf5(params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf5 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                param_hyperopt5, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "clf5 = f_clf5(space_eval(param_hyperopt5, best_clf5)).fit(X_train2, y_train2)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf5_val_score = roc_auc_score(y_test, clf5.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf5_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(\n",
        "a5=space_eval(param_hyperopt5, best_clf5)\n",
        "AA_classifier={'classifier':list(a5.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf5_val_score,'parameters':list(a5.values())[0]}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz0WsbNDCAMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from hyperopt import hp, tpe, fmin, Trials, STATUS_OK\n",
        "from sklearn import datasets\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble.forest import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline \n",
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "# explicitly require this experimental feature\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
        "# now you can import normally from ensemble\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "import numpy as np\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "import xgboost as xgb\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.utils import shuffle\n",
        "param_hyperopt= {\n",
        "        'DecisionTreeClassifier':\n",
        "            {\n",
        "            'max_depth': ho_scope.int(hp.quniform('c0_max_depth', 3, 7, 1)),\n",
        "            'max_features': hp.choice('c0_max_features', ['auto', 'sqrt', 'log2']),\n",
        "            'min_samples_split': ho_scope.int(hp.quniform('c0_min_samples_split', 3, 15,3)),\n",
        "            'min_samples_leaf': ho_scope.int(hp.quniform('c0_min_samples_leaf', 3, 15,3))                                   \n",
        "          \n",
        "         },\n",
        "        'RandomForestClassifier': \n",
        "            {\n",
        "            'max_depth': ho_scope.int(hp.quniform('c2_max_depth', 5, 30, 5)),\n",
        "            'n_estimators': ho_scope.int(hp.quniform('c2_n_estimators', 10, 50, 10)),\n",
        "            'max_features': hp.choice('c2_max_features', ['auto', 'sqrt']),\n",
        "            'criterion': hp.choice('c2_criterion', ['gini', 'entropy']),\n",
        "            'min_samples_split': ho_scope.int(hp.quniform('c2_min_samples_split', 5, 20,5)),\n",
        "            'min_samples_leaf': ho_scope.int(hp.quniform('c2_min_samples_leaf', 5, 20,5))\n",
        "                          \n",
        "         }  ,                                             \n",
        "          'HistGradientBoostingClassifier':\n",
        "            {'learning_rate': hp.loguniform('c3_learning_rate', np.log(0.01), np.log(0.1)),\n",
        "                'max_iter': ho_scope.int(hp.quniform('c3_max_iter', 10, 200, 5)),\n",
        "                'max_depth': ho_scope.int(hp.quniform('c3_max_depth', 2,14, 1)),\n",
        "                'max_leaf_nodes': ho_scope.int(hp.quniform('c3_max_leaf_nodes', 5, 35,5)),\n",
        "                'min_samples_leaf': ho_scope.int(hp.quniform('c3_min_samples_leaf', 5, 25,5)),\n",
        "                'max_bins': ho_scope.int(hp.quniform('c3_max_bins', 20, 200,20)),\n",
        "                'validation_fraction':0.1,\n",
        "                'n_iter_no_change':None,\n",
        "                 'tol':1e-07,\n",
        "                'l2_regularization':0.0 \n",
        "           },\n",
        "  'ExtraTreesClassifier':\n",
        "            {\n",
        "                'max_depth': ho_scope.int(hp.quniform('c4_max_depth', 5, 20, 5)),\n",
        "                'n_estimators': ho_scope.int(hp.quniform('c4_n_estimators', 5, 200, 5)),\n",
        "                'max_features': ho_scope.int(hp.quniform('c4_max_features', 5, 20, 5)),\n",
        "                'criterion': hp.choice('c4_criterion', ['gini', 'entropy']),\n",
        "                'min_samples_split': ho_scope.int(hp.quniform('c4_min_samples_split', 2, 20,2)),\n",
        "                'min_samples_leaf': ho_scope.int(hp.quniform('c4_min_samples_leaf', 2, 20,2))      \n",
        "             }  ,\n",
        " 'AdaBoostClassifier':\n",
        "   {\n",
        "          'base_estimator':hp.choice('base_estimator', [DecisionTreeClassifier(**AA_classifier['parameters'])]),\n",
        "          'learning_rate': hp.loguniform('c5_learning_rate', np.log(0.01), np.log(1)),\n",
        "          'n_estimators': ho_scope.int(hp.quniform('c5_n_estimators', 10, 40, 5)),\n",
        "          'algorithm': hp.choice('c5_algorithm',[\"SAMME\"]) \n",
        "       },\n",
        "       'XGBClassifier':\n",
        "    {   \n",
        "        'max_depth': ho_scope.int(hp.quniform('c2_max_depth', 5, 15, 5)),\n",
        "           'min_child_weight': hp.choice ('c2_min_child',np.arange(1, 10, dtype=int)),\n",
        "           'subsample': hp.uniform ('c2_subsample', 0.2, 1),\n",
        "           'n_estimators': ho_scope.int(hp.quniform('c2_n_estimators', 5, 45, 15)),\n",
        "          'eta':ho_scope.float(hp.quniform('c2_eta', 0.025, 0.5, 0.025)) ,\n",
        "          'gamma':ho_scope.float( hp.quniform('c2_gamma', 0.5, 1, 0.05)) ,\n",
        "        'colsample_bytree': ho_scope.float(hp.quniform('c2_colsample_bytree', 0.5, 1, 0.05)),\n",
        "         'objective': 'binary:logistic',\n",
        "          'n_iter_no_change':5\n",
        "       },\n",
        "   'LinearDiscriminantAnalysis':\n",
        "            {    }  ,\n",
        "  'QuadraticDiscriminantAnalysis' :\n",
        "        {  },\n",
        "   'LGBMClassifier':\n",
        "                  {\n",
        "                     'max_depth': ho_scope.int(hp.quniform('c6_max_depth', 2, 14, 2)),\n",
        "                   'num_leave': ho_scope.int(hp.quniform('c6_num_leaves', 5, 30, 5)),\n",
        "                   'min_child_samples': ho_scope.int(hp.quniform('c6_min_child_samples', 50, 100, 10)),\n",
        "                   'scale_pos_weight': ho_scope.float(hp.quniform('c6_scale_pos_weight', 50, 100, 10)),\n",
        "                   'subsample': ho_scope.float(hp.quniform('c6_subsample', 0.2, 0.9, 0.1)),\n",
        "                   'colsample_bytree': ho_scope.float(hp.quniform('c6_colsample_bytree', 0.2, 0.9, 0.1)),\n",
        "                   'reg_lambda': hp.choice(\"c6_reg_lambda\",[0, 1e-1, 1, 5, 10, 20, 50]),\n",
        "                   'reg_alpha': hp.choice(\"c6_reg_alpha\",[0, 1e-1, 1, 5, 10, 20, 50]) ,\n",
        "                   'max_bin': ho_scope.int(hp.quniform('c6_max_bin', 10, 200, 10)),\n",
        "                   'learning_rate': ho_scope.float(hp.quniform('c6_learning_rate', 0.001, 0.09, 0.005)),\n",
        "                   'num_iterations': ho_scope.int(hp.quniform('c6_num_iterations', 100, 2000, 100)),\n",
        "                    'n_iter_no_change':10,\n",
        "                   'metric' : 'binary_error'\n",
        "                   }     ,     \n",
        "  'LogisticRegression ':\n",
        "  { \n",
        "      ' classifier__penalty': hp.choice(' classifier__penalty', ['l1', 'l2']),\n",
        "     'classifier__C' :hp.loguniform('classifier__C', np.log(0.1), np.log(1)) ,\n",
        "    'classifier__solver' : 'liblinear'\n",
        "    }   ,\n",
        "  'MLPClassifier':\n",
        "  {\n",
        "      'hidden_layer_sizes': hp.choice(' c8_hidden_layer_sizes',[(50,50,50), (50,100,50), (100,)]),\n",
        "    'activation':hp.choice ('c8_activation',['tanh', 'relu']),\n",
        "    'solver': hp.choice('c8_solver',['sgd', 'adam']) ,\n",
        "    'alpha':hp.loguniform('c8_alpha',np.log(0.0001), np.log(0.05) ),\n",
        "    'learning_rate':hp.choice('c8_learning_rate', ['constant','adaptive'],)\n",
        "    },\n",
        "  'GaussianNB':\n",
        "  {  },\n",
        "  'BaggingClassifier':\n",
        "            {      \n",
        "             'base_estimator': hp.choice('base_estimator',[DecisionTreeClassifier(**AA_classifier['parameters'])]),\n",
        "              'n_estimators': ho_scope.int(hp.quniform('c2_n_estimators', 5, 40, 10)),\n",
        "              'oob_score': hp.choice('oob_score', [True,False]),\n",
        "               'random_state': 1\n",
        "            }         \n",
        "       \n",
        "} \n",
        "\n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2,random_state = 0)\n",
        "\n",
        "best_clf3=[]\n",
        "final_clf=[]\n",
        "clf1_val_score=[]\n",
        "y_score=[]\n",
        "Fr_classifier=[]\n",
        "a1=[]\n",
        "classifiers=[]\n",
        "for i in range(len(try1)):\n",
        "    param_hyperopt3={try1[i][0].__name__:param_hyperopt[try1[i][0].__name__]}\n",
        "    clf_name=try1[i][0].__name__\n",
        "    cls=try1[i][0]\n",
        "    def f_clf1(params):\n",
        "       if not (params[clf_name]):   \n",
        "           model6 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),(clf_name,cls(**f_unpack_dict(params)))])\n",
        "           return model6\n",
        "       else:\n",
        "            model6 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),\n",
        "                                    (clf_name,cls(**f_unpack_dict(params[clf_name])))])\n",
        "            return model6\n",
        "    def objective_function(params,X_train2, y_train2):\n",
        "        model=f_clf1(params)\n",
        "        shuffle = KFold(n_splits=5, shuffle=True)\n",
        "        score = cross_val_score(model, X_train2, y_train2, cv=shuffle,scoring='roc_auc', n_jobs=-1).mean()\n",
        "        return {'loss': -score, 'status': STATUS_OK}  \n",
        "    trials = Trials()\n",
        "    best_clf3.append(fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                    param_hyperopt3, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))) \n",
        "    final_clf.append(f_clf1(space_eval(param_hyperopt3, best_clf3[i])).fit(X_train2, y_train2)) \n",
        "    # Calculating performance on validation set\n",
        "    y_score.append(final_clf[i].predict_proba(X_test2))  \n",
        "    clf1_val_score.append(roc_auc_score(y_test2, y_score[i][:,1])) \n",
        "    print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.format(-trials.best_trial['result']['loss'],\n",
        "                                                                       clf1_val_score[i]))\n",
        "    a1.append(space_eval(param_hyperopt3, best_clf3[i]))\n",
        "    classifiers.append({'classifier':cls,'Cross-val score':-trials.best_trial['result']['loss'],\n",
        "                        'validation score':clf1_val_score[i],'parameters':list(a1[i].values())[0]})\n",
        "classifiers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjsBIxM_05gN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try1[1][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roVBwvIwn_kR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifiers=[{'Cross-val score': 0.8886177024210724,\n",
        "  'classifier': try1[0][0],\n",
        "  'parameters': {'l2_regularization': 0.0,\n",
        "   'learning_rate': 0.029641454778718972,\n",
        "   'max_bins': 100,\n",
        "   'max_depth': 10,\n",
        "   'max_iter': 175,\n",
        "   'max_leaf_nodes': 20,\n",
        "   'min_samples_leaf': 15,\n",
        "   'n_iter_no_change': None,\n",
        "   'tol': 1e-07,\n",
        "   'validation_fraction': 0.1},\n",
        "  'validation score': 0.8893314575776722},\n",
        " {'Cross-val score': 0.8864910228042046,\n",
        "  'classifier': try1[1][0],\n",
        "  'parameters': {'criterion': 'entropy',\n",
        "   'max_depth': 15,\n",
        "   'max_features': 15,\n",
        "   'min_samples_leaf': 16,\n",
        "   'min_samples_split': 16,\n",
        "   'n_estimators': 65},\n",
        "  'validation score': 0.8889058957589675},\n",
        " {'Cross-val score': 0.9013013591922643,\n",
        "  'classifier': try1[2][0],\n",
        "  'parameters': {'criterion': 'entropy',\n",
        "   'max_depth': 30,\n",
        "   'max_features': 'auto',\n",
        "   'min_samples_leaf': 5,\n",
        "   'min_samples_split': 10,\n",
        "   'n_estimators': 50},\n",
        "  'validation score': 0.9035072526030485},\n",
        " {'Cross-val score': 0.8987301923595489,\n",
        "  'classifier': try1[3][0],\n",
        "  'parameters': {'algorithm': 'SAMME',\n",
        "   'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
        "                          max_depth=35, max_features='sqrt', max_leaf_nodes=None,\n",
        "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                          min_samples_leaf=25, min_samples_split=20,\n",
        "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
        "                          random_state=None, splitter='best'),\n",
        "   'learning_rate': 0.07293190523422614,\n",
        "   'n_estimators': 40},\n",
        "  'validation score': 0.9008969943167825},\n",
        " {'Cross-val score': 0.8960769962978226,\n",
        "  'classifier': try1[5][0],\n",
        "  'parameters': {'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
        "                          max_depth=35, max_features='sqrt', max_leaf_nodes=None,\n",
        "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                          min_samples_leaf=25, min_samples_split=20,\n",
        "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
        "                          random_state=None, splitter='best'),\n",
        "   'n_estimators': 40,\n",
        "   'oob_score': True,\n",
        "   'random_state': 1},\n",
        "  'validation score': 0.8981087067738658},\n",
        " {'Cross-val score': 0.7325646123357055,\n",
        "  'classifier': try1[4][0],\n",
        "  'parameters': {'max_depth': 5,\n",
        "   'max_features': 'log2',\n",
        "   'min_samples_leaf': 12,\n",
        "   'min_samples_split': 6},\n",
        "  'validation score': 0.7204596302586113},\n",
        " {'Cross-val score': 0.8268455654813259,\n",
        "  'classifier': try1[6][0],\n",
        "  'parameters': {},\n",
        "  'validation score': 0.8292819955028902},\n",
        " {'Cross-val score': 0.9012510175710758,\n",
        "  'classifier': try1[7][0],\n",
        "  'parameters': {'colsample_bytree': 0.5,\n",
        "   'learning_rate': 0.09,\n",
        "   'max_bin': 80,\n",
        "   'max_depth': 10,\n",
        "   'metric': 'binary_error',\n",
        "   'min_child_samples': 80,\n",
        "   'n_iter_no_change': 10,\n",
        "   'num_iterations': 1000,\n",
        "   'num_leave': 10,\n",
        "   'reg_alpha': 50,\n",
        "   'reg_lambda': 5,\n",
        "   'scale_pos_weight': 50.0,\n",
        "   'subsample': 0.8},\n",
        "  'validation score': 0.9021447915521081},\n",
        " {'Cross-val score': 0.7867763433775503,\n",
        "  'classifier': try1[8][0],\n",
        "  'parameters': {},\n",
        "  'validation score': 0.7887682267477347},\n",
        " {'Cross-val score': 0.9030023304309267,\n",
        "  'classifier': try1[9][0],\n",
        "  'parameters': {'colsample_bytree': 1.0,\n",
        "   'eta': 0.125,\n",
        "   'gamma': 0.75,\n",
        "   'max_depth': 15,\n",
        "   'min_child_weight': 3,\n",
        "   'n_estimators': 15,\n",
        "   'n_iter_no_change': 5,\n",
        "   'objective': 'binary:logistic',\n",
        "   'subsample': 0.7468459518994333},\n",
        "  'validation score': 0.9048467840317365}]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zTbGlzeOArL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try1[7][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u525waZMUoZk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifiers=[{'Cross-val score': 0.889145641582062,\n",
        "  'classifier': try1[0][0],\n",
        "  'parameters': {'l2_regularization': 0.0,\n",
        "   'learning_rate': 0.029641454778718972,\n",
        "   'max_bins': 100,\n",
        "   'max_depth': 10,\n",
        "   'max_iter': 175,\n",
        "   'max_leaf_nodes': 20,\n",
        "   'min_samples_leaf': 15,\n",
        "   'n_iter_no_change': None,\n",
        "   'tol': 1e-07,\n",
        "   'validation_fraction': 0.1},\n",
        "  'validation score': 0.8890387314781489},\n",
        " {'Cross-val score': 0.8862456887587555,\n",
        "  'classifier': try1[1][0],\n",
        "  'parameters': {'criterion': 'entropy',\n",
        "   'max_depth': 15,\n",
        "   'max_features': 15,\n",
        "   'min_samples_leaf': 12,\n",
        "   'min_samples_split': 10,\n",
        "   'n_estimators': 20},\n",
        "  'validation score': 0.8818171380065061},\n",
        " {'Cross-val score': 0.8846250315039053,\n",
        "  'classifier': try1[2][0],\n",
        "  'parameters': {'criterion': 'entropy',\n",
        "   'max_depth': 14,\n",
        "   'max_features': 'auto',\n",
        "   'min_samples_leaf': 2,\n",
        "   'min_samples_split': 6,\n",
        "   'n_estimators': 45},\n",
        "  'validation score': 0.8835185140616291},\n",
        " {'Cross-val score': 0.7629741618747085,\n",
        "  'classifier': try1[3][0],\n",
        "  'parameters': {'algorithm': 'SAMME',\n",
        "   'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
        "                          max_depth=1, max_features=None, max_leaf_nodes=None,\n",
        "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                          min_samples_leaf=1, min_samples_split=2,\n",
        "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
        "                          random_state=None, splitter='best'),\n",
        "   'learning_rate': 0.30044074314959096,\n",
        "   'n_estimators': 80},\n",
        "  'validation score': 0.7644778116050867},\n",
        " {'Cross-val score': 0.8961590358829536,\n",
        "  'classifier': try1[6][0],\n",
        "  'parameters': {'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
        "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
        "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                          min_samples_leaf=1, min_samples_split=2,\n",
        "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
        "                          random_state=None, splitter='best'),\n",
        "   'n_estimators': 30,\n",
        "   'oob_score': True,\n",
        "   'random_state': 1},\n",
        "  'validation score': 0.8989057692956279},\n",
        " {'Cross-val score': 0.7520561338733224,\n",
        "  'classifier': try1[4][0],\n",
        "  'parameters': {'max_depth': 6,\n",
        "   'max_features': 'sqrt',\n",
        "   'min_samples_leaf': 9,\n",
        "   'min_samples_split': 9},\n",
        "  'validation score': 0.7841260149694038},\n",
        " {'Cross-val score': 0.8268494098728028,\n",
        "  'classifier': try1[5][0],\n",
        "  'parameters': {},\n",
        "  'validation score': 0.8292819955028902},\n",
        " {'Cross-val score': 0.9013294538476375,\n",
        "  'classifier': try1[7][0],\n",
        "  'parameters': {'colsample_bytree': 0.5,\n",
        "   'learning_rate': 0.09,\n",
        "   'max_bin': 80,\n",
        "   'max_depth': 10,\n",
        "   'min_child_samples': 80,\n",
        "   'n_iter_no_change': 10,\n",
        "   'num_iterations': 1000,\n",
        "   'num_leave': 10,\n",
        "   'reg_alpha': 50,\n",
        "   'reg_lambda': 5,\n",
        "   'scale_pos_weight': 50.0,\n",
        "   'subsample': 0.8},\n",
        "  'validation score': 0.9021447915521081},\n",
        " {'Cross-val score': 0.7867791499718135,\n",
        "  'classifier': try1[8][0],\n",
        "  'parameters': {},\n",
        "  'validation score': 0.7887682267477347},\n",
        " {'Cross-val score': 0.9030990105443255,\n",
        "  'classifier': try1[9][0],\n",
        "  'parameters': {'colsample_bytree': 1.0,\n",
        "   'eta': 0.125,\n",
        "   'gamma': 0.75,\n",
        "   'max_depth': 15,\n",
        "   'min_child_weight': 3,\n",
        "   'n_estimators': 15,\n",
        "   'n_iter_no_change': 5,\n",
        "   'objective': 'binary:logistic',\n",
        "   'subsample': 0.7468459518994333},\n",
        "  'validation score': 0.9048467840317365}]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-ZPMwgETLqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "from operator import itemgetter\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "newlist = sorted(classifiers, key=itemgetter('Cross-val score'), reverse=True)\n",
        "fiv=[]\n",
        "for i in range(len(newlist)):\n",
        "     if round(newlist[i]['Cross-val score'],2)>0.85:\n",
        "        fiv.append(newlist[i])  \n",
        "        \n",
        "names=[]\n",
        "for i, _ in enumerate(Accepted):\n",
        "    names.append(\"pipeline\"+str(i+1))  \n",
        "AcceptedR=[]\n",
        "for i in range(len(Accepted)):\n",
        "    AcceptedR.append(Accepted[i][1])\n",
        "uu=max(len(fiv),len(AcceptedR))\n",
        "for i in range(uu):\n",
        "    if len(fiv)<uu :\n",
        "        fiv2.insert(len(fiv),fiv[len(fiv)-1])\n",
        "    elif len(AcceptedR)<uu :\n",
        "         AcceptedR.insert(len(AcceptedR),AcceptedR[len(AcceptedR)-1])\n",
        "dict = {}\n",
        "for name, Accepted_val in zip(names, AcceptedR ):\n",
        "    dict[name] =make_pipeline(Accepted_val(),fiv[AcceptedR.index(Accepted_val)]['classifier'](**fiv[AcceptedR.index(Accepted_val)]['parameters']))\n",
        "\n",
        "sclf2 = StackingClassifier(classifiers=list(dict.values()), meta_classifier=LogisticRegression()) \n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test2, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test2, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_lR0iCoafFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIRna7u_Mgo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fiv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH6WnTQ0iK2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fiv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro64pK_AQBNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "param_hyperopt1= {\n",
        "  'HistGradientBoostingClassifier':\n",
        "            {'learning_rate': hp.loguniform('c3_learning_rate', np.log(0.01), np.log(0.1)),\n",
        "                'max_iter': ho_scope.int(hp.quniform('c3_max_iter', 10, 600, 5)),\n",
        "                'max_depth': ho_scope.int(hp.quniform('c3_max_depth', 2,14, 1)),\n",
        "                'max_leaf_nodes': ho_scope.int(hp.quniform('c3_max_leaf_nodes', 5, 35,5)),\n",
        "                'min_samples_leaf': ho_scope.int(hp.quniform('c3_min_samples_leaf', 5, 25,5)),\n",
        "                'max_bins': ho_scope.int(hp.quniform('c3_max_bins', 20, 200,20)),\n",
        "                'validation_fraction':0.1,\n",
        "                'n_iter_no_change':None,\n",
        "                 'tol':1e-07,\n",
        "                'l2_regularization':0.0 \n",
        "           }\n",
        "       \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2,random_state = 0)\n",
        "def f_clf1(params):\n",
        "  model6 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),\n",
        "             ('HistGradientBoostingClassifier',HistGradientBoostingClassifier(**f_unpack_dict(params['HistGradientBoostingClassifier'])))])\n",
        "  return model6    \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf1(params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf1 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2), param_hyperopt1, algo=tpe.suggest, max_evals=10,\n",
        "                 trials=trials, rstate=np.random.RandomState(1))\n",
        "clf1 = f_clf1(space_eval(param_hyperopt1, best_clf1)).fit(X_train2, y_train2)\n",
        "# Calculating performance on validation set\n",
        "y_score = clf1.predict_proba(X_test2)\n",
        "clf1_val_score = roc_auc_score(y_test2, y_score[:,1])\n",
        "#clf2_val_score = model_selection.cross_val_score(f_clf2, X_train2,  y_train2, cv=5, scoring='roc_auc')\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf1_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(space_eval(param_hyperopt, best_clf1))\n",
        "a1=space_eval(param_hyperopt1, best_clf1)\n",
        "Fr_classifier={'classifier':list(a1.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf1_val_score,'parameters':list(a1.values())[0]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQd6zlyfIBks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Fr_classifier={'Cross-val score': 0.8993079169559552,\n",
        " 'classifier': 'HistGradientBoostingClassifier',\n",
        " 'parameters': {'l2_regularization': 0.0,\n",
        "  'learning_rate': 0.029641454778718972,\n",
        "  'max_bins': 100,\n",
        "  'max_depth': 10,\n",
        "  'max_iter': 520,\n",
        "  'max_leaf_nodes': 20,\n",
        "  'min_samples_leaf': 15,\n",
        "  'n_iter_no_change': None,\n",
        "  'tol': 1e-07,\n",
        "  'validation_fraction': 0.1},\n",
        " 'validation score': 0.9002456136302449}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Azd6FkhgUPtB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Fr_classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bkp-7STuMRJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "param_hyperopt1= {\n",
        "   'ExtraTreesClassifier':\n",
        "            {\n",
        "                'max_depth': ho_scope.int(hp.quniform('c4_max_depth', 5, 20, 5)),\n",
        "                'n_estimators': ho_scope.int(hp.quniform('c4_n_estimators', 5, 200, 5)),\n",
        "                'max_features': ho_scope.int(hp.quniform('c4_max_features', 5, 20, 5)),\n",
        "                'criterion': hp.choice('c4_criterion', ['gini', 'entropy']),\n",
        "                'min_samples_split': ho_scope.int(hp.quniform('c4_min_samples_split', 2, 20,2)),\n",
        "                'min_samples_leaf': ho_scope.int(hp.quniform('c4_min_samples_leaf', 2, 20,2))      \n",
        "             }     \n",
        "       \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2,random_state = 0)\n",
        "def f_clf1(params):\n",
        "  model6 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),\n",
        "             ('ExtraTreesClassifier',ExtraTreesClassifier(**f_unpack_dict(params['ExtraTreesClassifier'])))])\n",
        "  return model6    \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf1(params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf1 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2), param_hyperopt1, algo=tpe.suggest, max_evals=10,\n",
        "                 trials=trials, rstate=np.random.RandomState(1))\n",
        "clf1 = f_clf1(space_eval(param_hyperopt1, best_clf1)).fit(X_train2, y_train2)\n",
        "# Calculating performance on validation set\n",
        "y_score = clf1.predict_proba(X_test2)\n",
        "clf1_val_score = roc_auc_score(y_test2, y_score[:,1])\n",
        "#clf2_val_score = model_selection.cross_val_score(f_clf2, X_train2,  y_train2, cv=5, scoring='roc_auc')\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf1_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(space_eval(param_hyperopt, best_clf1))\n",
        "a1=space_eval(param_hyperopt1, best_clf1)\n",
        "Fr_classifier={'classifier':list(a1.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf1_val_score,'parameters':list(a1.values())[0]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha8jZBOPyWoB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Fr_classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxF_ty_pK5vZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Fr_classifier={'Cross-val score': 0.8872052232768184,\n",
        " 'classifier': 'ExtraTreesClassifier',\n",
        " 'parameters': {'criterion': 'entropy',\n",
        "  'max_depth': 15,\n",
        "  'max_features': 15,\n",
        "  'min_samples_leaf': 16,\n",
        "  'min_samples_split': 16,\n",
        "  'n_estimators': 65},\n",
        " 'validation score': 0.885587100775846}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiM49RkuqGq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble.forest import RandomForestClassifier\n",
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "param_hyperopt2= {\n",
        "  'RandomForestClassifier': \n",
        "            {\n",
        "            'max_depth': ho_scope.int(hp.quniform('c2_max_depth', 2, 12, 5)),\n",
        "            'n_estimators': ho_scope.int(hp.quniform('c2_n_estimators', 7, 50, 10)),\n",
        "             'max_features': hp.choice('c2_max_features', ['auto', 'sqrt']),\n",
        "            # 'max_features': ho_scope.int(hp.quniform('c2_max_features', 2, 7, 3)),\n",
        "            'criterion': hp.choice('c2_criterion', ['gini', 'entropy']),\n",
        "            'min_samples_split': ho_scope.int(hp.quniform('c2_min_samples_split', 3, 12,5)),\n",
        "            'min_samples_leaf': ho_scope.int(hp.quniform('c2_min_samples_leaf', 3, 12,5))\n",
        "                          \n",
        "         } \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "def f_clf2(params):\n",
        "  model4 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),('RandomForestClassifier',RandomForestClassifier(**f_unpack_dict(params['RandomForestClassifier'])))])\n",
        "  return model4    \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf2(params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf2 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                 param_hyperopt2, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "clf2 = f_clf2(space_eval(param_hyperopt2, best_clf2)).fit(X_train2, y_train2)\n",
        "# Calculating performance on validation set\n",
        "y_score = clf2.predict_proba(X_test2)\n",
        "clf2_val_score = roc_auc_score(y_test2, y_score[:,1])\n",
        "#clf2_val_score = model_selection.cross_val_score(f_clf2, X_train2,  y_train2, cv=5, scoring='roc_auc')\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf2_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(\n",
        "a2=space_eval(param_hyperopt2, best_clf2)\n",
        "Se_classifier={'classifier':list(a2.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf2_val_score,'parameters':list(a2.values())[0]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XU9GE2uOipb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Se_classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0zd-Q0nMRGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Se_classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_cLstq3kPbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Se_classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_Qo9loEyJ9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_clf2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKMWqZQ8L3JV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.utils import shuffle\n",
        "param_hyperopt3= {      \n",
        "       'BaggingClassifier':\n",
        "            {      \n",
        "             'base_estimator': hp.choice('base_estimator',[DecisionTreeClassifier(**AA_classifier['parameters'])]),\n",
        "              'n_estimators': ho_scope.int(hp.quniform('n_estimators', 5, 45,15)),\n",
        "              'oob_score': hp.choice('oob_score', [True,False]),\n",
        "               'random_state': 1\n",
        "            }\n",
        "                  \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "def f_clf3(params):\n",
        "   model1 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),('BaggingClassifier', BaggingClassifier(**f_unpack_dict(params['BaggingClassifier'])))])   \n",
        "   return model1\n",
        "        \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf3(params)\n",
        "    shuffle = KFold(n_splits=5, shuffle=True)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=shuffle,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf3 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                param_hyperopt3, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "clf3 = f_clf3(space_eval(param_hyperopt3, best_clf3)).fit(X_train2, y_train2)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf3_val_score = roc_auc_score(y_test, clf3.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf3_val_score))\n",
        "print('Best parameters:')\n",
        "print(space_eval(param_hyperopt3, best_clf3))\n",
        "# Calculating performance on validation set\n",
        "clf3_val_score = roc_auc_score(y_test, clf3.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf3_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(\n",
        "a3=space_eval(param_hyperopt3, best_clf3)\n",
        "Thrid_classifier={'classifier':list(a3.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf3_val_score,'parameters':list(a3.values())[0]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-AC9k5VLzSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Thrid_classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isvNltHIFgwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Se_classifier={'Cross-val score': 0.901158842931584,\n",
        " 'classifier': 'RandomForestClassifier',\n",
        " 'parameters': {'criterion': 'entropy',\n",
        "  'max_depth': 30,\n",
        "  'max_features': 'auto',\n",
        "  'min_samples_leaf': 5,\n",
        "  'min_samples_split': 10,\n",
        "  },\n",
        " 'validation score': 0.9035936842810743}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB0p-1daEvuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.utils import shuffle\n",
        "param_hyperopt3= {      \n",
        "       'BaggingClassifier':\n",
        "            {      \n",
        "             'base_estimator': hp.choice('base_estimator',[DecisionTreeClassifier(**AA_classifier['parameters'])]),\n",
        "              'n_estimators': ho_scope.int(hp.quniform('c2_n_estimators', 5, 40, 10)),\n",
        "              'oob_score': hp.choice('oob_score', [True,False]),\n",
        "               'random_state': 1\n",
        "            }\n",
        "                \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "def f_clf3(params):\n",
        "   model1 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),('BaggingClassifier', BaggingClassifier(**f_unpack_dict(params['BaggingClassifier'])))])   \n",
        "   return model1\n",
        "        \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf3(params)\n",
        "    shuffle = KFold(n_splits=5, shuffle=True)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=shuffle,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf3 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                param_hyperopt3, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "clf3 = f_clf3(space_eval(param_hyperopt3, best_clf3)).fit(X_train2, y_train2)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf3_val_score = roc_auc_score(y_test, clf3.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf3_val_score))\n",
        "print('Best parameters:')\n",
        "print(space_eval(param_hyperopt3, best_clf3))\n",
        "# Calculating performance on validation set\n",
        "clf3_val_score = roc_auc_score(y_test, clf3.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf3_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(\n",
        "a3=space_eval(param_hyperopt3, best_clf3)\n",
        "Thrid_classifier={'classifier':list(a3.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf3_val_score,'parameters':list(a3.values())[0]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j37o6YxZZgrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Thrid_classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O2unkUj6im8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.utils import shuffle\n",
        "param_hyperopt3= {      \n",
        "       'AdaBoostClassifier':\n",
        "   {\n",
        "         'base_estimator':hp.choice('base_estimator', [DecisionTreeClassifier(**AA_classifier['parameters'])]),\n",
        "          'learning_rate': hp.loguniform('c5_learning_rate', np.log(0.01), np.log(1)),\n",
        "          'n_estimators': ho_scope.int(hp.quniform('c5_n_estimators', 10, 40, 10)),\n",
        "          'algorithm': hp.choice('c5_algorithm',[\"SAMME\"])  \n",
        "       }\n",
        "                \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "def f_clf3(params):\n",
        "   model1 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),('AdaBoostClassifier', AdaBoostClassifier(**f_unpack_dict(params['AdaBoostClassifier'])))])   \n",
        "   return model1\n",
        "        \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf3(params)\n",
        "    shuffle = KFold(n_splits=5, shuffle=True)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=shuffle,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf3 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                param_hyperopt3, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "clf3 = f_clf3(space_eval(param_hyperopt3, best_clf3)).fit(X_train2, y_train2)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf3_val_score = roc_auc_score(y_test, clf3.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf3_val_score))\n",
        "print('Best parameters:')\n",
        "print(space_eval(param_hyperopt3, best_clf3))\n",
        "# Calculating performance on validation set\n",
        "clf3_val_score = roc_auc_score(y_test, clf3.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf3_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(\n",
        "a3=space_eval(param_hyperopt3, best_clf3)\n",
        "fifth_classifier={'classifier':list(a3.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf3_val_score,'parameters':list(a3.values())[0]}\n",
        "fifth_classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6ZkDVZl8N3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fifth_classifier={'Cross-val score': 0.8996358063279757,\n",
        " 'classifier': 'AdaBoostClassifier',\n",
        " 'parameters': {'algorithm': 'SAMME',\n",
        "  'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
        "                         max_depth=20, max_features='sqrt', max_leaf_nodes=None,\n",
        "                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                         min_samples_leaf=5, min_samples_split=30,\n",
        "                         min_weight_fraction_leaf=0.0, presort='deprecated',\n",
        "                         random_state=None, splitter='best'),\n",
        "  'learning_rate': 0.07293190523422614,\n",
        "  'n_estimators': 40},\n",
        " 'validation score': 0.9128871828299814}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv4WPj9XX5OS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.utils import shuffle\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "import xgboost as xgb\n",
        "\n",
        "param_hyperopt4= {\n",
        "  'XGBClassifier':\n",
        "      {\n",
        "           'max_depth': ho_scope.int(hp.quniform('c16_max_depth', 5, 15, 5)),\n",
        "           'min_child_weight': hp.choice ('x_min_child',np.arange(1, 10, dtype=int)),\n",
        "           'subsample': hp.uniform ('x_subsample', 0.06, 1),\n",
        "           'n_estimators': ho_scope.int(hp.quniform('c2_n_estimators', 5, 45, 15)),\n",
        "       }         \n",
        "       \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "def f_clf4(params):\n",
        "   model1 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),('XGBClassifier', xgb.XGBClassifier())])   \n",
        "   return model1  \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf4(params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf4 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                param_hyperopt4, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "clf4 = f_clf4(space_eval(param_hyperopt4, best_clf4)).fit(X_train2, y_train2)\n",
        "# Calculating performance on validation set\n",
        "clf4_val_score = roc_auc_score(y_test, clf4.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf4_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(\n",
        "a4=space_eval(param_hyperopt4, best_clf4)\n",
        "Four_classifier={'classifier':list(a4.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf4_val_score,'parameters':list(a4.values())[0]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmuOPHfZZL9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Four_classifier\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6WfRi1-5dev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll import scope as ho_scope\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.utils import shuffle\n",
        "import lightgbm as lgb\n",
        "from functools import partial\n",
        "\n",
        "param_hyperopt5= {\n",
        "   'LGBMClassifier1':\n",
        "                  {\n",
        "                   'max_depth': ho_scope.int(hp.quniform('c16_max_depth', -2, 10, 1)),\n",
        "                   'num_leave': ho_scope.int(hp.quniform('c16_num_leaves', 10, 200, 10)),\n",
        "                   'min_child_samples': ho_scope.int(hp.quniform('c16_min_child_samples', 10, 90, 10)),\n",
        "                   'scale_pos_weight': ho_scope.int(hp.quniform('c16_scale_pos_weight', 10, 90, 10)),\n",
        "                   'subsample': ho_scope.float(hp.quniform('c16_subsample', 0.1, 0.9, 0.1)),\n",
        "                   'colsample_bytree': ho_scope.float(hp.quniform('c16_colsample_bytree', 0.1, 0.9, 0.1)),\n",
        "                   'reg_lambda': hp.choice(\"c16_reg_lambda\",[0, 1e-1, 1, 5, 10, 20, 50]),\n",
        "                   'reg_alpha': hp.choice(\"c16_reg_alpha\",[0, 1e-1, 1, 5, 10, 20, 50]),\n",
        "                   'bagging_fraction': ho_scope.float(hp.quniform('c16_bagging_fraction', 0.1, 0.9, 0.05)),\n",
        "                    'bagging_freq': ho_scope.int(hp.quniform('c6_bagging_freq', 1, 8, 1)),\n",
        "                   'min_data_in_leaf': ho_scope.int(hp.quniform('c16_min_data_in_leaf', 100, 1000, 100)),\n",
        "                  'min_sum_hessian_in_leaf': ho_scope.int(hp.quniform('c16_min_sum_hessian_in_leaf', 5, 20, 5)),\n",
        "                   'max_bin': ho_scope.int(hp.quniform('c16_max_bin', 10, 100, 10)),\n",
        "                   'learning_rate': ho_scope.float(hp.quniform('c16_learning_rate', 0.001, 0.09, 0.005)),\n",
        "                   'num_iterations': ho_scope.int(hp.quniform('c16_num_iterations', 100, 10000, 100)),\n",
        "                   'n_iter_no_change':10\n",
        "                       },\n",
        "                  \n",
        "                   'LGBMClassifier':\n",
        "                  {\n",
        "                     'max_depth': ho_scope.int(hp.quniform('c6_max_depth', 2, 14, 2)),\n",
        "                   'num_leave': ho_scope.int(hp.quniform('c6_num_leaves', 5, 30, 5)),\n",
        "                   'min_child_samples': ho_scope.int(hp.quniform('c6_min_child_samples', 50, 100, 10)),\n",
        "                   'scale_pos_weight': ho_scope.float(hp.quniform('c6_scale_pos_weight', 50, 100, 10)),\n",
        "                   'subsample': ho_scope.float(hp.quniform('c6_subsample', 0.2, 0.9, 0.1)),\n",
        "                   'colsample_bytree': ho_scope.float(hp.quniform('c6_colsample_bytree', 0.2, 0.9, 0.1)),\n",
        "                   'reg_lambda': hp.choice(\"c6_reg_lambda\",[0, 1e-1, 1, 5, 10, 20, 50]),\n",
        "                   'reg_alpha': hp.choice(\"c6_reg_alpha\",[0, 1e-1, 1, 5, 10, 20, 50]) ,\n",
        "                   'max_bin': ho_scope.int(hp.quniform('c6_max_bin', 10, 200, 10)),\n",
        "                   'learning_rate': ho_scope.float(hp.quniform('c6_learning_rate', 0.001, 0.09, 0.005)),\n",
        "                   'num_iterations': ho_scope.int(hp.quniform('c6_num_iterations', 100, 2000, 100)),\n",
        "                    'n_iter_no_change':10,\n",
        "                   'metric' : 'binary_error'\n",
        "                   }     \n",
        "} \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "def f_clf5(params):\n",
        "   model1 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),('LGBMClassifier', \n",
        "                                                         lgb.LGBMClassifier(**f_unpack_dict(params['LGBMClassifier'])))])   \n",
        "   return model1  \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf5(params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf5 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                param_hyperopt5, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "clf5 = f_clf5(space_eval(param_hyperopt5, best_clf5)).fit(X_train2, y_train2)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf5_val_score = roc_auc_score(y_test, clf5.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf5_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(\n",
        "a5=space_eval(param_hyperopt5, best_clf5)\n",
        "fith_classifier={'classifier':list(a5.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf5_val_score,'parameters':list(a5.values())[0]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5RjjjioGC-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fith_classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn4Xx2mh_YYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fith_classifier['parameters']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cgd1WCfT9JN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DecisionTreeClassifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpJXkL-3TzNp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AA_classifier['parameters']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3If0WbQtPcQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "from hyperopt.pyll import scope as ho_scope\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.utils import shuffle\n",
        "param_hyperopt6= {\n",
        "    'QuadraticDiscriminantAnalysis':\n",
        "      {\n",
        "           \n",
        "      }      \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "def f_clf6(params):\n",
        "   model3 =Pipeline(steps=[('encoder',Accepted[5][1]()),('QuadraticDiscriminantAnalysis',QuadraticDiscriminantAnalysis())])\n",
        "   return model3 \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf6(params)\n",
        "    #clf = RandomForestClassifier(**params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf6 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                param_hyperopt6, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "\n",
        "clf6 = f_clf6(space_eval(param_hyperopt6, best_clf6)).fit(X_train2, y_train2)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf6_val_score = roc_auc_score(y_test, clf6.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.format(-trials.best_trial['result']['loss'], clf6_val_score))\n",
        "print('Best parameters:')\n",
        "print(space_eval(param_hyperopt6, best_clf6))\n",
        "# Calculating performance on validation set\n",
        "clf6_val_score = roc_auc_score(y_test, clf6.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf6_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(\n",
        "a6=space_eval(param_hyperopt6, best_clf6)\n",
        "six_classifier={'classifier':list(a6.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf6_val_score,'parameters':list(a6.values())[0]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBtie6F3erRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
        "# now you can import normally from ensemble\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "Fr_classifier={'Cross-val score': 0.8872052232768184,\n",
        " 'classifier': 'ExtraTreesClassifier',\n",
        " 'parameters': {'criterion': 'entropy',\n",
        "  'max_depth': 15,\n",
        "  'max_features': 15,\n",
        "  'min_samples_leaf': 16,\n",
        "  'min_samples_split': 16,\n",
        "  'n_estimators': 65},\n",
        " 'validation score': 0.885587100775846}\n",
        "\n",
        "Se_classifier={'Cross-val score': 0.8993079169559552,\n",
        " 'classifier': 'HistGradientBoostingClassifier',\n",
        " 'parameters': {'l2_regularization': 0.0,\n",
        "  'learning_rate': 0.029641454778718972,\n",
        "  'max_bins': 100,\n",
        "  'max_depth': 10,\n",
        "  'max_iter': 520,\n",
        "  'max_leaf_nodes': 20,\n",
        "  'min_samples_leaf': 15,\n",
        "  'n_iter_no_change': None,\n",
        "  'tol': 1e-07,\n",
        "  'validation_fraction': 0.1},\n",
        " 'validation score': 0.9002456136302449}\n",
        "thrid_classifier={'Cross-val score': 0.9011091848955551,\n",
        " 'classifier': 'RandomForestClassifier',\n",
        " 'parameters': {'criterion': 'entropy',\n",
        "   'max_depth': 30,\n",
        "   'max_features': 'auto',\n",
        "   'min_samples_leaf': 5,\n",
        "   'min_samples_split': 10,\n",
        "   'n_estimators': 50},\n",
        " 'validation score': 0.9033358063569614}\n",
        "\n",
        "thrid_classifier1={'Cross-val score': 0.8872229092739982,\n",
        " 'classifier': 'RandomForestClassifier',\n",
        " 'parameters': {'criterion': 'entropy',\n",
        "   'max_depth': 14,\n",
        "   'max_features': 'auto',\n",
        "   'min_samples_leaf': 2,\n",
        "   'min_samples_split': 6,\n",
        "   'n_estimators': 45},\n",
        " 'validation score': 0.8885934547923751}\n",
        "thrid_classifier2={'Cross-val score': 0.8646501366567945,\n",
        " 'classifier': 'RandomForestClassifier',\n",
        " 'parameters': {'criterion': 'entropy',\n",
        "  'max_depth': 10,\n",
        "  'max_features': 6,\n",
        "  'min_samples_leaf': 15,\n",
        "  'min_samples_split': 10,\n",
        "  'n_estimators': 30},\n",
        " 'validation score': 0.8684014635305586}\n",
        "fourth_classifier={'Cross-val score': 0.8956732554663308,\n",
        " 'classifier': 'BaggingClassifier',\n",
        " 'parameters': {'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini',\n",
        "                         max_depth=None, max_features=None, max_leaf_nodes=None,\n",
        "                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                         min_samples_leaf=1, min_samples_split=2,\n",
        "                         min_weight_fraction_leaf=0.0,\n",
        "                         random_state=None, splitter='best'),\n",
        "  'n_estimators': 30,\n",
        "  'oob_score': True,\n",
        "  'random_state': 1},\n",
        " 'validation score': 0.9280463385108305}\n",
        "fifth_classifier={'Cross-val score': 0.8976371765417721,\n",
        " 'classifier': 'BaggingClassifier',\n",
        " 'parameters': {'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='entropy',\n",
        "                         max_depth=20, max_features='sqrt', max_leaf_nodes=None,\n",
        "                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                         min_samples_leaf=5, min_samples_split=30,\n",
        "                         min_weight_fraction_leaf=0.0,   random_state=None, splitter='best'),\n",
        "  'n_estimators': 45,\n",
        "  'oob_score': False,\n",
        "  'random_state': 1},\n",
        " 'validation score': 0.9073699757159257}\n",
        "\n",
        "six_classifier={'Cross-val score': 0.8996358063279757,\n",
        " 'classifier': 'AdaBoostClassifier',\n",
        " 'parameters': {'algorithm': 'SAMME',\n",
        "  'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='entropy',\n",
        "                         max_depth=20, max_features='sqrt', max_leaf_nodes=None,\n",
        "                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                         min_samples_leaf=5, min_samples_split=30,\n",
        "                         min_weight_fraction_leaf=0.0, \n",
        "                         random_state=None, splitter='best'),\n",
        "  'learning_rate': 0.07293190523422614,\n",
        "  'n_estimators': 40},\n",
        " 'validation score': 0.9128871828299814}\n",
        "seven_classifier={'Cross-val score': 0.8774978395308153,\n",
        " 'classifier': 'XGBClassifier',\n",
        " 'parameters': {'max_depth': 10,\n",
        "  'min_child_weight': 8,\n",
        "  'n_estimators': 15,\n",
        "  'subsample': 0.28517758976174756},\n",
        " 'validation score': 0.8781323643924608}\n",
        "pipe1 = make_pipeline(Accepted[3][1](),XGBClassifier(**seven_classifier['parameters']))\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier(**thrid_classifier['parameters']))  \n",
        "pipe3 = make_pipeline(Accepted[0][1](),ExtraTreesClassifier(**Fr_classifier['parameters'])) \n",
        "pipe6 = make_pipeline(Accepted[4][1](),AdaBoostClassifier(**six_classifier['parameters'])) \n",
        "pipe4 = make_pipeline(Accepted[2][1](),HistGradientBoostingClassifier(**Se_classifier['parameters']))\n",
        "pipe5 = make_pipeline(Accepted[5][1](), BaggingClassifier(**fifth_classifier['parameters']) )\n",
        "\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe5,pipe6], meta_classifier=LogisticRegression()) \n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test2, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test2, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oUvZj4cvDNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
        "# now you can import normally from ensemble\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "pipe1 = make_pipeline(Accepted[3][1](),XGBClassifier())\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier())  \n",
        "pipe3 = make_pipeline(Accepted[0][1](),ExtraTreesClassifier()) \n",
        "pipe6 = make_pipeline(Accepted[4][1](),lgb.LGBMClassifier(**fith_classifier['parameters'])) \n",
        "pipe4 = make_pipeline(Accepted[2][1](),HistGradientBoostingClassifier())\n",
        "pipe5 = make_pipeline(Accepted[5][1](),QuadraticDiscriminantAnalysis())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe5], meta_classifier=LogisticRegression()) \n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test2, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test2, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZRFjh3jzf8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
        "# now you can import normally from ensemble\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier())  \n",
        "pipe3 = make_pipeline(Accepted[0][1](),ExtraTreesClassifier()) \n",
        "pipe6 = make_pipeline(Accepted[3][1](),lgb.LGBMClassifier(**)) \n",
        "pipe4 = make_pipeline(Accepted[2][1](),HistGradientBoostingClassifier())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe2,pipe3,pipe4,pipe6], meta_classifier=LogisticRegression()) \n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test2, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test2, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DFF7PDGoM7j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import category_encoders\n",
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
        "# now you can import normally from ensemble\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "ff={'Cross-val score': 0.8846250315039053,\n",
        "  'classifier': 'RandomForestClassifier',\n",
        "  'parameters': {'criterion': 'entropy',\n",
        "   'max_depth': 14,\n",
        "   'max_features': 'auto',\n",
        "   'min_samples_leaf': 2,\n",
        "   'min_samples_split': 6,\n",
        "   'n_estimators': 45},\n",
        "  'validation score': 0.8835185140616291}\n",
        "  Fr_classifier={'Cross-val score': 0.8872052232768184,\n",
        " 'classifier': 'ExtraTreesClassifier',\n",
        " 'parameters': {'criterion': 'entropy',\n",
        "  'max_depth': 15,\n",
        "  'max_features': 15,\n",
        "  'min_samples_leaf': 16,\n",
        "  'min_samples_split': 16,\n",
        "  'n_estimators': 65},\n",
        " 'validation score': 0.885587100775846}\n",
        "\n",
        "Se_classifier={'Cross-val score': 0.8993079169559552,\n",
        " 'classifier': 'HistGradientBoostingClassifier',\n",
        " 'parameters': {'l2_regularization': 0.0,\n",
        "  'learning_rate': 0.029641454778718972,\n",
        "  'max_bins': 100,\n",
        "  'max_depth': 10,\n",
        "  'max_iter': 520,\n",
        "  'max_leaf_nodes': 20,\n",
        "  'min_samples_leaf': 15,\n",
        "  'n_iter_no_change': None,\n",
        "  'tol': 1e-07,\n",
        "  'validation_fraction': 0.1},\n",
        " 'validation score': 0.9002456136302449}\n",
        "thrid_classifier={'Cross-val score': 0.9011091848955551,\n",
        " 'classifier': 'RandomForestClassifier',\n",
        " 'parameters': {'criterion': 'entropy',\n",
        "  'max_depth': 30,\n",
        "  'max_features': 'auto',\n",
        "  'min_samples_leaf': 5,\n",
        "  'min_samples_split': 10,\n",
        "  'n_estimators': 50},\n",
        " 'validation score': 0.9033358063569614}\n",
        "thrid_classifier1={'Cross-val score': 0.8466430869946956,\n",
        " 'classifier': 'RandomForestClassifier',\n",
        " 'parameters': {'criterion': 'entropy',\n",
        "  'max_depth': 10,\n",
        "  'max_features': 4,\n",
        "  'min_samples_leaf': 15,\n",
        "  'min_samples_split': 15,\n",
        "  'n_estimators': 20},\n",
        " 'validation score': 0.8448705278585762}\n",
        "thrid_classifier2={'Cross-val score': 0.8846250315039053,\n",
        "  'classifier': 'RandomForestClassifier',\n",
        "  'parameters': {'criterion': 'entropy',\n",
        "   'max_depth': 14,\n",
        "   'max_features': 'auto',\n",
        "   'min_samples_leaf': 2,\n",
        "   'min_samples_split': 6,\n",
        "   'n_estimators': 45},\n",
        "  'validation score': 0.8835185140616291}\n",
        "thrid_classifier3={'Cross-val score': 0.8596536275262364,\n",
        " 'classifier': 'RandomForestClassifier',\n",
        " 'parameters': {'criterion': 'gini',\n",
        "  'max_depth': 10,\n",
        "  'max_features': 'auto',\n",
        "  'min_samples_leaf': 5,\n",
        "  'min_samples_split': 10,\n",
        "  'n_estimators': 30},\n",
        " 'validation score': 0.8552454210104643}\n",
        "thrid_classifier4={'Cross-val score': 0.8872052232768184,\n",
        " 'classifier': 'ExtraTreesClassifier',\n",
        " 'parameters': {'criterion': 'entropy',\n",
        "  'max_depth': 15,\n",
        "  'max_features': 15,\n",
        "  'min_samples_leaf': 16,\n",
        "  'min_samples_split': 16,\n",
        "  'n_estimators': 65},\n",
        " 'validation score': 0.885587100775846}\n",
        "thrid_classifier5={'Cross-val score': 0.8612379579229952,\n",
        " 'classifier': 'RandomForestClassifier',\n",
        " 'parameters': {'criterion': 'entropy',\n",
        "  'max_depth': 10,\n",
        "  'max_features': 'auto',\n",
        "  'min_samples_leaf': 10,\n",
        "  'min_samples_split': 5,\n",
        "  'n_estimators': 50},\n",
        " 'validation score': 0.864605381012056}\n",
        "fourth_classifier={'Cross-val score': 0.8956732554663308,\n",
        " 'classifier': 'BaggingClassifier',\n",
        " 'parameters': {'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini',\n",
        "                         max_depth=None, max_features=None, max_leaf_nodes=None,\n",
        "                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                         min_samples_leaf=1, min_samples_split=2,\n",
        "                         min_weight_fraction_leaf=0.0,\n",
        "                         random_state=None, splitter='best'),\n",
        "  'n_estimators': 30,\n",
        "  'oob_score': True,\n",
        "  'random_state': 1},\n",
        " 'validation score': 0.9280463385108305}\n",
        "fifth_classifier={'Cross-val score': 0.8976371765417721,\n",
        " 'classifier': 'BaggingClassifier',\n",
        " 'parameters': {'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='entropy',\n",
        "                         max_depth=20, max_features='sqrt', max_leaf_nodes=None,\n",
        "                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                         min_samples_leaf=5, min_samples_split=30,\n",
        "                         min_weight_fraction_leaf=0.0,   random_state=None, splitter='best'),\n",
        "  'n_estimators': 45,\n",
        "  'oob_score': False,\n",
        "  'random_state': 1},\n",
        " 'validation score': 0.9073699757159257}\n",
        "\n",
        "six_classifier={'Cross-val score': 0.8996358063279757,\n",
        " 'classifier': 'AdaBoostClassifier',\n",
        " 'parameters': {'algorithm': 'SAMME',\n",
        "  'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='entropy',\n",
        "                         max_depth=20, max_features='sqrt', max_leaf_nodes=None,\n",
        "                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                         min_samples_leaf=5, min_samples_split=30,\n",
        "                         min_weight_fraction_leaf=0.0, \n",
        "                         random_state=None, splitter='best'),\n",
        "  'learning_rate': 0.07293190523422614,\n",
        "  'n_estimators': 40},\n",
        " 'validation score': 0.9128871828299814}\n",
        "seven_classifier={'Cross-val score': 0.8774978395308153,\n",
        " 'classifier': 'XGBClassifier',\n",
        " 'parameters': {'max_depth': 10,\n",
        "  'min_child_weight': 8,\n",
        "  'n_estimators': 15,\n",
        "  'subsample': 0.28517758976174756},\n",
        " 'validation score': 0.8781323643924608}\n",
        "\n",
        "pipe1 = make_pipeline(category_encoders.cat_boost.CatBoostEncoder(),XGBClassifier(**seven_classifier['parameters']))\n",
        "pipe2 = make_pipeline(category_encoders.james_stein.JamesSteinEncoder(),RandomForestClassifier(**ff['parameters']))  \n",
        "pipe3 = make_pipeline(category_encoders.m_estimate.MEstimateEncoder(),ExtraTreesClassifier()) \n",
        "pipe6 = make_pipeline(category_encoders.ordinal.OrdinalEncoder(),lgb.LGBMClassifier()) \n",
        "pipe4 = make_pipeline(category_encoders.target_encoder.TargetEncoder(),HistGradientBoostingClassifier())\n",
        "pipe5 = make_pipeline(category_encoders.woe.WOEEncoder(), BaggingClassifier(DecisionTreeClassifier()) )\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe5], meta_classifier=LogisticRegression()) \n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test2, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test2, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGsaFTuPJ_rJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import category_encoders\n",
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
        "# now you can import normally from ensemble\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "ff={'Cross-val score': 0.8846250315039053,\n",
        "  'classifier': 'RandomForestClassifier',\n",
        "  'parameters': {'criterion': 'entropy',\n",
        "   'max_depth': 14,\n",
        "   'max_features': 'auto',\n",
        "   'min_samples_leaf': 2,\n",
        "   'min_samples_split': 6,\n",
        "   'n_estimators': 45},\n",
        "  'validation score': 0.8835185140616291}\n",
        "Fr_classifier={'Cross-val score': 0.8872052232768184,\n",
        " 'classifier': 'ExtraTreesClassifier',\n",
        " 'parameters': {'criterion': 'entropy',\n",
        "  'max_depth': 15,\n",
        "  'max_features': 15,\n",
        "  'min_samples_leaf': 16,\n",
        "  'min_samples_split': 16,\n",
        "  'n_estimators': 65},\n",
        " 'validation score': 0.885587100775846}\n",
        "\n",
        "Se_classifier={'Cross-val score': 0.8993079169559552,\n",
        " 'classifier': 'HistGradientBoostingClassifier',\n",
        " 'parameters': {'l2_regularization': 0.0,\n",
        "  'learning_rate': 0.029641454778718972,\n",
        "  'max_bins': 100,\n",
        "  'max_depth': 10,\n",
        "  'max_iter': 520,\n",
        "  'max_leaf_nodes': 20,\n",
        "  'min_samples_leaf': 15,\n",
        "  'n_iter_no_change': None,\n",
        "  'tol': 1e-07,\n",
        "  'validation_fraction': 0.1},\n",
        " 'validation score': 0.9002456136302449}\n",
        "thrid_classifier={'Cross-val score': 0.9011091848955551,\n",
        " 'classifier': 'RandomForestClassifier',\n",
        " 'parameters': {'criterion': 'entropy',\n",
        "  'max_depth': 30,\n",
        "  'max_features': 'auto',\n",
        "  'min_samples_leaf': 5,\n",
        "  'min_samples_split': 10,\n",
        "  'n_estimators': 50},\n",
        " 'validation score': 0.9033358063569614}\n",
        "thrid_classifier1={'Cross-val score': 0.8466430869946956,\n",
        " 'classifier': 'RandomForestClassifier',\n",
        " 'parameters': {'criterion': 'entropy',\n",
        "  'max_depth': 10,\n",
        "  'max_features': 4,\n",
        "  'min_samples_leaf': 15,\n",
        "  'min_samples_split': 15,\n",
        "  'n_estimators': 20},\n",
        " 'validation score': 0.8448705278585762}\n",
        "thrid_classifier2={'Cross-val score': 0.8846250315039053,\n",
        "  'classifier': 'RandomForestClassifier',\n",
        "  'parameters': {'criterion': 'entropy',\n",
        "   'max_depth': 14,\n",
        "   'max_features': 'auto',\n",
        "   'min_samples_leaf': 2,\n",
        "   'min_samples_split': 6,\n",
        "   'n_estimators': 45},\n",
        "  'validation score': 0.8835185140616291}\n",
        "thrid_classifier3={'Cross-val score': 0.8596536275262364,\n",
        " 'classifier': 'RandomForestClassifier',\n",
        " 'parameters': {'criterion': 'gini',\n",
        "  'max_depth': 10,\n",
        "  'max_features': 'auto',\n",
        "  'min_samples_leaf': 5,\n",
        "  'min_samples_split': 10,\n",
        "  'n_estimators': 30},\n",
        " 'validation score': 0.8552454210104643}\n",
        "thrid_classifier4={'Cross-val score': 0.8872052232768184,\n",
        " 'classifier': 'ExtraTreesClassifier',\n",
        " 'parameters': {'criterion': 'entropy',\n",
        "  'max_depth': 15,\n",
        "  'max_features': 15,\n",
        "  'min_samples_leaf': 16,\n",
        "  'min_samples_split': 16,\n",
        "  'n_estimators': 65},\n",
        " 'validation score': 0.885587100775846}\n",
        "thrid_classifier5={'Cross-val score': 0.8612379579229952,\n",
        " 'classifier': 'RandomForestClassifier',\n",
        " 'parameters': {'criterion': 'entropy',\n",
        "  'max_depth': 10,\n",
        "  'max_features': 'auto',\n",
        "  'min_samples_leaf': 10,\n",
        "  'min_samples_split': 5,\n",
        "  'n_estimators': 50},\n",
        " 'validation score': 0.864605381012056}\n",
        "fourth_classifier={'Cross-val score': 0.8956732554663308,\n",
        " 'classifier': 'BaggingClassifier',\n",
        " 'parameters': {'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini',\n",
        "                         max_depth=None, max_features=None, max_leaf_nodes=None,\n",
        "                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                         min_samples_leaf=1, min_samples_split=2,\n",
        "                         min_weight_fraction_leaf=0.0,\n",
        "                         random_state=None, splitter='best'),\n",
        "  'n_estimators': 30,\n",
        "  'oob_score': True,\n",
        "  'random_state': 1},\n",
        " 'validation score': 0.9280463385108305}\n",
        "fifth_classifier={'Cross-val score': 0.8976371765417721,\n",
        " 'classifier': 'BaggingClassifier',\n",
        " 'parameters': {'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='entropy',\n",
        "                         max_depth=20, max_features='sqrt', max_leaf_nodes=None,\n",
        "                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                         min_samples_leaf=5, min_samples_split=30,\n",
        "                         min_weight_fraction_leaf=0.0,   random_state=None, splitter='best'),\n",
        "  'n_estimators': 45,\n",
        "  'oob_score': False,\n",
        "  'random_state': 1},\n",
        " 'validation score': 0.9073699757159257}\n",
        "\n",
        "six_classifier={'Cross-val score': 0.8996358063279757,\n",
        " 'classifier': 'AdaBoostClassifier',\n",
        " 'parameters': {'algorithm': 'SAMME',\n",
        "  'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='entropy',\n",
        "                         max_depth=20, max_features='sqrt', max_leaf_nodes=None,\n",
        "                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                         min_samples_leaf=5, min_samples_split=30,\n",
        "                         min_weight_fraction_leaf=0.0, \n",
        "                         random_state=None, splitter='best'),\n",
        "  'learning_rate': 0.07293190523422614,\n",
        "  'n_estimators': 40},\n",
        " 'validation score': 0.9128871828299814}\n",
        "seven_classifier={'Cross-val score': 0.8774978395308153,\n",
        " 'classifier': 'XGBClassifier',\n",
        " 'parameters': {'max_depth': 10,\n",
        "  'min_child_weight': 8,\n",
        "  'n_estimators': 15,\n",
        "  'subsample': 0.28517758976174756},\n",
        " 'validation score': 0.8781323643924608}\n",
        "egith_classifier={'Cross-val score': 0.9012510175710758,\n",
        "  'classifier': 'LGBMClassifier',\n",
        "  'parameters': {'colsample_bytree': 0.5,\n",
        "   'learning_rate': 0.09,\n",
        "   'max_bin': 80,\n",
        "   'max_depth': 10,\n",
        "   'metric': 'binary_error',\n",
        "   'min_child_samples': 80,\n",
        "   'n_iter_no_change': 10,\n",
        "   'num_iterations': 1000,\n",
        "   'num_leave': 10,\n",
        "   'reg_alpha': 50,\n",
        "   'reg_lambda': 5,\n",
        "   'scale_pos_weight': 50.0,\n",
        "   'subsample': 0.8},\n",
        "  'validation score': 0.9021447915521081}\n",
        "\n",
        "pipe1 = make_pipeline(category_encoders.cat_boost.CatBoostEncoder(),XGBClassifier(**seven_classifier['parameters']))\n",
        "pipe2 = make_pipeline(category_encoders.james_stein.JamesSteinEncoder(),RandomForestClassifier(**thrid_classifier['parameters']))  \n",
        "pipe3 = make_pipeline(category_encoders.m_estimate.MEstimateEncoder(),ExtraTreesClassifier(**Fr_classifier['parameters'])) \n",
        "pipe6 = make_pipeline(category_encoders.ordinal.OrdinalEncoder(),lgb.LGBMClassifier(**egith_classifier['parameters'])) \n",
        "pipe4 = make_pipeline(category_encoders.target_encoder.TargetEncoder(),HistGradientBoostingClassifier(**Se_classifier['parameters']))\n",
        "pipe5 = make_pipeline(category_encoders.woe.WOEEncoder(), BaggingClassifier(**fourth_classifier['parameters']) )\n",
        "pipe7 = make_pipeline(category_encoders.cat_boost.CatBoostEncoder(), AdaBoostClassifier(**six_classifier['parameters']) )\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe5,pipe6,pipe7], meta_classifier=LogisticRegression()) \n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test2, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test2, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA4P3yftv5VC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Fr_classifier={'Cross-val score': 0.8755075622861487,\n",
        " 'classifier': 'ExtraTreesClassifier',\n",
        " 'parameters': {'criterion': 'entropy',\n",
        "  'max_depth': 15,\n",
        "  'max_features': 13,\n",
        "  'min_samples_leaf': 4,\n",
        "  'min_samples_split': 4,\n",
        "  'n_estimators': 13},\n",
        " 'validation score': 0.884938764808794}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72q7zxSvwQK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Fr_classifier['parameters']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT1x5xQ-vWFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Se_classifier={'Cross-val score': 0.8273926245659394,\n",
        " 'classifier': 'RandomForestClassifier',\n",
        " 'parameters': {'criterion': 'gini',\n",
        "  'max_depth': 7,\n",
        "  'max_features': 10,\n",
        "  'min_samples_leaf': 2,\n",
        "  'min_samples_split': 3,\n",
        "  'n_estimators': 23},\n",
        " 'validation score': 0.7973701565847332}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jb9jiievacj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Thrid_classifier={'Cross-val score': 0.8268064630182487,\n",
        " 'classifier': 'LinearDiscriminantAnalysis',\n",
        " 'parameters': {},\n",
        " 'validation score': 0.827194350739109}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWLcgpDlvecC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Four_classifier={'Cross-val score': 0.8761819849027198,\n",
        " 'classifier': 'XGBClassifier',\n",
        " 'parameters': {'max_depth': 7,\n",
        "  'min_child_weight': 1,\n",
        "  'n_estimators': 38,\n",
        "  'subsample': 0.8479101254812229},\n",
        " 'validation score': 0.8770024548105813}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT7GCH_evh_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fith_classifier={'Cross-val score': 0.8931760781045686,\n",
        " 'classifier': 'LGBMClassifier',\n",
        " 'parameters': {'colsample_bytree': 1.0,\n",
        "  'max_depth': 9,\n",
        "  'min_child_samples': 70,\n",
        "  'num_leave': 10,\n",
        "  'reg_alpha': 5,\n",
        "  'reg_lambda': 20,\n",
        "  'scale_pos_weight': 50.0,\n",
        "  'subsample': 1.0},\n",
        " 'validation score': 0.8944420319608248}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOeAy9aqvmT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "six_classifier={'Cross-val score': 0.786171897798788,\n",
        " 'classifier': 'QuadraticDiscriminantAnalysis',\n",
        " 'parameters': {},\n",
        " 'validation score': 0.787235957146868}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoY5YxNhSY0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a8={'ExtraTreesClassifier': {'criterion': 'gini',\n",
        "  'max_depth': 7,\n",
        "  'max_features': 7,\n",
        "  'min_samples_leaf': 4,\n",
        "  'min_samples_split': 4,\n",
        "  'n_estimators': 19},\n",
        " 'LGBMClassifier': {'colsample_bytree': 1.0,\n",
        "  'max_depth': 7,\n",
        "  'min_child_samples': 80,\n",
        "  'num_leave': 50,\n",
        "  'reg_alpha': 0,\n",
        "  'reg_lambda': 10,\n",
        "  'scale_pos_weight': 70.0,\n",
        "  'subsample': 1.0},\n",
        " 'RandomForestClassifier': {'criterion': 'entropy',\n",
        "  'max_depth': 5,\n",
        "  'max_features': 'sqrt',\n",
        "  'min_samples_leaf': 3,\n",
        "  'min_samples_split': 2,\n",
        "  'n_estimators': 30},\n",
        " 'XGBClassifier': {'max_depth': 13,\n",
        "  'min_child_weight': 3,\n",
        "  'subsample': 0.9406880083709813}}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQZrDLCO4tzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "{'ExtraTreesClassifier': {'criterion': 'gini',\n",
        "  'max_depth': 7,\n",
        "  'max_features': 7,\n",
        "  'min_samples_leaf': 4,\n",
        "  'min_samples_split': 4,\n",
        "  'n_estimators': 19},\n",
        " 'LGBMClassifier': {'colsample_bytree': 1.0,\n",
        "  'max_depth': 7,\n",
        "  'min_child_samples': 80,\n",
        "  'num_leave': 50,\n",
        "  'reg_alpha': 0,\n",
        "  'reg_lambda': 10,\n",
        "  'scale_pos_weight': 70.0,\n",
        "  'subsample': 1.0},\n",
        " 'RandomForestClassifier': {'criterion': 'entropy',\n",
        "  'max_depth': 5,\n",
        "  'max_features': 'sqrt',\n",
        "  'min_samples_leaf': 3,\n",
        "  'min_samples_split': 2,\n",
        "  'n_estimators': 30},\n",
        " 'XGBClassifier': {'max_depth': 13,\n",
        "  'min_child_weight': 3,\n",
        "  'subsample': 0.9406880083709813}}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc6sc6QfbVhx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier())\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier())\n",
        "pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier())\n",
        "pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe6], meta_classifier=LogisticRegression())\n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test2, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test2, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxWwikwQVEhi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier(**a8['RandomForestClassifier']))  \n",
        "pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier(**a8['ExtraTreesClassifier'])) \n",
        "pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier(**a8['LGBMClassifier'])) \n",
        "pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe2,pipe3,pipe4,pipe6], meta_classifier=LogisticRegression()) \n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test2, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_NjQ3a2n3HR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "for clf, label in zip([list(sclf2.named_classifiers.keys()),sclf2],[list(a8.keys())]):\n",
        "  for i in range(len(list(a8.keys()))):\n",
        "    if list(a8.keys())[i]  in label:\n",
        "      if (type((sclf2.named_classifiers[clf[0]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe1 = make_pipeline(Accepted[0][1](),\n",
        "                                XGBClassifier(**a8[(type((sclf2.named_classifiers[clf[0]][1])).__name__)]))\n",
        "      if (type((sclf2.named_classifiers[clf[1]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe2 = make_pipeline(Accepted[1][1](),\n",
        "                                RandomForestClassifier(**a8[(type((sclf2.named_classifiers[clf[1]][1])).__name__)]))  \n",
        "      if (type((sclf2.named_classifiers[clf[2]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe3 = make_pipeline(Accepted[2][1](),\n",
        "                                ExtraTreesClassifier(**a8[(type((sclf2.named_classifiers[clf[2]][1])).__name__)])) \n",
        "      if (type((sclf2.named_classifiers[clf[5]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe6 = make_pipeline(Accepted[5][1](),\n",
        "                                lgb.LGBMClassifier(**a8[(type((sclf2.named_classifiers[clf[5]][1])).__name__)])) \n",
        "    else:  \n",
        "            pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "            pipe5 = make_pipeline(Accepted[4][1](),QuadraticDiscriminantAnalysis())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe5,pipe6], meta_classifier=LogisticRegression()) \n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test2, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test2, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmXql2VSupkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier(**a8['XGBClassifier']))\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier(**a8['RandomForestClassifier']))  \n",
        "pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier(**a8['ExtraTreesClassifier'])) \n",
        "pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier(**a8['LGBMClassifier'])) \n",
        "pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "pipe5 = make_pipeline(Accepted[4][1](),QuadraticDiscriminantAnalysis())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe5,pipe6], meta_classifier=LogisticRegression()) \n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVt5r7R6nsh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tsh4A3eZtNDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list(pipe1.named_steps.keys())[1],list(a8.keys())[0].lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyjoLr3s4OzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yS_rBX750WcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for clf, label in zip([list(sclf2.named_classifiers.keys()),sclf2],[list(a8.keys())]):\n",
        "  for i in range(len(list(a8.keys()))):\n",
        "    if list(a8.keys())[i]  in label:\n",
        "      if (type((sclf2.named_classifiers[clf[0]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier(**a8[(type((sclf2.named_classifiers[clf[0]][1])).__name__)]))\n",
        "          pipe2\n",
        "      if (type((sclf2.named_classifiers[clf[1]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier(**a8[(type((sclf2.named_classifiers[clf[1]][1])).__name__)]))  \n",
        "          pipe2   \n",
        "      if (type((sclf2.named_classifiers[clf[2]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier(**a8[(type((sclf2.named_classifiers[clf[2]][1])).__name__)])) \n",
        "      if (type((sclf2.named_classifiers[clf[3]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier(**a8[(type((sclf2.named_classifiers[clf[5]][1])).__name__)])) \n",
        "    else:  \n",
        "            pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "            pipe5 = make_pipeline(Accepted[4][1](),QuadraticDiscriminantAnalysis())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe5,pipe6], meta_classifier=LogisticRegression())  \n",
        "model2=sclf2.fit(X_train, y_train)\n",
        "y_true3, y_pred3  =y_test, model2.predict(X_test)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train, y_train))\n",
        "clf2_val_score = roc_auc_score(y_test, sclf2.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVFICfs6qliC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for clf, label in zip([list(sclf2.named_classifiers.keys()),sclf2],[list(a8.keys())]):\n",
        "  if label[0].lower()==list(pipe1.named_steps.keys())[1]:\n",
        "    print(label)\n",
        "\n",
        "pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier())\n",
        "\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier())\n",
        "pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier())\n",
        "pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "pipe5 = make_pipeline(Accepted[4][1](),QuadraticDiscriminantAnalysis())\n",
        "pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiGbF94Mg6XY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sclf2.named_classifiers['pipeline-1'][1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaFe4_nXdj96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier(**a8['ExtraTreesClassifier'] )  )\n",
        "pipe3\n",
        "a8.keys(),list(sclf2.named_classifiers.keys())\n",
        "a8['XGBClassifier']\n",
        "pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier(**a8['XGBClassifier']))\n",
        "pipe1,a8['XGBClassifier']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHU7-dKPdJKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list(pipe1.named_steps.keys())[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5TMP6e7ab6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i,k in enumerate(list(sclf2.named_classifiers.keys())):\n",
        "  for j,val in enumerate(list(a8.keys())):\n",
        "    if type((sclf2.named_classifiers[k][1])).__name__ ==list(a8.keys())[j]:\n",
        "        print(list(a8.values())[j])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQa1FPcy3ucp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "{'ExtraTreesClassifier': {'criterion': 'gini', 'max_depth': 7, 'max_features': 7, 'min_samples_leaf': 4, 'min_samples_split': 4, 'n_estimators': 19}, \n",
        "'LGBMClassifier': {'colsample_bytree': 1.0, 'max_depth': 7, 'min_child_samples': 80, 'num_leave': 50, 'reg_alpha': 0, 'reg_lambda': 10, 'scale_pos_weight': 70.0, 'subsample': 1.0},\n",
        "'RandomForestClassifier': {'criterion': 'entropy', 'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 30},\n",
        "'XGBClassifier': {'max_depth': 13, 'min_child_weight': 3, 'subsample': 0.9406880083709813}}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwGKr5nNdGcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from hyperopt import hp, tpe, fmin, Trials, STATUS_OK\n",
        "from sklearn import datasets\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble.forest import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline \n",
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "import numpy as np\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "param_hyperopt={\n",
        "  'RandomForestClassifier': \n",
        "            {\n",
        "            'max_depth': ho_scope.int(hp.quniform('c2_max_depth', 5, 7, 1)),\n",
        "            'n_estimators': ho_scope.int(hp.quniform('c2_n_estimators', 5, 30, 1)),\n",
        "            'max_features': hp.choice('c2_max_features', ['auto', 'sqrt']),\n",
        "            'criterion': hp.choice('c2_criterion', ['gini', 'entropy']),\n",
        "            'min_samples_split': ho_scope.int(hp.quniform('c2_min_samples_split', 2, 5,1)),\n",
        "            'min_samples_leaf': ho_scope.int(hp.quniform('c2_min_samples_leaf', 2, 5,1))\n",
        "                          \n",
        "         },\n",
        "  'ExtraTreesClassifier':\n",
        "            {\n",
        "                'max_depth': ho_scope.int(hp.quniform('c4_max_depth', 5, 20, 1)),\n",
        "                'n_estimators': ho_scope.int(hp.quniform('c4_n_estimators', 5, 30, 1)),\n",
        "                'max_features': ho_scope.int(hp.quniform('c4_max_features', 5, 20, 1)),\n",
        "                'criterion': hp.choice('c4_criterion', ['gini', 'entropy']),\n",
        "                'min_samples_split': ho_scope.int(hp.quniform('c4_min_samples_split', 2, 5,1)),\n",
        "                'min_samples_leaf': ho_scope.int(hp.quniform('c4_min_samples_leaf', 2, 5,1))\n",
        "       },\n",
        "  'XGBClassifier':\n",
        "      {\n",
        "           'max_depth': hp.choice(\"x_max_depth\", np.arange(5, 25, dtype=int)),\n",
        "           'min_child_weight': hp.choice ('x_min_child',np.arange(1, 10, dtype=int)),\n",
        "           'subsample': hp.uniform ('x_subsample', 0.8, 1)\n",
        "       },\n",
        "  'LGBMClassifier':\n",
        "       {\n",
        "        'max_depth': ho_scope.int(hp.quniform('c6_max_depth', 3, 10, 1)),\n",
        "        'num_leave': ho_scope.int(hp.quniform('c6_num_leaves', 5, 50, 5)),\n",
        "        'min_child_samples': ho_scope.int(hp.quniform('c6_min_child_samples', 50, 100, 10)),\n",
        "        'scale_pos_weight': ho_scope.float(hp.quniform('c6_scale_pos_weight', 50, 100, 10)),\n",
        "        'subsample': ho_scope.float(hp.quniform('c6_subsample', 0.6, 0.9, 1)),\n",
        "        'colsample_bytree': ho_scope.float(hp.quniform('c6_colsample_bytree', 0.6, 0.9, 1)),\n",
        "        'reg_lambda': hp.choice(\"c6_reg_lambda\",[0, 1e-1, 1, 5, 10, 20, 50]),\n",
        "        'reg_alpha': hp.choice(\"c6_reg_alpha\",[0, 1e-1, 1, 5, 10, 20, 50])\n",
        "       }         \n",
        "       \n",
        "}  \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "def f_clf8(params):\n",
        "   model1 =Pipeline(steps=[('encoder',Accepted[0][1]()),\n",
        "             ('c4',ExtraTreesClassifier(**f_unpack_dict(params['ExtraTreesClassifier'])))])\n",
        "   model2 =Pipeline(steps=[('encoder',Accepted[1][1]()),('LGBMClassifier', lgb.LGBMClassifier(**f_unpack_dict(params['LGBMClassifier'])))]) \n",
        "   model3 =Pipeline(steps=[('encoder',Accepted[2][1]()),('XGBClassifier', xgb.XGBClassifier(**f_unpack_dict(params['XGBClassifier'])))])   \n",
        "   model4 =Pipeline(steps=[('encoder',Accepted[3][1]()),('LinearDiscriminantAnalysis', LinearDiscriminantAnalysis())]) \n",
        "   model5 =Pipeline(steps=[('encoder',Accepted[4][1]()),\n",
        "             ('c2',RandomForestClassifier(**f_unpack_dict(params['RandomForestClassifier'])))]) \n",
        "   model6 =Pipeline(steps=[('encoder',Accepted[5][1]()),('QuadraticDiscriminantAnalysis', QuadraticDiscriminantAnalysis())])\n",
        "   sclf2 = StackingClassifier(classifiers=[model1,model2,model3,model4,model5,model6],meta_classifier=LogisticRegression())\n",
        "   return sclf2\n",
        "  \n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf8(params)\n",
        "    #clf = RandomForestClassifier(**params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf8 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                param_hyperopt, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "\n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval\n",
        "from hyperopt.pyll import scope as ho_scope\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.utils import shuffle\n",
        "clf8 = f_clf8(space_eval(param_hyperopt, best_clf8)).fit(X_train, y_train)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf8_val_score = roc_auc_score(y_test, clf8.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf8_val_score))\n",
        "print('Best parameters:')\n",
        "print(space_eval(param_hyperopt, best_clf8))\n",
        "oo=-trials.best_trial['result']['loss']\n",
        "oo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd05nQ3EojhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from hyperopt import hp, tpe, fmin, Trials, STATUS_OK\n",
        "from sklearn import datasets\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble.forest import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline \n",
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "import numpy as np\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "\n",
        "param_hyperopt= {\n",
        "    'AdaBoostClassifier':\n",
        "     {\n",
        "          'learning_rate': hp.loguniform('c5_learning_rate', np.log(0.01), np.log(1)),\n",
        "           'n_estimators': ho_scope.int(hp.quniform('c5_n_estimators', 5, 30, 1)),\n",
        "           'algorithm': hp.choice('c5_algorithm',[\"SAMME\"])  \n",
        "       }    \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "def f_clf7(params):\n",
        "   model2 =Pipeline(steps=[('encoder',Accepted[5][1]()),\n",
        "             ('c5',AdaBoostClassifier(**f_unpack_dict(params['AdaBoostClassifier'])))]) \n",
        "   return model2\n",
        "        \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf7(params)\n",
        "    #clf = RandomForestClassifier(**params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf7 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                 param_hyperopt, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "\n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval\n",
        "from hyperopt.pyll import scope as ho_scope\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.utils import shuffle\n",
        "clf7 = f_clf7(space_eval(param_hyperopt, best_clf7)).fit(X_train2, y_train2)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf7_val_score = roc_auc_score(y_test, clf7.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf7_val_score))\n",
        "print('Best parameters:')\n",
        "print(space_eval(param_hyperopt, best_clf7))\n",
        "cc=-trials.best_trial['result']['loss']\n",
        "cc"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}