{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarah127/udacity-data-analysis-with-r/blob/master/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgzROkNIH4do",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAV8exRXypkt",
        "colab_type": "code",
        "outputId": "892985cc-a13f-4a1d-af25-8efa315494f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanizeii\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh5NCoc8fsSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1R6gDJMND6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP8zG65mYk86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD03caqDhFhA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "from  pyspark import SparkConf\n",
        "#conf = SparkConf().setMaster(\"local\").setAppName(\"My app\").set(\"spark.executor.memory\", \"1g\")\n",
        "#sc = SparkContext()\n",
        "# ===============================read from csv==============================================\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "from pyspark.sql.types import StructType,StructField,StringType,TimestampType,IntegerType,DoubleType,LongType,FloatType\n",
        "Df5_Schema = StructType([StructField(\"Airline\", StringType(),True),StructField(\"Flight\", DoubleType()),\n",
        "StructField(\"AirportFrom\",StringType()),StructField(\"AirportTo\", StringType()),StructField(\"DayOfWeek\", IntegerType()),\n",
        "StructField(\"Time\",IntegerType()), StructField(\"Length\", IntegerType()),StructField(\"codrna_X1\",DoubleType()),\n",
        "StructField(\"codrna_X2\",DoubleType()),StructField(\"codrna_X3\",DoubleType()),StructField(\"codrna_X4\", DoubleType()),\n",
        "StructField(\"codrna_X5\", DoubleType()),StructField(\"codrna_X6\", DoubleType()),StructField(\"codrna_X7\", DoubleType()),\n",
        "StructField(\"codrna_X8\", DoubleType()),StructField(\"age\", IntegerType()),StructField(\"workclass\", StringType()),\n",
        "StructField(\"fnlwgt\", IntegerType()),StructField(\"education\", StringType()),StructField(\"education-num\", IntegerType()),\n",
        "StructField(\"marital-status\", StringType()),StructField(\"occupation\", StringType()),StructField(\"relationship\", StringType()),\n",
        "StructField(\"race\", StringType()),StructField(\"sex\", StringType()),StructField(\"capitalgain\", IntegerType()),\n",
        "StructField(\"capitalloss\", IntegerType()),StructField(\"hoursperweek\", IntegerType()),StructField(\"native-country\", StringType()),\n",
        "StructField(\"Delay\", IntegerType())])\n",
        "CSV_2007= '/content/drive/My Drive/AirlinesCodrnaAdult.csv'\n",
        "df5 =spark.read.schema(Df5_Schema).option(\"header\",\"true\").option(\"mode\", \"DROPMALFORMED\").csv(CSV_2007)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQvods40rPBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "names = [f.name for f in df5.schema.fields]\n",
        "dataTypes = [f.dataType for f in df5.schema.fields]\n",
        "ls=list(zip(names,dataTypes))\n",
        "str_cols=[]\n",
        "for i in range(len(ls)):\n",
        "     if type(ls[i][1])==StringType and len(df5.select(df5[ls[i][0]]).distinct().collect())<=20:\n",
        "         str_cols.append([i,ls[i][0]]) \n",
        "strings=[]\n",
        "for i in range(len(str_cols)):\n",
        "     strings.append(str_cols[i][1])  \n",
        "\n",
        "#the process of convertiong catogerical columns into numeric is not included in the machine learning operation \n",
        "#it will be left for future improvements !!!ان شاء الله      \n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df5) for column in strings]\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "df6 = pipeline.fit(df5).transform(df5)\n",
        "df6=df6.drop(*strings)\n",
        "df6_pd=df6.toPandas()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIM7xkiDsRJI",
        "colab_type": "code",
        "outputId": "d880a09c-62e8-478f-88fa-840545f31448",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "! pip install category_encoders"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: category_encoders in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.25.3)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.17.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.22.1)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.5.1)\n",
            "Requirement already satisfied: statsmodels>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.10.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.4.1->category_encoders) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB96hBgnrflo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import f1_score\n",
        "import category_encoders as ce\n",
        "from sklearn.utils import resample\n",
        "\n",
        "encoder_list = [ce.backward_difference.BackwardDifferenceEncoder,ce.basen.BaseNEncoder,ce.binary.BinaryEncoder,\n",
        "                ce.cat_boost.CatBoostEncoder,ce.helmert.HelmertEncoder,\n",
        "                ce.james_stein.JamesSteinEncoder,ce.one_hot.OneHotEncoder,ce.leave_one_out.LeaveOneOutEncoder,\n",
        "                ce.m_estimate.MEstimateEncoder,ce.ordinal.OrdinalEncoder,\n",
        "                ce.sum_coding.SumEncoder,ce.target_encoder.TargetEncoder,ce.woe.WOEEncoder]\n",
        "#numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "#categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X = df6_pd.drop('Delay', axis=1)\n",
        "y = df6_pd['Delay']\n",
        "#le = preprocessing.LabelEncoder()\n",
        "#label_encoder = le.fit(y)\n",
        "#y = label_encoder.transform(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "block_size=int(round(X_train.shape[0]/len(encoder_list)))\n",
        "n = block_size  #chunk row size\n",
        "list_X_train = [X_train[i:i+n] for i in range(0,X_train.shape[0],n)]\n",
        "list_y_train = [y_train[i:i+n] for i in range(0,y_train.shape[0],n)]\n",
        "list_X_test = [X_test[i:i+n] for i in range(0,X_test.shape[0],n)]\n",
        "list_y_test = [y_test[i:i+n] for i in range(0,y_test.shape[0],n)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "486lmPRrsNS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_numeric_dataset=[]\n",
        "intermidate=[]\n",
        "testing_numeric_dataset=[]\n",
        "for i,encoder in enumerate(encoder_list):\n",
        "  #(encoder(handle_unknown='ignore',cols=['AirportFrom','AirportTo','native-country'])).fit_transform(boot1[i])\n",
        "   training_numeric_dataset.append(encoder_list[i](cols=['AirportFrom','AirportTo','native-country']).fit(list_X_train[i],list_y_train[i]))\n",
        "   intermidate.append((training_numeric_dataset[i]).transform(list_X_train[i],list_y_train[i]))\n",
        "for i in range(len(list_X_test)):\n",
        "    for j,encoder in enumerate(encoder_list):\n",
        "        testing_numeric_dataset.append((training_numeric_dataset[j]).transform(list_X_test[i])) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KMjFk6msm2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xx=[]\n",
        "for i in range(len(encoder_list)):\n",
        "    #xx.append([i,min(intermidate[i].columns)] )\n",
        "    xx.append([encoder_list[i],len(intermidate[i].columns)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XrUFnRFs2ZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import category_encoders as ce\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "block_size=int(round(X_train2.shape[0]/12))\n",
        "n = block_size  #chunk row size\n",
        "list_X_train2 = [X_train2[i:i+n] for i in range(0,X_train2.shape[0],n)]\n",
        "list_y_train2 = [y_train2[i:i+n] for i in range(0,y_train2.shape[0],n)]\n",
        "list_X_test2 = [X_test2[i:i+n] for i in range(0,X_test2.shape[0],n)]\n",
        "list_y_test2 = [y_test2[i:i+n] for i in range(0,y_test2.shape[0],n)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAV2AZs2tE00",
        "colab_type": "code",
        "outputId": "c8df39d4-3543-4533-e978-57251e34debf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "import category_encoders as ce\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from heapq import nlargest\n",
        "\n",
        "encoder_list = [ce.backward_difference.BackwardDifferenceEncoder(),ce.basen.BaseNEncoder(),\n",
        "                ce.binary.BinaryEncoder(),ce.cat_boost.CatBoostEncoder(),\n",
        "                ce.helmert.HelmertEncoder(),ce.james_stein.JamesSteinEncoder(),\n",
        "                ce.one_hot.OneHotEncoder(),ce.leave_one_out.LeaveOneOutEncoder()\n",
        "                ,ce.m_estimate.MEstimateEncoder(),ce.ordinal.OrdinalEncoder(),ce.sum_coding.SumEncoder(),\n",
        "                ce.target_encoder.TargetEncoder(),ce.woe.WOEEncoder()]\n",
        "my_dict={}\n",
        "for encoder in encoder_list:\n",
        "    model = Pipeline(steps=[('preprocessor', encoder),('classifier',RandomForestClassifier())])               \n",
        "    modelOpt = model.fit(X_train, y_train)\n",
        "    y_true,y_pred =y_test, model.predict(X_test)\n",
        "    clf2_val_score = roc_auc_score(y_test, modelOpt.predict_proba(X_test)[:, 1])\n",
        "    my_dict[type(encoder)]= [modelOpt.score(X_train, y_train),clf2_val_score]\n",
        "\n",
        "num=int(round(len(encoder_list)*0.5))    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-655b5e9d21ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoder_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'preprocessor'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'classifier'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mmodelOpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mclf2_val_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelOpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n\u001b[1;32m    353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    381\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     n_samples_bootstrap=n_samples_bootstrap)\n\u001b[0;32m--> 383\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    878\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    365\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNP-3BVuIy8l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z={}\n",
        "for i in range(len(my_dict)):\n",
        "   z[list(my_dict.keys())[i]]=list(my_dict.items())[i][1][0] \n",
        "Highest1 = nlargest(num, z, key = z.get)  \n",
        "\n",
        "zz={}\n",
        "for i in range(len(my_dict)):\n",
        "   zz[list(my_dict.keys())[i]]=list(my_dict.items())[i][1][1] \n",
        "Highest2 = nlargest(num, zz, key = zz.get)  \n",
        "Highest3=list(set(Highest1+Highest2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN3Vj_8VJDhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Accepted=[]\n",
        "keys = my_dict.keys()\n",
        "for i,each in enumerate(xx):\n",
        " for j,val in enumerate(Highest3): \n",
        "   if xx[i][0]==val and  xx[i][1]==len(X_train.columns)  : \n",
        "           #  Accepted.append(\"%s :%s: %s\" % (i,type(each), my_dict.get(each))) \n",
        "           Accepted.append((i,val))\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20rpUs7-EfrN",
        "colab_type": "code",
        "outputId": "00d78ce4-5fb4-47fa-b8ae-37e17c253015",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "Accepted      "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(3, category_encoders.cat_boost.CatBoostEncoder),\n",
              " (5, category_encoders.james_stein.JamesSteinEncoder),\n",
              " (8, category_encoders.m_estimate.MEstimateEncoder),\n",
              " (9, category_encoders.ordinal.OrdinalEncoder),\n",
              " (11, category_encoders.target_encoder.TargetEncoder),\n",
              " (12, category_encoders.woe.WOEEncoder)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AWt8EE1Jl76",
        "colab_type": "code",
        "outputId": "fc230191-4d00-4507-d988-385ebfee5ca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "pipe1 = make_pipeline(Accepted[0][1](),RandomForestClassifier())\n",
        "pipe2 = make_pipeline(Accepted[1][1](), KNeighborsClassifier())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2], meta_classifier=LogisticRegression())\n",
        "\n",
        "model2=sclf2.fit(X_train, y_train)\n",
        "y_true3, y_pred3  =y_test, model2.predict(X_test)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train, y_train))\n",
        "clf2_val_score = roc_auc_score(y_test, sclf2.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P6auG6ahBf-",
        "colab_type": "code",
        "outputId": "ac10a693-047d-4a76-ee7d-90d0a37bc0e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier())\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier())\n",
        "pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier())\n",
        "pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "pipe5 = make_pipeline(Accepted[4][1](),AdaBoostClassifier())\n",
        "pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe5,pipe6], meta_classifier=LogisticRegression())\n",
        "\n",
        "model2=sclf2.fit(X_train, y_train)\n",
        "y_true3, y_pred3  =y_test, model2.predict(X_test)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train, y_train))\n",
        "clf2_val_score = roc_auc_score(y_test, sclf2.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.78      0.77     94227\n",
            "           1       0.82      0.80      0.81    121131\n",
            "\n",
            "    accuracy                           0.79    215358\n",
            "   macro avg       0.79      0.79      0.79    215358\n",
            "weighted avg       0.79      0.79      0.79    215358\n",
            "\n",
            "StackingClassifier score: 0.912733\n",
            "Cross-val score: 0.86497\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZZsULnlhHEM",
        "colab_type": "code",
        "outputId": "727c340d-7afb-48ac-a20b-85dd5bb70801",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier())\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier())\n",
        "pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier())\n",
        "pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "#pipe5 = make_pipeline(Accepted[4][1](),AdaBoostClassifier())\n",
        "pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe6], meta_classifier=LogisticRegression())\n",
        "\n",
        "model2=sclf2.fit(X_train, y_train)\n",
        "y_true3, y_pred3  =y_test, model2.predict(X_test)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train, y_train))\n",
        "clf2_val_score = roc_auc_score(y_test, sclf2.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.78      0.77     94227\n",
            "           1       0.82      0.80      0.81    121131\n",
            "\n",
            "    accuracy                           0.79    215358\n",
            "   macro avg       0.79      0.79      0.79    215358\n",
            "weighted avg       0.79      0.79      0.79    215358\n",
            "\n",
            "StackingClassifier score: 0.912755\n",
            "Cross-val score: 0.86483\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuYcc4nnZkp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from hyperopt import fmin, hp, tpe, Trials, space_eval\n",
        "from hyperopt.pyll import scope as ho_scope\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.utils import shuffle\n",
        "clf1 = f_clf1(space_eval(param_hyperopt, best_clf3)).fit(X_train, y_train)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf1_val_score = roc_auc_score(y_test, clf1.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf1_val_score))\n",
        "print('Best parameters:')\n",
        "print(space_eval(param_hyperopt, best_clf3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyBAgEvxakfh",
        "colab_type": "code",
        "outputId": "b82d30a2-80bf-4dd6-c7bc-36d44c6485ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "y_true3, y_pred3  =y_test, model2.predict(X_test)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"RandomForestClassifier score: %f\" % clf1.score(X_train, y_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.79      0.77     94676\n",
            "           1       0.83      0.78      0.80    120682\n",
            "\n",
            "    accuracy                           0.79    215358\n",
            "   macro avg       0.78      0.79      0.78    215358\n",
            "weighted avg       0.79      0.79      0.79    215358\n",
            "\n",
            "RandomForestClassifier score: 0.731440\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "13701896-f9ea-4707-c543-e1927773389d",
        "id": "GoT4rgyqbNPr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "import warnings\n",
        "pipe1 = make_pipeline(Accepted[0][1](),RandomForestClassifier(n_estimators= 21,\n",
        "                max_depth= 7,min_samples_split= 4,min_samples_leaf= 4,max_features= 'sqrt'))\n",
        "pipe2 = make_pipeline(Accepted[1][1](),AdaBoostClassifier(DecisionTreeClassifier(max_depth=1)\n",
        ",n_estimators=28,learning_rate=0.8051561754791255)) \n",
        "pipe3 = make_pipeline(Accepted[2][1](),XGBClassifier())\n",
        "pipe4 = make_pipeline(Accepted[3][1](),ExtraTreesClassifier(criterion= 'entropy'))\n",
        "pipe5 = make_pipeline(Accepted[4][1](),GradientBoostingClassifier())\n",
        "pipe6 = make_pipeline(Accepted[5][1](),GaussianNB())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe5,pipe6],\n",
        "                           meta_classifier=LogisticRegression())\n",
        "\n",
        "model2=sclf2.fit(X_train, y_train)\n",
        "y_true3, y_pred3  =y_test, model2.predict(X_test)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"RandomForestClassifier score: %f\" % model2.score(X_train, y_train))\n",
        "clf2_val_score = roc_auc_score(y_test, sclf2.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.79      0.77     94676\n",
            "           1       0.83      0.78      0.80    120682\n",
            "\n",
            "    accuracy                           0.79    215358\n",
            "   macro avg       0.78      0.79      0.78    215358\n",
            "weighted avg       0.79      0.79      0.79    215358\n",
            "\n",
            "RandomForestClassifier score: 0.913660\n",
            "Cross-val score: 0.84756\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APRcNVp5qCWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive \n",
        "from google.colab import auth \n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7jAi0SyqJ6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGO4odcgVfWt",
        "colab_type": "code",
        "outputId": "7644bcf2-56b9-4f96-af4f-60e295c4b311",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Compare Algorithms\n",
        "import pandas\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis,QuadraticDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.pipeline import Pipeline \n",
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "# explicitly require this experimental feature\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
        "# now you can import normally from ensemble\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import xgboost\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import lightgbm as lgb\n",
        "# load dataset\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "#le = preprocessing.LabelEncoder()\n",
        "#label_encoder = le.fit(y)\n",
        "#y = label_encoder.transform(y)\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2)\n",
        "block_size=int(round(X_train2.shape[0]/13))\n",
        "n = block_size  #chunk row size\n",
        "list_X_train = [X_train2[i:i+n] for i in range(0,X_train2.shape[0],n)]\n",
        "list_y_train = [y_train2[i:i+n] for i in range(0,y_train2.shape[0],n)]\n",
        "list_X_test = [X_test2[i:i+n] for i in range(0,X_test2.shape[0],n)]\n",
        "list_y_test = [y_test2[i:i+n] for i in range(0,y_test2.shape[0],n)]\n",
        "test_data = lgb.Dataset(list_X_test[0], label=list_y_test[0])\n",
        "# Build the encoder\n",
        "# prepare configuration for cross validation test harness\n",
        "seed = 7\n",
        "# prepare models\n",
        "models = []\n",
        "parameters = {\n",
        "    'application': 'binary','objective': 'binary','metric': 'auc','is_unbalance': 'true',\n",
        "    'boosting': 'gbdt','num_leaves': 31,'feature_fraction': 0.5,'bagging_fraction': 0.5,\n",
        "    'bagging_freq': 20,'learning_rate': 0.05,'verbose': 0\n",
        "}\n",
        "fit_params={\"early_stopping_rounds\":10,\"eval_metric\" : 'auc',\"eval_set\" : [(X_test,y_test)],\n",
        "            'eval_names': ['valid'],'verbose': 100,\n",
        "            'feature_name': 'auto', # that's actually the default\n",
        "            'categorical_feature': 'auto' # that's actually the default\n",
        "           }\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                               ('RandomForestClassifier',RandomForestClassifier(n_estimators=20))])) \n",
        "                               .fit(list_X_train[0], list_y_train[0])) \n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                               ('AdaBoostClassifier',AdaBoostClassifier(n_estimators=25))]))\n",
        "                               .fit(list_X_train[1], list_y_train[1])) \n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                       ('HistGradientBoostingClassifier', HistGradientBoostingClassifier())])).fit(list_X_train[2], list_y_train[2]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                              ('XGBClassifier',XGBClassifier(max_depth=2,n_estimators=10,objective='binary:logistic'))]))\n",
        "                            .fit(list_X_train[3], list_y_train[3]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                              ('ExtraTreesClassifier',ExtraTreesClassifier(criterion= 'entropy'))]))\n",
        "                               .fit(list_X_train[4], list_y_train[4]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "              ('lightgbm',lgb.LGBMClassifier(n_estimators=5 , num_leaves= 15,\n",
        "           max_depth=-1,colsample_bytree=0.9,subsample=0.9,learning_rate=0.1))])).fit(list_X_train[5], list_y_train[5]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                     ('LDA', LinearDiscriminantAnalysis())])).fit(list_X_train[6], list_y_train[6]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                     ('MLPClassifier',\n",
        "                      MLPClassifier(hidden_layer_sizes=(50,30,30), max_iter=5, alpha=0.0001,\n",
        "                     solver='sgd', verbose=10,  random_state=1))])).fit(list_X_train[7], list_y_train[7]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                     ('LR', LogisticRegression())])).fit(list_X_train[8], list_y_train[8]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                     ('QDA', QuadraticDiscriminantAnalysis())])).fit(list_X_train[9], list_y_train[9]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                      ('CART', DecisionTreeClassifier())])).fit(list_X_train[10], list_y_train[10]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                       ('NB', GaussianNB())])).fit(list_X_train[11], list_y_train[11]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "   ('BaggingClassifier',BaggingClassifier( DecisionTreeClassifier(random_state=42), n_estimators=100,\n",
        "    max_samples=100, bootstrap=True, n_jobs=-1, random_state=42,oob_score = True))])).fit(list_X_train[12], list_y_train[12]))\n",
        "\n",
        "\n",
        "   \n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "for name, model in enumerate(models):\n",
        "    #kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
        "    print('1')\n",
        "    cv_results = model_selection.cross_val_score(model, list_X_train[name], \n",
        "                            list_y_train[name], cv=5, scoring=scoring)\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "    print(msg)\n",
        "# boxplot algorithm comparison\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Algorithm Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 2238378401.67726231\n",
            "Iteration 3, loss = 2238370922.70406103\n",
            "Iteration 4, loss = 2238363443.75872326\n",
            "Iteration 5, loss = 2238355964.83891582\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "0: 0.779639 (0.001584)\n",
            "1\n",
            "1: 0.750845 (0.003918)\n",
            "1\n",
            "2: 0.795289 (0.002631)\n",
            "1\n",
            "3: 0.708741 (0.003972)\n",
            "1\n",
            "4: 0.779790 (0.002653)\n",
            "1\n",
            "5: 0.733686 (0.003593)\n",
            "1\n",
            "6: 0.745684 (0.000697)\n",
            "1\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 1435429285.64168787\n",
            "Iteration 3, loss = 1435425220.76746178\n",
            "Iteration 4, loss = 1435421155.90849543\n",
            "Iteration 5, loss = 1435417091.06200981\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 1910221322.75826144\n",
            "Iteration 3, loss = 1910215913.36123753\n",
            "Iteration 4, loss = 1910210503.98316240\n",
            "Iteration 5, loss = 1910205094.62138057\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 4221358289.52686882\n",
            "Iteration 3, loss = 4221346335.41921091\n",
            "Iteration 4, loss = 4221334381.34890795\n",
            "Iteration 5, loss = 4221322427.31345081\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 8194480371.85061455\n",
            "Iteration 3, loss = 8194457166.59933567\n",
            "Iteration 4, loss = 8194433961.41694450\n",
            "Iteration 5, loss = 8194410756.30125237\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 26896457637.52940750\n",
            "Iteration 3, loss = 26896381879.27381134\n",
            "Iteration 4, loss = 26896306121.23326874\n",
            "Iteration 5, loss = 26896230363.40713882\n",
            "7: 0.558855 (0.000034)\n",
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "8: 0.617108 (0.007406)\n",
            "1\n",
            "9: 0.723847 (0.003046)\n",
            "1\n",
            "10: 0.747268 (0.003695)\n",
            "1\n",
            "11: 0.612912 (0.002277)\n",
            "1\n",
            "12: 0.747314 (0.001747)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEVCAYAAADuAi4fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGSR_9eGDZB4",
        "colab_type": "code",
        "outputId": "cc6abfc6-46dc-4116-b2fe-72780e132373",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "last=[]\n",
        "for name, model in enumerate(models):\n",
        "  last.append([type(model[1]),results[name].mean()])\n",
        "last\n",
        "def Sort(sub_li): \n",
        "   return(sorted(sub_li, key = lambda x: x[1], reverse=True))     \n",
        "bb=(Sort(last))\n",
        "try1=[]\n",
        "for i in range(len(bb)):\n",
        "  if bb[i][1]>=0.70:\n",
        "     try1.append(bb[i])\n",
        "try1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier,\n",
              "  0.7952885494446024],\n",
              " [sklearn.ensemble._forest.ExtraTreesClassifier, 0.7797899416881368],\n",
              " [sklearn.ensemble._forest.RandomForestClassifier, 0.779638984631879],\n",
              " [sklearn.ensemble._weight_boosting.AdaBoostClassifier, 0.7508450187963523],\n",
              " [sklearn.ensemble._bagging.BaggingClassifier, 0.7473137884094563],\n",
              " [sklearn.tree._classes.DecisionTreeClassifier, 0.7472684962820098],\n",
              " [sklearn.discriminant_analysis.LinearDiscriminantAnalysis,\n",
              "  0.7456839266116535],\n",
              " [lightgbm.sklearn.LGBMClassifier, 0.7336864573589532],\n",
              " [sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis,\n",
              "  0.7238470094270895],\n",
              " [xgboost.sklearn.XGBClassifier, 0.708740898899267]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09XV0e_yx2rJ",
        "colab_type": "code",
        "outputId": "09dfa82b-760c-4dcc-a6ad-6a7c3d71bb8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "bb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier,\n",
              "  0.7945943590239859],\n",
              " [sklearn.ensemble._forest.ExtraTreesClassifier, 0.7808462675667986],\n",
              " [sklearn.ensemble._forest.RandomForestClassifier, 0.7805897845111422],\n",
              " [sklearn.ensemble._weight_boosting.AdaBoostClassifier, 0.7512525215403791],\n",
              " [sklearn.ensemble._bagging.BaggingClassifier, 0.7480078888675838],\n",
              " [sklearn.tree._classes.DecisionTreeClassifier, 0.747223233762471],\n",
              " [sklearn.discriminant_analysis.LinearDiscriminantAnalysis,\n",
              "  0.7454878038309398],\n",
              " [lightgbm.sklearn.LGBMClassifier, 0.7381382966693678],\n",
              " [sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis,\n",
              "  0.7266539484001128],\n",
              " [xgboost.sklearn.XGBClassifier, 0.7097669734676023],\n",
              " [sklearn.linear_model._logistic.LogisticRegression, 0.6158701938309009],\n",
              " [sklearn.naive_bayes.GaussianNB, 0.6143154689315227],\n",
              " [sklearn.neural_network._multilayer_perceptron.MLPClassifier,\n",
              "  0.5603495113294544]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz0WsbNDCAMr",
        "colab_type": "code",
        "outputId": "f9df4ae5-dc88-4727-94fe-03d514b7ff89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from hyperopt import hp, tpe, fmin, Trials, STATUS_OK\n",
        "from sklearn import datasets\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble.forest import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline \n",
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "# explicitly require this experimental feature\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
        "# now you can import normally from ensemble\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "import numpy as np\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "import xgboost as xgb\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.utils import shuffle\n",
        "param_hyperopt= {\n",
        "        'DecisionTreeClassifier':\n",
        "            {\n",
        "            'max_depth': ho_scope.int(hp.quniform('c0_max_depth', 3, 7, 1)),\n",
        "            'max_features': hp.choice('c0_max_features', ['auto', 'sqrt', 'log2']),\n",
        "            'min_samples_split': ho_scope.int(hp.quniform('c0_min_samples_split', 3, 15,3)),\n",
        "            'min_samples_leaf': ho_scope.int(hp.quniform('c0_min_samples_leaf', 3, 15,3))                                   \n",
        "          \n",
        "         },\n",
        "        'RandomForestClassifier': \n",
        "            {\n",
        "             'max_depth': ho_scope.int(hp.quniform('c2_max_depth', 2, 14, 2)),\n",
        "            'n_estimators': ho_scope.int(hp.quniform('c2_n_estimators', 10, 50, 5)),\n",
        "            'max_features': hp.choice('c2_max_features', ['auto', 'sqrt']),\n",
        "            'criterion': hp.choice('c2_criterion', ['gini', 'entropy']),\n",
        "            'min_samples_split': ho_scope.int(hp.quniform('c2_min_samples_split', 2, 14,2)),\n",
        "            'min_samples_leaf': ho_scope.int(hp.quniform('c2_min_samples_leaf', 2, 14,2))\n",
        "                          \n",
        "         } ,                                             \n",
        "          'HistGradientBoostingClassifier':\n",
        "            {'learning_rate': hp.loguniform('c3_learning_rate', np.log(0.01), np.log(0.1)),\n",
        "                'max_iter': ho_scope.int(hp.quniform('c3_max_iter', 10, 200, 5)),\n",
        "                'max_depth': ho_scope.int(hp.quniform('c3_max_depth', 2,14, 1)),\n",
        "                'max_leaf_nodes': ho_scope.int(hp.quniform('c3_max_leaf_nodes', 5, 35,5)),\n",
        "                'min_samples_leaf': ho_scope.int(hp.quniform('c3_min_samples_leaf', 5, 25,5)),\n",
        "                'max_bins': ho_scope.int(hp.quniform('c3_max_bins', 20, 200,20)),\n",
        "                'validation_fraction':0.1,\n",
        "                'n_iter_no_change':None,\n",
        "                 'tol':1e-07,\n",
        "                'l2_regularization':0.0 \n",
        "           },\n",
        "  'ExtraTreesClassifier':\n",
        "            {\n",
        "                'max_depth': ho_scope.int(hp.quniform('c4_max_depth', 5, 20, 5)),\n",
        "                'n_estimators': ho_scope.int(hp.quniform('c4_n_estimators', 5, 50, 5)),\n",
        "                'max_features': ho_scope.int(hp.quniform('c4_max_features', 5, 20, 5)),\n",
        "                'criterion': hp.choice('c4_criterion', ['gini', 'entropy']),\n",
        "                'min_samples_split': ho_scope.int(hp.quniform('c4_min_samples_split', 2, 14,2)),\n",
        "                'min_samples_leaf': ho_scope.int(hp.quniform('c4_min_samples_leaf', 2, 14,2))    \n",
        "       },\n",
        " 'AdaBoostClassifier':\n",
        "   {\n",
        "          'base_estimator':hp.choice('base_estimator', [DecisionTreeClassifier(max_depth=1)]),\n",
        "          'learning_rate': hp.loguniform('c5_learning_rate', np.log(0.01), np.log(1)),\n",
        "          'n_estimators': ho_scope.int(hp.quniform('c5_n_estimators', 40, 140, 40)),\n",
        "          'algorithm': hp.choice('c5_algorithm',[\"SAMME\"]) \n",
        "       },\n",
        "       'XGBClassifier':\n",
        "    {   \n",
        "        'max_depth': ho_scope.int(hp.quniform('c2_max_depth', 5, 15, 5)),\n",
        "           'min_child_weight': hp.choice ('c2_min_child',np.arange(1, 10, dtype=int)),\n",
        "           'subsample': hp.uniform ('c2_subsample', 0.2, 1),\n",
        "           'n_estimators': ho_scope.int(hp.quniform('c2_n_estimators', 5, 45, 15)),\n",
        "          'eta':ho_scope.float(hp.quniform('c2_eta', 0.025, 0.5, 0.025)) ,\n",
        "          'gamma':ho_scope.float( hp.quniform('c2_gamma', 0.5, 1, 0.05)) ,\n",
        "        'colsample_bytree': ho_scope.float(hp.quniform('c2_colsample_bytree', 0.5, 1, 0.05)),\n",
        "         'objective': 'binary:logistic',\n",
        "          'n_iter_no_change':5\n",
        "       },\n",
        "   'LinearDiscriminantAnalysis':\n",
        "            {    }  ,\n",
        "  'QuadraticDiscriminantAnalysis' :\n",
        "        {  },\n",
        " 'LGBMClassifier':\n",
        " {\n",
        "                    'max_depth': ho_scope.int(hp.quniform('c6_max_depth', 2, 14, 2)),\n",
        "                   'num_leave': ho_scope.int(hp.quniform('c6_num_leaves', 5, 30, 5)),\n",
        "                   'min_child_samples': ho_scope.int(hp.quniform('c6_min_child_samples', 50, 100, 10)),\n",
        "                   'scale_pos_weight': ho_scope.float(hp.quniform('c6_scale_pos_weight', 50, 100, 10)),\n",
        "                   'subsample': ho_scope.float(hp.quniform('c6_subsample', 0.2, 0.9, 0.1)),\n",
        "                   'colsample_bytree': ho_scope.float(hp.quniform('c6_colsample_bytree', 0.2, 0.9, 0.1)),\n",
        "                   'reg_lambda': hp.choice(\"c6_reg_lambda\",[0, 1e-1, 1, 5, 10, 20, 50]),\n",
        "                   'reg_alpha': hp.choice(\"c6_reg_alpha\",[0, 1e-1, 1, 5, 10, 20, 50]) ,\n",
        "                   'max_bin': ho_scope.int(hp.quniform('c6_max_bin', 10, 200, 10)),\n",
        "                   'learning_rate': ho_scope.float(hp.quniform('c6_learning_rate', 0.001, 0.09, 0.005)),\n",
        "                   'num_iterations': ho_scope.int(hp.quniform('c6_num_iterations', 100, 2000, 100)),\n",
        "                    'n_iter_no_change':10\n",
        "     },     \n",
        "  'LogisticRegression ':\n",
        "  { \n",
        "      ' classifier__penalty': hp.choice(' classifier__penalty', ['l1', 'l2']),\n",
        "     'classifier__C' :hp.loguniform('classifier__C', np.log(0.1), np.log(1)) ,\n",
        "    'classifier__solver' : 'liblinear'\n",
        "    }   ,\n",
        "  'MLPClassifier':\n",
        "  {\n",
        "      'hidden_layer_sizes': hp.choice(' c8_hidden_layer_sizes',[(50,50,50), (50,100,50), (100,)]),\n",
        "    'activation':hp.choice ('c8_activation',['tanh', 'relu']),\n",
        "    'solver': hp.choice('c8_solver',['sgd', 'adam']) ,\n",
        "    'alpha':hp.loguniform('c8_alpha',np.log(0.0001), np.log(0.05) ),\n",
        "    'learning_rate':hp.choice('c8_learning_rate', ['constant','adaptive'],)\n",
        "    },\n",
        "  'GaussianNB':\n",
        "  {  },\n",
        "  'BaggingClassifier':\n",
        "            {      \n",
        "              'base_estimator': hp.choice('base_estimator',[DecisionTreeClassifier()]),\n",
        "              'n_estimators': ho_scope.int(hp.quniform('n_estimators', 5, 25, 15)),\n",
        "              'oob_score': hp.choice('oob_score', [True,False]),\n",
        "               'random_state': 1\n",
        "            }\n",
        "                         \n",
        "       \n",
        "} \n",
        "\n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2,random_state = 0)\n",
        "\n",
        "best_clf3=[]\n",
        "final_clf=[]\n",
        "clf1_val_score=[]\n",
        "y_score=[]\n",
        "Fr_classifier=[]\n",
        "a1=[]\n",
        "classifiers=[]\n",
        "for i in range(len(try1)):\n",
        "    param_hyperopt3={try1[i][0].__name__:param_hyperopt[try1[i][0].__name__]}\n",
        "    clf_name=try1[i][0].__name__\n",
        "    cls=try1[i][0]\n",
        "    def f_clf1(params):\n",
        "       if not (params[clf_name]):   \n",
        "           model6 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),(clf_name,cls(**f_unpack_dict(params)))])\n",
        "           return model6\n",
        "       else:\n",
        "            model6 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),\n",
        "                                    (clf_name,cls(**f_unpack_dict(params[clf_name])))])\n",
        "            return model6\n",
        "    def objective_function(params,X_train2, y_train2):\n",
        "        model=f_clf1(params)\n",
        "        shuffle = KFold(n_splits=5, shuffle=True)\n",
        "        score = cross_val_score(model, X_train2, y_train2, cv=shuffle,scoring='roc_auc', n_jobs=-1).mean()\n",
        "        return {'loss': -score, 'status': STATUS_OK}  \n",
        "    trials = Trials()\n",
        "    best_clf3.append(fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                    param_hyperopt3, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))) \n",
        "    final_clf.append(f_clf1(space_eval(param_hyperopt3, best_clf3[i])).fit(X_train2, y_train2)) \n",
        "    # Calculating performance on validation set\n",
        "    y_score.append(final_clf[i].predict_proba(X_test2))  \n",
        "    clf1_val_score.append(roc_auc_score(y_test2, y_score[i][:,1])) \n",
        "    print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.format(-trials.best_trial['result']['loss'],\n",
        "                                                                       clf1_val_score[i]))\n",
        "    a1.append(space_eval(param_hyperopt3, best_clf3[i]))\n",
        "    classifiers.append({'classifier':cls,'Cross-val score':-trials.best_trial['result']['loss'],\n",
        "                        'validation score':clf1_val_score[i],'parameters':list(a1[i].values())[0]})\n",
        "classifiers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 20%|██        | 2/10 [02:34<12:12, 91.59s/it, best loss: -0.8811879851963225] "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [10:12<00:00, 77.07s/it, best loss: -0.889145641582062]\n",
            "Cross-val score: 0.88915; validation score: 0.88904\n",
            "  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [15:19<00:00, 95.05s/it, best loss: -0.8862456887587555]\n",
            "Cross-val score: 0.88625; validation score: 0.88182\n",
            " 50%|█████     | 5/10 [09:18<09:24, 112.82s/it, best loss: -0.8713451934772849]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [17:07<00:00, 80.95s/it, best loss: -0.8846250315039053]\n",
            "Cross-val score: 0.88463; validation score: 0.88352\n",
            "100%|██████████| 10/10 [41:46<00:00, 229.90s/it, best loss: -0.7629741618747085]\n",
            "Cross-val score: 0.76297; validation score: 0.76448\n",
            "  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [1:08:22<00:00, 392.89s/it, best loss: -0.8961590358829536]\n",
            "Cross-val score: 0.89616; validation score: 0.89891\n",
            "100%|██████████| 10/10 [02:34<00:00, 15.39s/it, best loss: -0.7520561338733224]\n",
            "Cross-val score: 0.75206; validation score: 0.78413\n",
            "  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [04:05<00:00, 24.51s/it, best loss: -0.8268494098728028]\n",
            "Cross-val score: 0.82685; validation score: 0.82928\n",
            "100%|██████████| 10/10 [38:50<00:00, 253.82s/it, best loss: -0.9013294538476375]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cross-val score: 0.90133; validation score: 0.90214\n",
            "100%|██████████| 10/10 [03:18<00:00, 19.86s/it, best loss: -0.7867791499718135]\n",
            "Cross-val score: 0.78678; validation score: 0.78877\n",
            "  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [19:24<00:00, 146.92s/it, best loss: -0.9030990105443255]\n",
            "Cross-val score: 0.90310; validation score: 0.90485\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Cross-val score': 0.889145641582062,\n",
              "  'classifier': sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier,\n",
              "  'parameters': {'l2_regularization': 0.0,\n",
              "   'learning_rate': 0.029641454778718972,\n",
              "   'max_bins': 100,\n",
              "   'max_depth': 10,\n",
              "   'max_iter': 175,\n",
              "   'max_leaf_nodes': 20,\n",
              "   'min_samples_leaf': 15,\n",
              "   'n_iter_no_change': None,\n",
              "   'tol': 1e-07,\n",
              "   'validation_fraction': 0.1},\n",
              "  'validation score': 0.8890387314781489},\n",
              " {'Cross-val score': 0.8862456887587555,\n",
              "  'classifier': sklearn.ensemble._forest.ExtraTreesClassifier,\n",
              "  'parameters': {'criterion': 'entropy',\n",
              "   'max_depth': 15,\n",
              "   'max_features': 15,\n",
              "   'min_samples_leaf': 12,\n",
              "   'min_samples_split': 10,\n",
              "   'n_estimators': 20},\n",
              "  'validation score': 0.8818171380065061},\n",
              " {'Cross-val score': 0.8846250315039053,\n",
              "  'classifier': sklearn.ensemble._forest.RandomForestClassifier,\n",
              "  'parameters': {'criterion': 'entropy',\n",
              "   'max_depth': 14,\n",
              "   'max_features': 'auto',\n",
              "   'min_samples_leaf': 2,\n",
              "   'min_samples_split': 6,\n",
              "   'n_estimators': 45},\n",
              "  'validation score': 0.8835185140616291},\n",
              " {'Cross-val score': 0.7629741618747085,\n",
              "  'classifier': sklearn.ensemble._weight_boosting.AdaBoostClassifier,\n",
              "  'parameters': {'algorithm': 'SAMME',\n",
              "   'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                          max_depth=1, max_features=None, max_leaf_nodes=None,\n",
              "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                          min_samples_leaf=1, min_samples_split=2,\n",
              "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                          random_state=None, splitter='best'),\n",
              "   'learning_rate': 0.30044074314959096,\n",
              "   'n_estimators': 80},\n",
              "  'validation score': 0.7644778116050867},\n",
              " {'Cross-val score': 0.8961590358829536,\n",
              "  'classifier': sklearn.ensemble._bagging.BaggingClassifier,\n",
              "  'parameters': {'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                          min_samples_leaf=1, min_samples_split=2,\n",
              "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                          random_state=None, splitter='best'),\n",
              "   'n_estimators': 30,\n",
              "   'oob_score': True,\n",
              "   'random_state': 1},\n",
              "  'validation score': 0.8989057692956279},\n",
              " {'Cross-val score': 0.7520561338733224,\n",
              "  'classifier': sklearn.tree._classes.DecisionTreeClassifier,\n",
              "  'parameters': {'max_depth': 6,\n",
              "   'max_features': 'sqrt',\n",
              "   'min_samples_leaf': 9,\n",
              "   'min_samples_split': 9},\n",
              "  'validation score': 0.7841260149694038},\n",
              " {'Cross-val score': 0.8268494098728028,\n",
              "  'classifier': sklearn.discriminant_analysis.LinearDiscriminantAnalysis,\n",
              "  'parameters': {},\n",
              "  'validation score': 0.8292819955028902},\n",
              " {'Cross-val score': 0.9013294538476375,\n",
              "  'classifier': lightgbm.sklearn.LGBMClassifier,\n",
              "  'parameters': {'colsample_bytree': 0.5,\n",
              "   'learning_rate': 0.09,\n",
              "   'max_bin': 80,\n",
              "   'max_depth': 10,\n",
              "   'min_child_samples': 80,\n",
              "   'n_iter_no_change': 10,\n",
              "   'num_iterations': 1000,\n",
              "   'num_leave': 10,\n",
              "   'reg_alpha': 50,\n",
              "   'reg_lambda': 5,\n",
              "   'scale_pos_weight': 50.0,\n",
              "   'subsample': 0.8},\n",
              "  'validation score': 0.9021447915521081},\n",
              " {'Cross-val score': 0.7867791499718135,\n",
              "  'classifier': sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis,\n",
              "  'parameters': {},\n",
              "  'validation score': 0.7887682267477347},\n",
              " {'Cross-val score': 0.9030990105443255,\n",
              "  'classifier': xgboost.sklearn.XGBClassifier,\n",
              "  'parameters': {'colsample_bytree': 1.0,\n",
              "   'eta': 0.125,\n",
              "   'gamma': 0.75,\n",
              "   'max_depth': 15,\n",
              "   'min_child_weight': 3,\n",
              "   'n_estimators': 15,\n",
              "   'n_iter_no_change': 5,\n",
              "   'objective': 'binary:logistic',\n",
              "   'subsample': 0.7468459518994333},\n",
              "  'validation score': 0.9048467840317365}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae7JMNadmIqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifiers=[{'Cross-val score': 0.889145641582062,\n",
        "  'classifier': sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier,\n",
        "  'parameters': {'l2_regularization': 0.0,\n",
        "   'learning_rate': 0.029641454778718972,\n",
        "   'max_bins': 100,\n",
        "   'max_depth': 10,\n",
        "   'max_iter': 175,\n",
        "   'max_leaf_nodes': 20,\n",
        "   'min_samples_leaf': 15,\n",
        "   'n_iter_no_change': None,\n",
        "   'tol': 1e-07,\n",
        "   'validation_fraction': 0.1},\n",
        "  'validation score': 0.8890387314781489},\n",
        " {'Cross-val score': 0.8862456887587555,\n",
        "  'classifier': sklearn.ensemble._forest.ExtraTreesClassifier,\n",
        "  'parameters': {'criterion': 'entropy',\n",
        "   'max_depth': 15,\n",
        "   'max_features': 15,\n",
        "   'min_samples_leaf': 12,\n",
        "   'min_samples_split': 10,\n",
        "   'n_estimators': 20},\n",
        "  'validation score': 0.8818171380065061},\n",
        " {'Cross-val score': 0.8846250315039053,\n",
        "  'classifier': sklearn.ensemble._forest.RandomForestClassifier,\n",
        "  'parameters': {'criterion': 'entropy',\n",
        "   'max_depth': 14,\n",
        "   'max_features': 'auto',\n",
        "   'min_samples_leaf': 2,\n",
        "   'min_samples_split': 6,\n",
        "   'n_estimators': 45},\n",
        "  'validation score': 0.8835185140616291},\n",
        " {'Cross-val score': 0.7629741618747085,\n",
        "  'classifier': sklearn.ensemble._weight_boosting.AdaBoostClassifier,\n",
        "  'parameters': {'algorithm': 'SAMME',\n",
        "   'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
        "                          max_depth=1, max_features=None, max_leaf_nodes=None,\n",
        "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                          min_samples_leaf=1, min_samples_split=2,\n",
        "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
        "                          random_state=None, splitter='best'),\n",
        "   'learning_rate': 0.30044074314959096,\n",
        "   'n_estimators': 80},\n",
        "  'validation score': 0.7644778116050867},\n",
        " {'Cross-val score': 0.8961590358829536,\n",
        "  'classifier': sklearn.ensemble._bagging.BaggingClassifier,\n",
        "  'parameters': {'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
        "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
        "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                          min_samples_leaf=1, min_samples_split=2,\n",
        "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
        "                          random_state=None, splitter='best'),\n",
        "   'n_estimators': 30,\n",
        "   'oob_score': True,\n",
        "   'random_state': 1},\n",
        "  'validation score': 0.8989057692956279},\n",
        " {'Cross-val score': 0.7520561338733224,\n",
        "  'classifier': sklearn.tree._classes.DecisionTreeClassifier,\n",
        "  'parameters': {'max_depth': 6,\n",
        "   'max_features': 'sqrt',\n",
        "   'min_samples_leaf': 9,\n",
        "   'min_samples_split': 9},\n",
        "  'validation score': 0.7841260149694038},\n",
        " {'Cross-val score': 0.8268494098728028,\n",
        "  'classifier': sklearn.discriminant_analysis.LinearDiscriminantAnalysis,\n",
        "  'parameters': {},\n",
        "  'validation score': 0.8292819955028902},\n",
        " {'Cross-val score': 0.9013294538476375,\n",
        "  'classifier': lightgbm.sklearn.LGBMClassifier,\n",
        "  'parameters': {'colsample_bytree': 0.5,\n",
        "   'learning_rate': 0.09,\n",
        "   'max_bin': 80,\n",
        "   'max_depth': 10,\n",
        "   'min_child_samples': 80,\n",
        "   'n_iter_no_change': 10,\n",
        "   'num_iterations': 1000,\n",
        "   'num_leave': 10,\n",
        "   'reg_alpha': 50,\n",
        "   'reg_lambda': 5,\n",
        "   'scale_pos_weight': 50.0,\n",
        "   'subsample': 0.8},\n",
        "  'validation score': 0.9021447915521081},\n",
        " {'Cross-val score': 0.7867791499718135,\n",
        "  'classifier': sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis,\n",
        "  'parameters': {},\n",
        "  'validation score': 0.7887682267477347},\n",
        " {'Cross-val score': 0.9030990105443255,\n",
        "  'classifier': xgboost.sklearn.XGBClassifier,\n",
        "  'parameters': {'colsample_bytree': 1.0,\n",
        "   'eta': 0.125,\n",
        "   'gamma': 0.75,\n",
        "   'max_depth': 15,\n",
        "   'min_child_weight': 3,\n",
        "   'n_estimators': 15,\n",
        "   'n_iter_no_change': 5,\n",
        "   'objective': 'binary:logistic',\n",
        "   'subsample': 0.7468459518994333},\n",
        "  'validation score': 0.9048467840317365}]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDhgh0xkJRO_",
        "colab_type": "code",
        "outputId": "7f4f73cb-cdec-4eff-a268-72fe16e17493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "classifiers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Cross-val score': 0.889145641582062,\n",
              "  'classifier': sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier,\n",
              "  'parameters': {'l2_regularization': 0.0,\n",
              "   'learning_rate': 0.029641454778718972,\n",
              "   'max_bins': 100,\n",
              "   'max_depth': 10,\n",
              "   'max_iter': 175,\n",
              "   'max_leaf_nodes': 20,\n",
              "   'min_samples_leaf': 15,\n",
              "   'n_iter_no_change': None,\n",
              "   'tol': 1e-07,\n",
              "   'validation_fraction': 0.1},\n",
              "  'validation score': 0.8890387314781489},\n",
              " {'Cross-val score': 0.8862456887587555,\n",
              "  'classifier': sklearn.ensemble._forest.ExtraTreesClassifier,\n",
              "  'parameters': {'criterion': 'entropy',\n",
              "   'max_depth': 15,\n",
              "   'max_features': 15,\n",
              "   'min_samples_leaf': 12,\n",
              "   'min_samples_split': 10,\n",
              "   'n_estimators': 20},\n",
              "  'validation score': 0.8818171380065061},\n",
              " {'Cross-val score': 0.8846250315039053,\n",
              "  'classifier': sklearn.ensemble._forest.RandomForestClassifier,\n",
              "  'parameters': {'criterion': 'entropy',\n",
              "   'max_depth': 14,\n",
              "   'max_features': 'auto',\n",
              "   'min_samples_leaf': 2,\n",
              "   'min_samples_split': 6,\n",
              "   'n_estimators': 45},\n",
              "  'validation score': 0.8835185140616291},\n",
              " {'Cross-val score': 0.7629741618747085,\n",
              "  'classifier': sklearn.ensemble._weight_boosting.AdaBoostClassifier,\n",
              "  'parameters': {'algorithm': 'SAMME',\n",
              "   'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                          max_depth=1, max_features=None, max_leaf_nodes=None,\n",
              "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                          min_samples_leaf=1, min_samples_split=2,\n",
              "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                          random_state=None, splitter='best'),\n",
              "   'learning_rate': 0.30044074314959096,\n",
              "   'n_estimators': 80},\n",
              "  'validation score': 0.7644778116050867},\n",
              " {'Cross-val score': 0.8961590358829536,\n",
              "  'classifier': sklearn.ensemble._bagging.BaggingClassifier,\n",
              "  'parameters': {'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                          min_samples_leaf=1, min_samples_split=2,\n",
              "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                          random_state=None, splitter='best'),\n",
              "   'n_estimators': 30,\n",
              "   'oob_score': True,\n",
              "   'random_state': 1},\n",
              "  'validation score': 0.8989057692956279},\n",
              " {'Cross-val score': 0.7520561338733224,\n",
              "  'classifier': sklearn.tree._classes.DecisionTreeClassifier,\n",
              "  'parameters': {'max_depth': 6,\n",
              "   'max_features': 'sqrt',\n",
              "   'min_samples_leaf': 9,\n",
              "   'min_samples_split': 9},\n",
              "  'validation score': 0.7841260149694038},\n",
              " {'Cross-val score': 0.8268494098728028,\n",
              "  'classifier': sklearn.discriminant_analysis.LinearDiscriminantAnalysis,\n",
              "  'parameters': {},\n",
              "  'validation score': 0.8292819955028902},\n",
              " {'Cross-val score': 0.9013294538476375,\n",
              "  'classifier': lightgbm.sklearn.LGBMClassifier,\n",
              "  'parameters': {'colsample_bytree': 0.5,\n",
              "   'learning_rate': 0.09,\n",
              "   'max_bin': 80,\n",
              "   'max_depth': 10,\n",
              "   'min_child_samples': 80,\n",
              "   'n_iter_no_change': 10,\n",
              "   'num_iterations': 1000,\n",
              "   'num_leave': 10,\n",
              "   'reg_alpha': 50,\n",
              "   'reg_lambda': 5,\n",
              "   'scale_pos_weight': 50.0,\n",
              "   'subsample': 0.8},\n",
              "  'validation score': 0.9021447915521081},\n",
              " {'Cross-val score': 0.7867791499718135,\n",
              "  'classifier': sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis,\n",
              "  'parameters': {},\n",
              "  'validation score': 0.7887682267477347},\n",
              " {'Cross-val score': 0.9030990105443255,\n",
              "  'classifier': xgboost.sklearn.XGBClassifier,\n",
              "  'parameters': {'colsample_bytree': 1.0,\n",
              "   'eta': 0.125,\n",
              "   'gamma': 0.75,\n",
              "   'max_depth': 15,\n",
              "   'min_child_weight': 3,\n",
              "   'n_estimators': 15,\n",
              "   'n_iter_no_change': 5,\n",
              "   'objective': 'binary:logistic',\n",
              "   'subsample': 0.7468459518994333},\n",
              "  'validation score': 0.9048467840317365}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB_NrMJyJYUQ",
        "colab_type": "code",
        "outputId": "c25e1d5d-f13f-4e72-e0bb-a3a88524fbd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "try1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier,\n",
              "  0.7945943590239859],\n",
              " [sklearn.ensemble._forest.ExtraTreesClassifier, 0.7808462675667986],\n",
              " [sklearn.ensemble._forest.RandomForestClassifier, 0.7805897845111422],\n",
              " [sklearn.ensemble._weight_boosting.AdaBoostClassifier, 0.7512525215403791],\n",
              " [sklearn.ensemble._bagging.BaggingClassifier, 0.7480078888675838],\n",
              " [sklearn.tree._classes.DecisionTreeClassifier, 0.747223233762471],\n",
              " [sklearn.discriminant_analysis.LinearDiscriminantAnalysis,\n",
              "  0.7454878038309398],\n",
              " [lightgbm.sklearn.LGBMClassifier, 0.7381382966693678],\n",
              " [sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis,\n",
              "  0.7266539484001128],\n",
              " [xgboost.sklearn.XGBClassifier, 0.7097669734676023]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSvkaeU_x5n4",
        "colab_type": "code",
        "outputId": "8a48974b-d319-4e40-b8b0-defa87b89e94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "classifiers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Cross-val score': 0.8711131366323283,\n",
              "  'classifier': sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier,\n",
              "  'parameters': {'l2_regularization': 0.0,\n",
              "   'learning_rate': 0.029641454778718972,\n",
              "   'max_bins': 60,\n",
              "   'max_depth': 5,\n",
              "   'max_iter': 70,\n",
              "   'max_leaf_nodes': 20,\n",
              "   'min_samples_leaf': 15,\n",
              "   'n_iter_no_change': None,\n",
              "   'tol': 1e-07,\n",
              "   'validation_fraction': 0.1},\n",
              "  'validation score': 0.8723130830966283},\n",
              " {'Cross-val score': 0.8194761104287241,\n",
              "  'classifier': sklearn.ensemble.forest.RandomForestClassifier,\n",
              "  'parameters': {'criterion': 'gini',\n",
              "   'max_depth': 7,\n",
              "   'max_features': 'auto',\n",
              "   'min_samples_leaf': 2,\n",
              "   'min_samples_split': 3,\n",
              "   'n_estimators': 23},\n",
              "  'validation score': 0.8300823968398406},\n",
              " {'Cross-val score': 0.8819835573800467,\n",
              "  'classifier': sklearn.ensemble.forest.ExtraTreesClassifier,\n",
              "  'parameters': {'criterion': 'entropy',\n",
              "   'max_depth': 15,\n",
              "   'max_features': 13,\n",
              "   'min_samples_leaf': 4,\n",
              "   'min_samples_split': 4,\n",
              "   'n_estimators': 13},\n",
              "  'validation score': 0.8811540398175102},\n",
              " {'Cross-val score': 0.8268491843425343,\n",
              "  'classifier': sklearn.discriminant_analysis.LinearDiscriminantAnalysis,\n",
              "  'parameters': {},\n",
              "  'validation score': 0.8292819955028902},\n",
              " {'Cross-val score': 0.7595038153922112,\n",
              "  'classifier': sklearn.ensemble.weight_boosting.AdaBoostClassifier,\n",
              "  'parameters': {'algorithm': 'SAMME',\n",
              "   'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
              "                          max_features=None, max_leaf_nodes=None,\n",
              "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                          min_samples_leaf=1, min_samples_split=2,\n",
              "                          min_weight_fraction_leaf=0.0, presort=False,\n",
              "                          random_state=None, splitter='best'),\n",
              "   'learning_rate': 0.22606271709759262,\n",
              "   'n_estimators': 60},\n",
              "  'validation score': 0.7631986456925348},\n",
              " {'Cross-val score': 0.899563550320555,\n",
              "  'classifier': sklearn.ensemble.bagging.BaggingClassifier,\n",
              "  'parameters': {'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
              "                          max_features=None, max_leaf_nodes=None,\n",
              "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                          min_samples_leaf=1, min_samples_split=2,\n",
              "                          min_weight_fraction_leaf=0.0, presort=False,\n",
              "                          random_state=None, splitter='best'),\n",
              "   'n_estimators': 90,\n",
              "   'oob_score': True,\n",
              "   'random_state': 1},\n",
              "  'validation score': 0.9024391189705803},\n",
              " {'Cross-val score': 0.7599043700113812,\n",
              "  'classifier': sklearn.tree.tree.DecisionTreeClassifier,\n",
              "  'parameters': {'max_depth': 5,\n",
              "   'max_features': 'auto',\n",
              "   'min_samples_leaf': 6,\n",
              "   'min_samples_split': 15},\n",
              "  'validation score': 0.8019212055193932},\n",
              " {'Cross-val score': 0.89299038605694,\n",
              "  'classifier': lightgbm.sklearn.LGBMClassifier,\n",
              "  'parameters': {'colsample_bytree': 1.0,\n",
              "   'max_depth': 9,\n",
              "   'min_child_samples': 70,\n",
              "   'num_leave': 10,\n",
              "   'reg_alpha': 5,\n",
              "   'reg_lambda': 20,\n",
              "   'scale_pos_weight': 50.0,\n",
              "   'subsample': 1.0},\n",
              "  'validation score': 0.8937718515059235},\n",
              " {'Cross-val score': 0.7867801551049096,\n",
              "  'classifier': sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis,\n",
              "  'parameters': {},\n",
              "  'validation score': 0.7887682267477347},\n",
              " {'Cross-val score': 0.904275078504711,\n",
              "  'classifier': xgboost.sklearn.XGBClassifier,\n",
              "  'parameters': {'booster': 'gbtree',\n",
              "   'colsample_bytree': 0.65,\n",
              "   'eta': 0.25,\n",
              "   'eval_metric': 'auc',\n",
              "   'gamma': 0.7000000000000001,\n",
              "   'max_depth': 10,\n",
              "   'min_child_weight': 5,\n",
              "   'n_estimators': 100,\n",
              "   'n_iter_no_change': 5,\n",
              "   'objective': 'binary:logistic',\n",
              "   'silent': 1,\n",
              "   'subsample': 0.7000000000000001,\n",
              "   'tree_method': 'exact'},\n",
              "  'validation score': 0.9055016264998491}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cStf2DxWSTIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifiers=[{'Cross-val score': 0.8709960449970973,\n",
        "  'classifier': 'H',\n",
        "  'parameters': {'l2_regularization': 0.0,\n",
        "   'learning_rate': 0.029641454778718972,\n",
        "   'max_bins': 60,\n",
        "   'max_depth': 5,\n",
        "   'max_iter': 70,\n",
        "   'max_leaf_nodes': 20,\n",
        "   'min_samples_leaf': 15,\n",
        "   'n_iter_no_change': None,\n",
        "   'tol': 1e-07,\n",
        "   'validation_fraction': 0.1},\n",
        "  'validation score': 0.8723034213769378},\n",
        " {'Cross-val score': 0.8240602823420584,\n",
        "  'classifier': 'a',\n",
        "  'parameters': {'criterion': 'entropy',\n",
        "   'max_depth': 7,\n",
        "   'max_features': 'auto',\n",
        "   'min_samples_leaf': 2,\n",
        "   'min_samples_split': 3,\n",
        "   'n_estimators': 27},\n",
        "  'validation score': 0.834652607183466},\n",
        " {'Cross-val score': 0.8747138258478401,\n",
        "  'classifier': 't',\n",
        "  'parameters': {'criterion': 'entropy',\n",
        "   'max_depth': 15,\n",
        "   'max_features': 13,\n",
        "   'min_samples_leaf': 4,\n",
        "   'min_samples_split': 4,\n",
        "   'n_estimators': 13},\n",
        "  'validation score': 0.8850623181186879},\n",
        " {'Cross-val score': 0.8268467618939667,\n",
        "  'classifier': 'e',\n",
        "  'parameters': {},\n",
        "  'validation score': 0.8292819955028902},\n",
        " {'Cross-val score': 0.7697799769268939,\n",
        "  'classifier': 's',\n",
        "  'parameters': {'max_depth': 6,\n",
        "   'max_features': 'sqrt',\n",
        "   'min_samples_leaf': 9,\n",
        "   'min_samples_split': 9},\n",
        "  'validation score': 0.6699164900759476},\n",
        " {'Cross-val score': 0.8984435660132537,\n",
        "  'classifier': 'n',\n",
        "  'parameters': {'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
        "                          max_features=None, max_leaf_nodes=None,\n",
        "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                          min_samples_leaf=1, min_samples_split=2,\n",
        "                          min_weight_fraction_leaf=0.0, presort=False,\n",
        "                          random_state=None, splitter='best'),\n",
        "   'n_estimators': 50,\n",
        "   'oob_score': True,\n",
        "   'random_state': 1},\n",
        "  'validation score': 0.9012734481461568},\n",
        " {'Cross-val score': 0.7604508228998579,\n",
        "  'classifier': 's',\n",
        "  'parameters': {'algorithm': 'SAMME',\n",
        "   'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
        "                          max_features=None, max_leaf_nodes=None,\n",
        "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                          min_samples_leaf=1, min_samples_split=2,\n",
        "                          min_weight_fraction_leaf=0.0, presort=False,\n",
        "                          random_state=None, splitter='best'),\n",
        "   'learning_rate': 0.06654968297130961,\n",
        "   'n_estimators': 60},\n",
        "  'validation score': 0.7625440706795947},\n",
        " {'Cross-val score': 0.8929875025198267,\n",
        "  'classifier': 's',\n",
        "  'parameters': {'colsample_bytree': 1.0,\n",
        "   'max_depth': 9,\n",
        "   'min_child_samples': 70,\n",
        "   'num_leave': 10,\n",
        "   'reg_alpha': 5,\n",
        "   'reg_lambda': 20,\n",
        "   'scale_pos_weight': 50.0,\n",
        "   'subsample': 1.0},\n",
        "  'validation score': 0.8937718515059235},\n",
        " {'Cross-val score': 0.7867886912438091,\n",
        "  'classifier': 'c',\n",
        "  'parameters': {},\n",
        "  'validation score': 0.7887682267477347},\n",
        " {'Cross-val score': 0.9045777452981845,\n",
        "  'classifier': 'f',\n",
        "  'parameters': {'booster': 'gbtree',\n",
        "   'colsample_bytree': 0.65,\n",
        "   'eta': 0.25,\n",
        "   'eval_metric': 'auc',\n",
        "   'gamma': 0.7000000000000001,\n",
        "   'max_depth': 10,\n",
        "   'min_child_weight': 5,\n",
        "   'n_estimators': 110,\n",
        "   'n_iter_no_change': 5,\n",
        "   'nthread': 10,\n",
        "   'objective': 'binary:logistic',\n",
        "   'silent': 1,\n",
        "   'subsample': 0.7000000000000001,\n",
        "   'tree_method': 'exact'},\n",
        "  'validation score': 0.9058466258248712}]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtoVqobuCLvo",
        "colab_type": "code",
        "outputId": "e0a3c890-b36e-49af-efad-682264c99863",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "classifiers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Cross-val score': 0.8711131366323283,\n",
              "  'classifier': sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier,\n",
              "  'parameters': {'l2_regularization': 0.0,\n",
              "   'learning_rate': 0.029641454778718972,\n",
              "   'max_bins': 60,\n",
              "   'max_depth': 5,\n",
              "   'max_iter': 70,\n",
              "   'max_leaf_nodes': 20,\n",
              "   'min_samples_leaf': 15,\n",
              "   'n_iter_no_change': None,\n",
              "   'tol': 1e-07,\n",
              "   'validation_fraction': 0.1},\n",
              "  'validation score': 0.8723130830966283},\n",
              " {'Cross-val score': 0.8194761104287241,\n",
              "  'classifier': sklearn.ensemble.forest.RandomForestClassifier,\n",
              "  'parameters': {'criterion': 'gini',\n",
              "   'max_depth': 7,\n",
              "   'max_features': 'auto',\n",
              "   'min_samples_leaf': 2,\n",
              "   'min_samples_split': 3,\n",
              "   'n_estimators': 23},\n",
              "  'validation score': 0.8300823968398406},\n",
              " {'Cross-val score': 0.8819835573800467,\n",
              "  'classifier': sklearn.ensemble.forest.ExtraTreesClassifier,\n",
              "  'parameters': {'criterion': 'entropy',\n",
              "   'max_depth': 15,\n",
              "   'max_features': 13,\n",
              "   'min_samples_leaf': 4,\n",
              "   'min_samples_split': 4,\n",
              "   'n_estimators': 13},\n",
              "  'validation score': 0.8811540398175102},\n",
              " {'Cross-val score': 0.8268491843425343,\n",
              "  'classifier': sklearn.discriminant_analysis.LinearDiscriminantAnalysis,\n",
              "  'parameters': {},\n",
              "  'validation score': 0.8292819955028902},\n",
              " {'Cross-val score': 0.7595038153922112,\n",
              "  'classifier': sklearn.ensemble.weight_boosting.AdaBoostClassifier,\n",
              "  'parameters': {'algorithm': 'SAMME',\n",
              "   'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
              "                          max_features=None, max_leaf_nodes=None,\n",
              "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                          min_samples_leaf=1, min_samples_split=2,\n",
              "                          min_weight_fraction_leaf=0.0, presort=False,\n",
              "                          random_state=None, splitter='best'),\n",
              "   'learning_rate': 0.22606271709759262,\n",
              "   'n_estimators': 60},\n",
              "  'validation score': 0.7631986456925348},\n",
              " {'Cross-val score': 0.899563550320555,\n",
              "  'classifier': sklearn.ensemble.bagging.BaggingClassifier,\n",
              "  'parameters': {'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
              "                          max_features=None, max_leaf_nodes=None,\n",
              "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                          min_samples_leaf=1, min_samples_split=2,\n",
              "                          min_weight_fraction_leaf=0.0, presort=False,\n",
              "                          random_state=None, splitter='best'),\n",
              "   'n_estimators': 90,\n",
              "   'oob_score': True,\n",
              "   'random_state': 1},\n",
              "  'validation score': 0.9024391189705803},\n",
              " {'Cross-val score': 0.7599043700113812,\n",
              "  'classifier': sklearn.tree.tree.DecisionTreeClassifier,\n",
              "  'parameters': {'max_depth': 5,\n",
              "   'max_features': 'auto',\n",
              "   'min_samples_leaf': 6,\n",
              "   'min_samples_split': 15},\n",
              "  'validation score': 0.8019212055193932},\n",
              " {'Cross-val score': 0.89299038605694,\n",
              "  'classifier': lightgbm.sklearn.LGBMClassifier,\n",
              "  'parameters': {'colsample_bytree': 1.0,\n",
              "   'max_depth': 9,\n",
              "   'min_child_samples': 70,\n",
              "   'num_leave': 10,\n",
              "   'reg_alpha': 5,\n",
              "   'reg_lambda': 20,\n",
              "   'scale_pos_weight': 50.0,\n",
              "   'subsample': 1.0},\n",
              "  'validation score': 0.8937718515059235},\n",
              " {'Cross-val score': 0.7867801551049096,\n",
              "  'classifier': sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis,\n",
              "  'parameters': {},\n",
              "  'validation score': 0.7887682267477347},\n",
              " {'Cross-val score': 0.904275078504711,\n",
              "  'classifier': xgboost.sklearn.XGBClassifier,\n",
              "  'parameters': {'booster': 'gbtree',\n",
              "   'colsample_bytree': 0.65,\n",
              "   'eta': 0.25,\n",
              "   'eval_metric': 'auc',\n",
              "   'gamma': 0.7000000000000001,\n",
              "   'max_depth': 10,\n",
              "   'min_child_weight': 5,\n",
              "   'n_estimators': 100,\n",
              "   'n_iter_no_change': 5,\n",
              "   'objective': 'binary:logistic',\n",
              "   'silent': 1,\n",
              "   'subsample': 0.7000000000000001,\n",
              "   'tree_method': 'exact'},\n",
              "  'validation score': 0.9055016264998491}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-ZPMwgETLqx",
        "colab_type": "code",
        "outputId": "94721b45-ff44-49da-8c68-b8f70642bce3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "from operator import itemgetter\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "newlist = sorted(classifiers, key=itemgetter('Cross-val score'), reverse=True)\n",
        "fiv=[]\n",
        "for i in range(len(newlist)):\n",
        "     if round(newlist[i]['Cross-val score'],2)>0.80:\n",
        "        fiv.append(newlist[i])  \n",
        "        \n",
        "names=[]\n",
        "for i, _ in enumerate(Accepted):\n",
        "    names.append(\"pipeline\"+str(i+1))  \n",
        "AcceptedR=[]\n",
        "for i in range(len(Accepted)):\n",
        "    AcceptedR.append(Accepted[i][1])\n",
        "dict = {}\n",
        "for name, Accepted_val in zip(names, AcceptedR ):\n",
        "    dict[name] =make_pipeline(Accepted_val(),fiv[AcceptedR.index(Accepted_val)]['classifier'](**fiv[AcceptedR.index(Accepted_val)]['parameters']))\n",
        "\n",
        "sclf2 = StackingClassifier(classifiers=list(dict.values()), meta_classifier=LogisticRegression()) \n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test2, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test2, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-0fa24fb40ab9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAccepted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mnames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pipeline\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mAcceptedR\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Accepted' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro64pK_AQBNh",
        "colab_type": "code",
        "outputId": "9ce97d55-8e2a-4665-8a04-8e99d0026d2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "param_hyperopt1= {\n",
        "  'HistGradientBoostingClassifier':\n",
        "            {'learning_rate': hp.loguniform('c3_learning_rate', np.log(0.01), np.log(0.1)),\n",
        "                'max_iter': ho_scope.int(hp.quniform('c3_max_iter', 10, 200, 5)),\n",
        "                'max_depth': ho_scope.int(hp.quniform('c3_max_depth', 2,14, 1)),\n",
        "                'max_leaf_nodes': ho_scope.int(hp.quniform('c3_max_leaf_nodes', 5, 35,5)),\n",
        "                'min_samples_leaf': ho_scope.int(hp.quniform('c3_min_samples_leaf', 5, 25,5)),\n",
        "                'max_bins': ho_scope.int(hp.quniform('c3_max_bins', 20, 200,20)),\n",
        "                'validation_fraction':0.1,\n",
        "                'n_iter_no_change':None,\n",
        "                 'tol':1e-07,\n",
        "                'l2_regularization':0.0 \n",
        "           }\n",
        "       \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2,random_state = 0)\n",
        "def f_clf1(params):\n",
        "  model6 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),\n",
        "             ('HistGradientBoostingClassifier',HistGradientBoostingClassifier(**f_unpack_dict(params['HistGradientBoostingClassifier'])))])\n",
        "  return model6    \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf1(params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf1 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2), param_hyperopt1, algo=tpe.suggest, max_evals=10,\n",
        "                 trials=trials, rstate=np.random.RandomState(1))\n",
        "clf1 = f_clf1(space_eval(param_hyperopt1, best_clf1)).fit(X_train2, y_train2)\n",
        "# Calculating performance on validation set\n",
        "y_score = clf1.predict_proba(X_test2)\n",
        "clf1_val_score = roc_auc_score(y_test2, y_score[:,1])\n",
        "#clf2_val_score = model_selection.cross_val_score(f_clf2, X_train2,  y_train2, cv=5, scoring='roc_auc')\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf1_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(space_eval(param_hyperopt, best_clf1))\n",
        "a1=space_eval(param_hyperopt1, best_clf1)\n",
        "Fr_classifier={'classifier':list(a1.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf1_val_score,'parameters':list(a1.values())[0]}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 10%|█         | 1/10 [01:40<15:07, 100.83s/it, best loss: -0.880866960072799]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [02:18<10:55, 81.89s/it, best loss: -0.880866960072799] "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 40%|████      | 4/10 [03:31<06:01, 60.24s/it, best loss: -0.880866960072799]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [04:00<04:13, 50.73s/it, best loss: -0.880866960072799]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [04:19<02:45, 41.38s/it, best loss: -0.880866960072799]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [05:14<02:15, 45.22s/it, best loss: -0.880866960072799]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [06:10<01:37, 48.52s/it, best loss: -0.8876534360829765]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [07:40<01:00, 60.89s/it, best loss: -0.8876534360829765]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 10/10 [09:10<00:00, 69.80s/it, best loss: -0.8890878006805651]\n",
            "Cross-val score: 0.88909; validation score: 0.88939\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Azd6FkhgUPtB",
        "colab_type": "code",
        "outputId": "744052d9-c38d-49a5-f323-fd39eee31fb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "Fr_classifier"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Cross-val score': 0.8890878006805651,\n",
              " 'classifier': 'HistGradientBoostingClassifier',\n",
              " 'parameters': {'l2_regularization': 0.0,\n",
              "  'learning_rate': 0.029641454778718972,\n",
              "  'max_bins': 100,\n",
              "  'max_depth': 10,\n",
              "  'max_iter': 175,\n",
              "  'max_leaf_nodes': 20,\n",
              "  'min_samples_leaf': 15,\n",
              "  'n_iter_no_change': None,\n",
              "  'tol': 1e-07,\n",
              "  'validation_fraction': 0.1},\n",
              " 'validation score': 0.889389222120687}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bkp-7STuMRJ7",
        "colab_type": "code",
        "outputId": "fe558f46-1e7e-4af1-a4ca-a52ef80b9b84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "param_hyperopt1= {\n",
        "   'ExtraTreesClassifier':\n",
        "            {\n",
        "                'max_depth': ho_scope.int(hp.quniform('c4_max_depth', 5, 20, 5)),\n",
        "                'n_estimators': ho_scope.int(hp.quniform('c4_n_estimators', 5, 50, 5)),\n",
        "                'max_features': ho_scope.int(hp.quniform('c4_max_features', 5, 20, 5)),\n",
        "                'criterion': hp.choice('c4_criterion', ['gini', 'entropy']),\n",
        "                'min_samples_split': ho_scope.int(hp.quniform('c4_min_samples_split', 2, 14,2)),\n",
        "                'min_samples_leaf': ho_scope.int(hp.quniform('c4_min_samples_leaf', 2, 14,2))      \n",
        "             }     \n",
        "       \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2,random_state = 0)\n",
        "def f_clf1(params):\n",
        "  model6 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),\n",
        "             ('ExtraTreesClassifier',ExtraTreesClassifier(**f_unpack_dict(params['ExtraTreesClassifier'])))])\n",
        "  return model6    \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf1(params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf1 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2), param_hyperopt1, algo=tpe.suggest, max_evals=10,\n",
        "                 trials=trials, rstate=np.random.RandomState(1))\n",
        "clf1 = f_clf1(space_eval(param_hyperopt1, best_clf1)).fit(X_train2, y_train2)\n",
        "# Calculating performance on validation set\n",
        "y_score = clf1.predict_proba(X_test2)\n",
        "clf1_val_score = roc_auc_score(y_test2, y_score[:,1])\n",
        "#clf2_val_score = model_selection.cross_val_score(f_clf2, X_train2,  y_train2, cv=5, scoring='roc_auc')\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf1_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(space_eval(param_hyperopt, best_clf1))\n",
        "a1=space_eval(param_hyperopt1, best_clf1)\n",
        "Fr_classifier={'classifier':list(a1.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf1_val_score,'parameters':list(a1.values())[0]}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-e777f8d0d739>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m best_clf1 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2), param_hyperopt1, algo=tpe.suggest, max_evals=10,\n\u001b[0;32m---> 55\u001b[0;31m                  trials=trials, rstate=np.random.RandomState(1))\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0mclf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_clf1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_hyperopt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_clf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Calculating performance on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             show_progressbar=show_progressbar)\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    405\u001b[0m                     show_progressbar=show_progressbar)\n\u001b[1;32m    406\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                         \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-e777f8d0d739>\u001b[0m in \u001b[0;36mobjective_function\u001b[0;34m(params, X_train2, y_train2)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mobjective_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf_clf1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'status'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSTATUS_OK\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    388\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    391\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 236\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha8jZBOPyWoB",
        "colab_type": "code",
        "outputId": "e944662d-c7b0-4e5d-d2be-df9c050f7649",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "Fr_classifier"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Cross-val score': 0.8865282784493205,\n",
              " 'classifier': 'ExtraTreesClassifier',\n",
              " 'parameters': {'criterion': 'entropy',\n",
              "  'max_depth': 15,\n",
              "  'max_features': 15,\n",
              "  'min_samples_leaf': 12,\n",
              "  'min_samples_split': 10,\n",
              "  'n_estimators': 20},\n",
              " 'validation score': 0.8852764300265377}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiM49RkuqGq3",
        "colab_type": "code",
        "outputId": "f891bf2d-4b27-4fd5-9030-6626cb1d9710",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "from sklearn.ensemble.forest import RandomForestClassifier\n",
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "param_hyperopt2= {\n",
        "  'RandomForestClassifier': \n",
        "            {\n",
        "            'max_depth': ho_scope.int(hp.quniform('c2_max_depth', 2, 14, 2)),\n",
        "            'n_estimators': ho_scope.int(hp.quniform('c2_n_estimators', 10, 50, 5)),\n",
        "            'max_features': hp.choice('c2_max_features', ['auto', 'sqrt']),\n",
        "            'criterion': hp.choice('c2_criterion', ['gini', 'entropy']),\n",
        "            'min_samples_split': ho_scope.int(hp.quniform('c2_min_samples_split', 2, 14,2)),\n",
        "            'min_samples_leaf': ho_scope.int(hp.quniform('c2_min_samples_leaf', 2, 14,2))\n",
        "                          \n",
        "         } \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "def f_clf2(params):\n",
        "  model4 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),('RandomForestClassifier',RandomForestClassifier(**f_unpack_dict(params['RandomForestClassifier'])))])\n",
        "  return model4    \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf2(params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf2 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                 param_hyperopt2, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "clf2 = f_clf2(space_eval(param_hyperopt2, best_clf2)).fit(X_train2, y_train2)\n",
        "# Calculating performance on validation set\n",
        "y_score = clf2.predict_proba(X_test2)\n",
        "clf2_val_score = roc_auc_score(y_test2, y_score[:,1])\n",
        "#clf2_val_score = model_selection.cross_val_score(f_clf2, X_train2,  y_train2, cv=5, scoring='roc_auc')\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf2_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(\n",
        "a2=space_eval(param_hyperopt2, best_clf2)\n",
        "Se_classifier={'classifier':list(a2.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf2_val_score,'parameters':list(a2.values())[0]}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 20%|██        | 2/10 [03:24<12:12, 91.55s/it, best loss: -0.8579616542577266]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 50%|█████     | 5/10 [09:25<09:31, 114.23s/it, best loss: -0.8739648796765478]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [17:46<00:00, 86.68s/it, best loss: -0.8838360349561534]\n",
            "Cross-val score: 0.88384; validation score: 0.87969\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_cLstq3kPbq",
        "colab_type": "code",
        "outputId": "9ece8ef3-8154-406c-8665-d58c02a1c226",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        " RandomForestClassifier()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_Qo9loEyJ9x",
        "colab_type": "code",
        "outputId": "484342be-24f6-446d-a6e1-24526623b584",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "best_clf2\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'c2_criterion': 1,\n",
              " 'c2_max_depth': 10.0,\n",
              " 'c2_max_features': 0,\n",
              " 'c2_min_samples_leaf': 2.0,\n",
              " 'c2_min_samples_split': 4.0,\n",
              " 'c2_n_estimators': 35.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKMWqZQ8L3JV",
        "colab_type": "code",
        "outputId": "9f698df3-bb1a-4140-a311-58fa44b0117b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.utils import shuffle\n",
        "param_hyperopt3= {      \n",
        "       'BaggingClassifier':\n",
        "            {      \n",
        "             'base_estimator': hp.choice('base_estimator',[DecisionTreeClassifier()]),\n",
        "              'n_estimators': ho_scope.int(hp.quniform('n_estimators', 10, 50, 10)),\n",
        "              'oob_score': hp.choice('oob_score', [True,False]),\n",
        "               'random_state': 1\n",
        "            }\n",
        "                  \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "def f_clf3(params):\n",
        "   model1 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),('BaggingClassifier', BaggingClassifier(**f_unpack_dict(params['BaggingClassifier'])))])   \n",
        "   return model1\n",
        "        \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf3(params)\n",
        "    shuffle = KFold(n_splits=5, shuffle=True)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=shuffle,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf3 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                param_hyperopt3, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "clf3 = f_clf3(space_eval(param_hyperopt3, best_clf3)).fit(X_train2, y_train2)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf3_val_score = roc_auc_score(y_test, clf3.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf3_val_score))\n",
        "print('Best parameters:')\n",
        "print(space_eval(param_hyperopt3, best_clf3))\n",
        "# Calculating performance on validation set\n",
        "clf3_val_score = roc_auc_score(y_test, clf3.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf3_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(\n",
        "a3=space_eval(param_hyperopt3, best_clf3)\n",
        "Thrid_classifier={'classifier':list(a3.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf3_val_score,'parameters':list(a3.values())[0]}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 50%|█████     | 5/10 [1:04:13<1:04:12, 770.49s/it, best loss: -0.8983805004954327]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [2:00:02<00:00, 687.90s/it, best loss: -0.8983805004954327]\n",
            "Cross-val score: 0.89838; validation score: 0.93051\n",
            "Best parameters:\n",
            "{'BaggingClassifier': {'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
            "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
            "                       random_state=None, splitter='best'), 'n_estimators': 50, 'oob_score': True, 'random_state': 1}}\n",
            "Cross-val score: 0.89838; validation score: 0.93051\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-AC9k5VLzSY",
        "colab_type": "code",
        "outputId": "70ecde8f-4e17-4b5f-9b6e-b36238505ec4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "Thrid_classifier\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Cross-val score': 0.8983805004954327,\n",
              " 'classifier': 'BaggingClassifier',\n",
              " 'parameters': {'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                         max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                         min_samples_leaf=1, min_samples_split=2,\n",
              "                         min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                         random_state=None, splitter='best'),\n",
              "  'n_estimators': 50,\n",
              "  'oob_score': True,\n",
              "  'random_state': 1},\n",
              " 'validation score': 0.9305090729151654}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB0p-1daEvuD",
        "colab_type": "code",
        "outputId": "df30d798-b8ae-45ff-c27b-c7c91840591b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.utils import shuffle\n",
        "param_hyperopt3= {      \n",
        "       'BaggingClassifier':\n",
        "            {      \n",
        "             'base_estimator': hp.choice('base_estimator',[DecisionTreeClassifier()]),\n",
        "              'n_estimators': ho_scope.int(hp.quniform('n_estimators', 5, 25, 15)),\n",
        "              'oob_score': hp.choice('oob_score', [True,False]),\n",
        "               'random_state': 1\n",
        "            }\n",
        "                \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "def f_clf3(params):\n",
        "   model1 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),('BaggingClassifier', BaggingClassifier(**f_unpack_dict(params['BaggingClassifier'])))])   \n",
        "   return model1\n",
        "        \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf3(params)\n",
        "    shuffle = KFold(n_splits=5, shuffle=True)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=shuffle,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf3 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                param_hyperopt3, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "clf3 = f_clf3(space_eval(param_hyperopt3, best_clf3)).fit(X_train2, y_train2)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf3_val_score = roc_auc_score(y_test, clf3.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf3_val_score))\n",
        "print('Best parameters:')\n",
        "print(space_eval(param_hyperopt3, best_clf3))\n",
        "# Calculating performance on validation set\n",
        "clf3_val_score = roc_auc_score(y_test, clf3.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf3_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(\n",
        "a3=space_eval(param_hyperopt3, best_clf3)\n",
        "Thrid_classifier={'classifier':list(a3.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf3_val_score,'parameters':list(a3.values())[0]}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 10%|█         | 1/10 [11:53<1:46:59, 713.31s/it, best loss: -0.895919376299091]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-234a05933d03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m best_clf3 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n\u001b[0;32m---> 56\u001b[0;31m                 param_hyperopt3, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mclf3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_clf3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_hyperopt3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_clf3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             show_progressbar=show_progressbar)\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    405\u001b[0m                     show_progressbar=show_progressbar)\n\u001b[1;32m    406\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                         \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-234a05933d03>\u001b[0m in \u001b[0;36mobjective_function\u001b[0;34m(params, X_train2, y_train2)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf_clf3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'status'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSTATUS_OK\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    388\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    391\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 236\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j37o6YxZZgrL",
        "colab_type": "code",
        "outputId": "b6dca997-d289-4f4b-8d30-2221de374c28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "Thrid_classifier"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Cross-val score': 0.8956732554663308,\n",
              " 'classifier': 'BaggingClassifier',\n",
              " 'parameters': {'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                         max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                         min_samples_leaf=1, min_samples_split=2,\n",
              "                         min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                         random_state=None, splitter='best'),\n",
              "  'n_estimators': 30,\n",
              "  'oob_score': True,\n",
              "  'random_state': 1},\n",
              " 'validation score': 0.9280463385108305}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O2unkUj6im8",
        "colab_type": "code",
        "outputId": "e7f447e1-0d90-49ba-a52a-ccc3ee1e549f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.utils import shuffle\n",
        "param_hyperopt3= {      \n",
        "       'AdaBoostClassifier':\n",
        "   {\n",
        "         'base_estimator':hp.choice('base_estimator', [DecisionTreeClassifier(max_depth=1)]),\n",
        "          'learning_rate': hp.loguniform('c5_learning_rate', np.log(0.01), np.log(1)),\n",
        "          'n_estimators': ho_scope.int(hp.quniform('c5_n_estimators', 40, 140, 40)),\n",
        "          'algorithm': hp.choice('c5_algorithm',[\"SAMME\"])  \n",
        "       }\n",
        "                \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "def f_clf3(params):\n",
        "   model1 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),('AdaBoostClassifier', AdaBoostClassifier(**f_unpack_dict(params['AdaBoostClassifier'])))])   \n",
        "   return model1\n",
        "        \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf3(params)\n",
        "    shuffle = KFold(n_splits=5, shuffle=True)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=shuffle,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf3 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                param_hyperopt3, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "clf3 = f_clf3(space_eval(param_hyperopt3, best_clf3)).fit(X_train2, y_train2)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf3_val_score = roc_auc_score(y_test, clf3.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf3_val_score))\n",
        "print('Best parameters:')\n",
        "print(space_eval(param_hyperopt3, best_clf3))\n",
        "# Calculating performance on validation set\n",
        "clf3_val_score = roc_auc_score(y_test, clf3.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf3_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(\n",
        "a3=space_eval(param_hyperopt3, best_clf3)\n",
        "fifth_classifier={'classifier':list(a3.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf3_val_score,'parameters':list(a3.values())[0]}\n",
        "fifth_classifier"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [45:42<00:00, 250.16s/it, best loss: -0.7632769733847417]\n",
            "Cross-val score: 0.76328; validation score: 0.76264\n",
            "Best parameters:\n",
            "{'AdaBoostClassifier': {'algorithm': 'SAMME', 'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
            "                       max_depth=1, max_features=None, max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
            "                       random_state=None, splitter='best'), 'learning_rate': 0.30044074314959096, 'n_estimators': 80}}\n",
            "Cross-val score: 0.76328; validation score: 0.76264\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Cross-val score': 0.7632769733847417,\n",
              " 'classifier': 'AdaBoostClassifier',\n",
              " 'parameters': {'algorithm': 'SAMME',\n",
              "  'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                         max_depth=1, max_features=None, max_leaf_nodes=None,\n",
              "                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                         min_samples_leaf=1, min_samples_split=2,\n",
              "                         min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                         random_state=None, splitter='best'),\n",
              "  'learning_rate': 0.30044074314959096,\n",
              "  'n_estimators': 80},\n",
              " 'validation score': 0.7626419534117789}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6ZkDVZl8N3H",
        "colab_type": "code",
        "outputId": "f5be2124-01bf-415b-b075-7ec9250df240",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "AdaBoostClassifier(),DecisionTreeClassifier()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
              "                    n_estimators=50, random_state=None),\n",
              " DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                        min_samples_leaf=1, min_samples_split=2,\n",
              "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                        random_state=None, splitter='best'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv4WPj9XX5OS",
        "colab_type": "code",
        "outputId": "574f1cb6-df71-4c2e-de27-2b844774672f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.utils import shuffle\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "import xgboost as xgb\n",
        "\n",
        "param_hyperopt4= {\n",
        "  'XGBClassifier':\n",
        "      {\n",
        "           'max_depth': ho_scope.int(hp.quniform('c16_max_depth', 5, 15, 5)),\n",
        "           'min_child_weight': hp.choice ('x_min_child',np.arange(1, 10, dtype=int)),\n",
        "           'subsample': hp.uniform ('x_subsample', 0.2, 1),\n",
        "           'n_estimators': ho_scope.int(hp.quniform('c2_n_estimators', 5, 45, 15)),\n",
        "       }         \n",
        "       \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "def f_clf4(params):\n",
        "   model1 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),('XGBClassifier', xgb.XGBClassifier())])   \n",
        "   return model1  \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf4(params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf4 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                param_hyperopt4, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "clf4 = f_clf4(space_eval(param_hyperopt4, best_clf4)).fit(X_train2, y_train2)\n",
        "# Calculating performance on validation set\n",
        "clf4_val_score = roc_auc_score(y_test, clf4.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf4_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(\n",
        "a4=space_eval(param_hyperopt4, best_clf4)\n",
        "Four_classifier={'classifier':list(a4.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf4_val_score,'parameters':list(a4.values())[0]}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [03:11<28:42, 191.36s/it, best loss: -0.8774978395308153]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [06:26<25:39, 192.48s/it, best loss: -0.8774978395308153]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [09:38<22:25, 192.27s/it, best loss: -0.8774978395308153]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [12:51<19:15, 192.63s/it, best loss: -0.8774978395308153]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [16:03<16:01, 192.33s/it, best loss: -0.8774978395308153]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [19:15<12:48, 192.16s/it, best loss: -0.8774978395308153]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 80%|████████  | 8/10 [25:44<06:27, 193.70s/it, best loss: -0.8774978395308153]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [29:01<03:14, 194.72s/it, best loss: -0.8774978395308153]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 10/10 [32:21<00:00, 196.06s/it, best loss: -0.8774978395308153]\n",
            "Cross-val score: 0.87750; validation score: 0.87816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmuOPHfZZL9T",
        "colab_type": "code",
        "outputId": "56963711-3dbd-4674-e8d5-b749c222d5ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "Four_classifier\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Cross-val score': 0.8774978395308153,\n",
              " 'classifier': 'XGBClassifier',\n",
              " 'parameters': {'max_depth': 10,\n",
              "  'min_child_weight': 8,\n",
              "  'n_estimators': 15,\n",
              "  'subsample': 0.3916405019248916},\n",
              " 'validation score': 0.8781637598970042}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6WfRi1-5dev",
        "colab_type": "code",
        "outputId": "da36fa74-6578-4d4f-9707-83eb7062f3c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll import scope as ho_scope\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.utils import shuffle\n",
        "import lightgbm as lgb\n",
        "from functools import partial\n",
        "\n",
        "param_hyperopt5= {\n",
        "   'LGBMClassifier1':\n",
        "                  {\n",
        "                   'max_depth': ho_scope.int(hp.quniform('c16_max_depth', -2, 10, 1)),\n",
        "                   'num_leave': ho_scope.int(hp.quniform('c16_num_leaves', 10, 200, 10)),\n",
        "                   'min_child_samples': ho_scope.int(hp.quniform('c16_min_child_samples', 10, 90, 10)),\n",
        "                   'scale_pos_weight': ho_scope.int(hp.quniform('c16_scale_pos_weight', 10, 90, 10)),\n",
        "                   'subsample': ho_scope.float(hp.quniform('c16_subsample', 0.1, 0.9, 0.1)),\n",
        "                   'colsample_bytree': ho_scope.float(hp.quniform('c16_colsample_bytree', 0.1, 0.9, 0.1)),\n",
        "                   'reg_lambda': hp.choice(\"c16_reg_lambda\",[0, 1e-1, 1, 5, 10, 20, 50]),\n",
        "                   'reg_alpha': hp.choice(\"c16_reg_alpha\",[0, 1e-1, 1, 5, 10, 20, 50]),\n",
        "                   'bagging_fraction': ho_scope.float(hp.quniform('c16_bagging_fraction', 0.1, 0.9, 0.05)),\n",
        "                    'bagging_freq': ho_scope.int(hp.quniform('c6_bagging_freq', 1, 8, 1)),\n",
        "                   'min_data_in_leaf': ho_scope.int(hp.quniform('c16_min_data_in_leaf', 100, 1000, 100)),\n",
        "                  'min_sum_hessian_in_leaf': ho_scope.int(hp.quniform('c16_min_sum_hessian_in_leaf', 5, 20, 5)),\n",
        "                   'max_bin': ho_scope.int(hp.quniform('c16_max_bin', 10, 100, 10)),\n",
        "                   'learning_rate': ho_scope.float(hp.quniform('c16_learning_rate', 0.001, 0.09, 0.005)),\n",
        "                   'num_iterations': ho_scope.int(hp.quniform('c16_num_iterations', 100, 10000, 100)),\n",
        "                   'n_iter_no_change':10\n",
        "                       },\n",
        "                  \n",
        "                   'LGBMClassifier':\n",
        "                  {\n",
        "                     'max_depth': ho_scope.int(hp.quniform('c6_max_depth', 2, 14, 2)),\n",
        "                   'num_leave': ho_scope.int(hp.quniform('c6_num_leaves', 5, 30, 5)),\n",
        "                   'min_child_samples': ho_scope.int(hp.quniform('c6_min_child_samples', 50, 100, 10)),\n",
        "                   'scale_pos_weight': ho_scope.float(hp.quniform('c6_scale_pos_weight', 50, 100, 10)),\n",
        "                   'subsample': ho_scope.float(hp.quniform('c6_subsample', 0.2, 0.9, 0.1)),\n",
        "                   'colsample_bytree': ho_scope.float(hp.quniform('c6_colsample_bytree', 0.2, 0.9, 0.1)),\n",
        "                   'reg_lambda': hp.choice(\"c6_reg_lambda\",[0, 1e-1, 1, 5, 10, 20, 50]),\n",
        "                   'reg_alpha': hp.choice(\"c6_reg_alpha\",[0, 1e-1, 1, 5, 10, 20, 50]) ,\n",
        "                   'max_bin': ho_scope.int(hp.quniform('c6_max_bin', 10, 200, 10)),\n",
        "                   'learning_rate': ho_scope.float(hp.quniform('c6_learning_rate', 0.001, 0.09, 0.005)),\n",
        "                   'num_iterations': ho_scope.int(hp.quniform('c6_num_iterations', 100, 2000, 100)),\n",
        "                    'n_iter_no_change':10\n",
        "                   }     \n",
        "} \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "def f_clf5(params):\n",
        "   model1 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),('LGBMClassifier', \n",
        "                                                         lgb.LGBMClassifier(**f_unpack_dict(params['LGBMClassifier'])))])   \n",
        "   return model1  \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf5(params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf5 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                param_hyperopt5, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "clf5 = f_clf5(space_eval(param_hyperopt5, best_clf5)).fit(X_train2, y_train2)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf5_val_score = roc_auc_score(y_test, clf5.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf5_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(\n",
        "a5=space_eval(param_hyperopt5, best_clf5)\n",
        "fith_classifier={'classifier':list(a5.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf5_val_score,'parameters':list(a5.values())[0]}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 40%|████      | 4/10 [14:44<23:31, 235.22s/it, best loss: -0.9012358759310123]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [18:29<19:21, 232.28s/it, best loss: -0.9012358759310123]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 70%|███████   | 7/10 [26:39<12:04, 241.42s/it, best loss: -0.9012358759310123]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [36:10<00:00, 213.62s/it, best loss: -0.9012358759310123]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cross-val score: 0.90124; validation score: 0.90329\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn4Xx2mh_YYt",
        "colab_type": "code",
        "outputId": "f1b46ade-1411-4dae-fb74-d66cbaf587ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "fith_classifier"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Cross-val score': 0.9012358759310123,\n",
              " 'classifier': 'LGBMClassifier',\n",
              " 'parameters': {'colsample_bytree': 0.5,\n",
              "  'learning_rate': 0.09,\n",
              "  'max_bin': 80,\n",
              "  'max_depth': 10,\n",
              "  'min_child_samples': 80,\n",
              "  'n_iter_no_change': 10,\n",
              "  'num_iterations': 1000,\n",
              "  'num_leave': 10,\n",
              "  'reg_alpha': 50,\n",
              "  'reg_lambda': 5,\n",
              "  'scale_pos_weight': 50.0,\n",
              "  'subsample': 0.8},\n",
              " 'validation score': 0.9032920414079589}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3If0WbQtPcQM",
        "colab_type": "code",
        "outputId": "db558e83-8156-45ed-91db-65fbd9e211cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "from hyperopt.pyll import scope as ho_scope\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.utils import shuffle\n",
        "param_hyperopt6= {\n",
        "    'QuadraticDiscriminantAnalysis':\n",
        "      {\n",
        "           \n",
        "      }      \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "def f_clf6(params):\n",
        "   model3 =Pipeline(steps=[('encoder',Accepted[5][1]()),('QuadraticDiscriminantAnalysis',QuadraticDiscriminantAnalysis())])\n",
        "   return model3 \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf6(params)\n",
        "    #clf = RandomForestClassifier(**params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf6 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                param_hyperopt6, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "\n",
        "clf6 = f_clf6(space_eval(param_hyperopt6, best_clf6)).fit(X_train2, y_train2)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf6_val_score = roc_auc_score(y_test, clf6.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.format(-trials.best_trial['result']['loss'], clf6_val_score))\n",
        "print('Best parameters:')\n",
        "print(space_eval(param_hyperopt6, best_clf6))\n",
        "# Calculating performance on validation set\n",
        "clf6_val_score = roc_auc_score(y_test, clf6.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf6_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(\n",
        "a6=space_eval(param_hyperopt6, best_clf6)\n",
        "six_classifier={'classifier':list(a6.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf6_val_score,'parameters':list(a6.values())[0]}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [02:06<00:00, 12.49s/it, best loss: -0.786171897798788]\n",
            "Cross-val score: 0.78617; validation score: 0.78724\n",
            "Best parameters:\n",
            "{'QuadraticDiscriminantAnalysis': {}}\n",
            "Cross-val score: 0.78617; validation score: 0.78724\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oUvZj4cvDNH",
        "colab_type": "code",
        "outputId": "1e3ebf97-c916-412e-e1a6-4aaf36265c24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "pipe1 = make_pipeline(Accepted[3][1](),XGBClassifier(**Four_classifier['parameters']))\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier(**Se_classifier['parameters']))  \n",
        "pipe3 = make_pipeline(Accepted[0][1](),ExtraTreesClassifier(**Fr_classifier['parameters'])) \n",
        "pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier(**fith_classifier['parameters'])) \n",
        "pipe4 = make_pipeline(Accepted[2][1](),LinearDiscriminantAnalysis())\n",
        "pipe5 = make_pipeline(Accepted[5][1](),QuadraticDiscriminantAnalysis())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe5,pipe6], meta_classifier=LogisticRegression()) \n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test2, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test2, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.82      0.77     94841\n",
            "           1       0.85      0.76      0.80    120517\n",
            "\n",
            "    accuracy                           0.79    215358\n",
            "   macro avg       0.79      0.79      0.79    215358\n",
            "weighted avg       0.79      0.79      0.79    215358\n",
            "\n",
            "StackingClassifier score: 0.789651\n",
            "Cross-val score: 0.85474\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZRFjh3jzf8x",
        "colab_type": "code",
        "outputId": "5be9aa9d-e324-48f6-b8fc-a5d2cfb4face",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier())  \n",
        "pipe3 = make_pipeline(Accepted[0][1](),ExtraTreesClassifier()) \n",
        "pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier()) \n",
        "pipe4 = make_pipeline(Accepted[2][1](),LinearDiscriminantAnalysis())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe2,pipe3,pipe4,pipe6], meta_classifier=LogisticRegression()) \n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test2, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test2, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.76      0.76     94841\n",
            "           1       0.81      0.81      0.81    120517\n",
            "\n",
            "    accuracy                           0.79    215358\n",
            "   macro avg       0.79      0.79      0.79    215358\n",
            "weighted avg       0.79      0.79      0.79    215358\n",
            "\n",
            "StackingClassifier score: 0.905730\n",
            "Cross-val score: 0.86727\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA4P3yftv5VC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Fr_classifier={'Cross-val score': 0.8755075622861487,\n",
        " 'classifier': 'ExtraTreesClassifier',\n",
        " 'parameters': {'criterion': 'entropy',\n",
        "  'max_depth': 15,\n",
        "  'max_features': 13,\n",
        "  'min_samples_leaf': 4,\n",
        "  'min_samples_split': 4,\n",
        "  'n_estimators': 13},\n",
        " 'validation score': 0.884938764808794}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72q7zxSvwQK6",
        "colab_type": "code",
        "outputId": "367c1766-0447-4db5-c048-a6e2f9b376a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "Fr_classifier['parameters']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'criterion': 'entropy',\n",
              " 'max_depth': 15,\n",
              " 'max_features': 13,\n",
              " 'min_samples_leaf': 4,\n",
              " 'min_samples_split': 4,\n",
              " 'n_estimators': 13}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT1x5xQ-vWFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Se_classifier={'Cross-val score': 0.8273926245659394,\n",
        " 'classifier': 'RandomForestClassifier',\n",
        " 'parameters': {'criterion': 'gini',\n",
        "  'max_depth': 7,\n",
        "  'max_features': 'auto',\n",
        "  'min_samples_leaf': 2,\n",
        "  'min_samples_split': 3,\n",
        "  'n_estimators': 23},\n",
        " 'validation score': 0.7973701565847332}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jb9jiievacj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Thrid_classifier={'Cross-val score': 0.8268064630182487,\n",
        " 'classifier': 'LinearDiscriminantAnalysis',\n",
        " 'parameters': {},\n",
        " 'validation score': 0.827194350739109}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWLcgpDlvecC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Four_classifier={'Cross-val score': 0.8761819849027198,\n",
        " 'classifier': 'XGBClassifier',\n",
        " 'parameters': {'max_depth': 7,\n",
        "  'min_child_weight': 1,\n",
        "  'n_estimators': 38,\n",
        "  'subsample': 0.8479101254812229},\n",
        " 'validation score': 0.8770024548105813}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT7GCH_evh_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fith_classifier={'Cross-val score': 0.8931760781045686,\n",
        " 'classifier': 'LGBMClassifier',\n",
        " 'parameters': {'colsample_bytree': 1.0,\n",
        "  'max_depth': 9,\n",
        "  'min_child_samples': 70,\n",
        "  'num_leave': 10,\n",
        "  'reg_alpha': 5,\n",
        "  'reg_lambda': 20,\n",
        "  'scale_pos_weight': 50.0,\n",
        "  'subsample': 1.0},\n",
        " 'validation score': 0.8944420319608248}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOeAy9aqvmT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "six_classifier={'Cross-val score': 0.786171897798788,\n",
        " 'classifier': 'QuadraticDiscriminantAnalysis',\n",
        " 'parameters': {},\n",
        " 'validation score': 0.787235957146868}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoY5YxNhSY0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a8={'ExtraTreesClassifier': {'criterion': 'gini',\n",
        "  'max_depth': 7,\n",
        "  'max_features': 7,\n",
        "  'min_samples_leaf': 4,\n",
        "  'min_samples_split': 4,\n",
        "  'n_estimators': 19},\n",
        " 'LGBMClassifier': {'colsample_bytree': 1.0,\n",
        "  'max_depth': 7,\n",
        "  'min_child_samples': 80,\n",
        "  'num_leave': 50,\n",
        "  'reg_alpha': 0,\n",
        "  'reg_lambda': 10,\n",
        "  'scale_pos_weight': 70.0,\n",
        "  'subsample': 1.0},\n",
        " 'RandomForestClassifier': {'criterion': 'entropy',\n",
        "  'max_depth': 5,\n",
        "  'max_features': 'sqrt',\n",
        "  'min_samples_leaf': 3,\n",
        "  'min_samples_split': 2,\n",
        "  'n_estimators': 30},\n",
        " 'XGBClassifier': {'max_depth': 13,\n",
        "  'min_child_weight': 3,\n",
        "  'subsample': 0.9406880083709813}}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQZrDLCO4tzY",
        "colab_type": "code",
        "outputId": "c582b68c-85a5-4e0b-8489-5f14779fcd65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "{'ExtraTreesClassifier': {'criterion': 'gini',\n",
        "  'max_depth': 7,\n",
        "  'max_features': 7,\n",
        "  'min_samples_leaf': 4,\n",
        "  'min_samples_split': 4,\n",
        "  'n_estimators': 19},\n",
        " 'LGBMClassifier': {'colsample_bytree': 1.0,\n",
        "  'max_depth': 7,\n",
        "  'min_child_samples': 80,\n",
        "  'num_leave': 50,\n",
        "  'reg_alpha': 0,\n",
        "  'reg_lambda': 10,\n",
        "  'scale_pos_weight': 70.0,\n",
        "  'subsample': 1.0},\n",
        " 'RandomForestClassifier': {'criterion': 'entropy',\n",
        "  'max_depth': 5,\n",
        "  'max_features': 'sqrt',\n",
        "  'min_samples_leaf': 3,\n",
        "  'min_samples_split': 2,\n",
        "  'n_estimators': 30},\n",
        " 'XGBClassifier': {'max_depth': 13,\n",
        "  'min_child_weight': 3,\n",
        "  'subsample': 0.9406880083709813}}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e735912700ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mFr_classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'Fr_classifier' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc6sc6QfbVhx",
        "colab_type": "code",
        "outputId": "29f521ad-9c86-49e6-8b04-a46f03df8b63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier())\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier())\n",
        "pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier())\n",
        "pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe6], meta_classifier=LogisticRegression())\n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test2, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test2, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.78      0.77     94841\n",
            "           1       0.82      0.80      0.81    120517\n",
            "\n",
            "    accuracy                           0.79    215358\n",
            "   macro avg       0.79      0.79      0.79    215358\n",
            "weighted avg       0.79      0.79      0.79    215358\n",
            "\n",
            "StackingClassifier score: 0.913023\n",
            "Cross-val score: 0.86650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxWwikwQVEhi",
        "colab_type": "code",
        "outputId": "b3d5b3a5-0fec-48c2-c0b2-1451a54da5ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier(**a8['RandomForestClassifier']))  \n",
        "pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier(**a8['ExtraTreesClassifier'])) \n",
        "pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier(**a8['LGBMClassifier'])) \n",
        "pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe2,pipe3,pipe4,pipe6], meta_classifier=LogisticRegression()) \n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test2, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.48      0.46     94433\n",
            "           1       0.56      0.52      0.54    120925\n",
            "\n",
            "    accuracy                           0.50    215358\n",
            "   macro avg       0.50      0.50      0.50    215358\n",
            "weighted avg       0.51      0.50      0.50    215358\n",
            "\n",
            "StackingClassifier score: 0.809074\n",
            "Cross-val score: 0.50016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_NjQ3a2n3HR",
        "colab_type": "code",
        "outputId": "6020a5fe-fbf9-484b-8fac-728a0220d551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "for clf, label in zip([list(sclf2.named_classifiers.keys()),sclf2],[list(a8.keys())]):\n",
        "  for i in range(len(list(a8.keys()))):\n",
        "    if list(a8.keys())[i]  in label:\n",
        "      if (type((sclf2.named_classifiers[clf[0]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe1 = make_pipeline(Accepted[0][1](),\n",
        "                                XGBClassifier(**a8[(type((sclf2.named_classifiers[clf[0]][1])).__name__)]))\n",
        "      if (type((sclf2.named_classifiers[clf[1]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe2 = make_pipeline(Accepted[1][1](),\n",
        "                                RandomForestClassifier(**a8[(type((sclf2.named_classifiers[clf[1]][1])).__name__)]))  \n",
        "      if (type((sclf2.named_classifiers[clf[2]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe3 = make_pipeline(Accepted[2][1](),\n",
        "                                ExtraTreesClassifier(**a8[(type((sclf2.named_classifiers[clf[2]][1])).__name__)])) \n",
        "      if (type((sclf2.named_classifiers[clf[5]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe6 = make_pipeline(Accepted[5][1](),\n",
        "                                lgb.LGBMClassifier(**a8[(type((sclf2.named_classifiers[clf[5]][1])).__name__)])) \n",
        "    else:  \n",
        "            pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "            pipe5 = make_pipeline(Accepted[4][1](),QuadraticDiscriminantAnalysis())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe5,pipe6], meta_classifier=LogisticRegression()) \n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test2, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test2, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.84      0.79     94841\n",
            "           1       0.86      0.78      0.82    120517\n",
            "\n",
            "    accuracy                           0.81    215358\n",
            "   macro avg       0.81      0.81      0.81    215358\n",
            "weighted avg       0.81      0.81      0.81    215358\n",
            "\n",
            "StackingClassifier score: 0.813905\n",
            "Cross-val score: 0.87589\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmXql2VSupkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier(**a8['XGBClassifier']))\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier(**a8['RandomForestClassifier']))  \n",
        "pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier(**a8['ExtraTreesClassifier'])) \n",
        "pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier(**a8['LGBMClassifier'])) \n",
        "pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "pipe5 = make_pipeline(Accepted[4][1](),QuadraticDiscriminantAnalysis())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe5,pipe6], meta_classifier=LogisticRegression()) \n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVt5r7R6nsh3",
        "colab_type": "code",
        "outputId": "e6169280-bbd4-43fe-e1bc-db87afbce428",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "a8"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ExtraTreesClassifier': {'criterion': 'gini',\n",
              "  'max_depth': 7,\n",
              "  'max_features': 7,\n",
              "  'min_samples_leaf': 4,\n",
              "  'min_samples_split': 4,\n",
              "  'n_estimators': 19},\n",
              " 'LGBMClassifier': {'colsample_bytree': 1.0,\n",
              "  'max_depth': 7,\n",
              "  'min_child_samples': 80,\n",
              "  'num_leave': 50,\n",
              "  'reg_alpha': 0,\n",
              "  'reg_lambda': 10,\n",
              "  'scale_pos_weight': 70.0,\n",
              "  'subsample': 1.0},\n",
              " 'RandomForestClassifier': {'criterion': 'entropy',\n",
              "  'max_depth': 5,\n",
              "  'max_features': 'sqrt',\n",
              "  'min_samples_leaf': 3,\n",
              "  'min_samples_split': 2,\n",
              "  'n_estimators': 30},\n",
              " 'XGBClassifier': {'max_depth': 13,\n",
              "  'min_child_weight': 3,\n",
              "  'subsample': 0.9406880083709813}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tsh4A3eZtNDm",
        "colab_type": "code",
        "outputId": "ce2a2c8c-e6da-4b64-e765-8c1c6e23887b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "list(pipe1.named_steps.keys())[1],list(a8.keys())[0].lower()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('xgbclassifier', 'extratreesclassifier')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyjoLr3s4OzU",
        "colab_type": "code",
        "outputId": "febab780-ac65-4655-aec1-4f82539af121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "a8"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ExtraTreesClassifier': {'criterion': 'gini',\n",
              "  'max_depth': 7,\n",
              "  'max_features': 7,\n",
              "  'min_samples_leaf': 4,\n",
              "  'min_samples_split': 4,\n",
              "  'n_estimators': 19},\n",
              " 'LGBMClassifier': {'colsample_bytree': 1.0,\n",
              "  'max_depth': 7,\n",
              "  'min_child_samples': 80,\n",
              "  'num_leave': 50,\n",
              "  'reg_alpha': 0,\n",
              "  'reg_lambda': 10,\n",
              "  'scale_pos_weight': 70.0,\n",
              "  'subsample': 1.0},\n",
              " 'RandomForestClassifier': {'criterion': 'entropy',\n",
              "  'max_depth': 5,\n",
              "  'max_features': 'sqrt',\n",
              "  'min_samples_leaf': 3,\n",
              "  'min_samples_split': 2,\n",
              "  'n_estimators': 30},\n",
              " 'XGBClassifier': {'max_depth': 13,\n",
              "  'min_child_weight': 3,\n",
              "  'subsample': 0.9406880083709813}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yS_rBX750WcG",
        "colab_type": "code",
        "outputId": "684e1d1d-3603-4dcd-d668-e03588a3b3e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "for clf, label in zip([list(sclf2.named_classifiers.keys()),sclf2],[list(a8.keys())]):\n",
        "  for i in range(len(list(a8.keys()))):\n",
        "    if list(a8.keys())[i]  in label:\n",
        "      if (type((sclf2.named_classifiers[clf[0]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier(**a8[(type((sclf2.named_classifiers[clf[0]][1])).__name__)]))\n",
        "          pipe2\n",
        "      if (type((sclf2.named_classifiers[clf[1]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier(**a8[(type((sclf2.named_classifiers[clf[1]][1])).__name__)]))  \n",
        "          pipe2   \n",
        "      if (type((sclf2.named_classifiers[clf[2]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier(**a8[(type((sclf2.named_classifiers[clf[2]][1])).__name__)])) \n",
        "      if (type((sclf2.named_classifiers[clf[3]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier(**a8[(type((sclf2.named_classifiers[clf[5]][1])).__name__)])) \n",
        "    else:  \n",
        "            pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "            pipe5 = make_pipeline(Accepted[4][1](),QuadraticDiscriminantAnalysis())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe5,pipe6], meta_classifier=LogisticRegression())  \n",
        "model2=sclf2.fit(X_train, y_train)\n",
        "y_true3, y_pred3  =y_test, model2.predict(X_test)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train, y_train))\n",
        "clf2_val_score = roc_auc_score(y_test, sclf2.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('catboostencoder',\n",
              "                 CatBoostEncoder(a=1, cols=None, drop_invariant=False,\n",
              "                                 handle_missing='value', handle_unknown='value',\n",
              "                                 random_state=None, return_df=True, sigma=None,\n",
              "                                 verbose=0)),\n",
              "                ('xgbclassifier',\n",
              "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
              "                               colsample_bylevel=1, colsample_bynode=1,\n",
              "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
              "                               max_delta_step=0, max_depth=13,\n",
              "                               min_child_weight=3, missing=None,\n",
              "                               n_estimators=100, n_jobs=1, nthread=None,\n",
              "                               objective='binary:logistic', random_state=0,\n",
              "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
              "                               seed=None, silent=None,\n",
              "                               subsample=0.9406880083709813, verbosity=1))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVFICfs6qliC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for clf, label in zip([list(sclf2.named_classifiers.keys()),sclf2],[list(a8.keys())]):\n",
        "  if label[0].lower()==list(pipe1.named_steps.keys())[1]:\n",
        "    print(label)\n",
        "\n",
        "pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier())\n",
        "\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier())\n",
        "pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier())\n",
        "pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "pipe5 = make_pipeline(Accepted[4][1](),QuadraticDiscriminantAnalysis())\n",
        "pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiGbF94Mg6XY",
        "colab_type": "code",
        "outputId": "5aa1680c-f325-446f-a5be-4e03b7543f2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "sclf2.named_classifiers['pipeline-1'][1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.1, max_delta_step=0,\n",
              "              max_depth={'colsample_bytree': 1.0, 'max_depth': 7,\n",
              "                         'min_child_samples': 80, 'num_leave': 50,\n",
              "                         'reg_alpha': 0, 'reg_lambda': 10,\n",
              "                         'scale_pos_weight': 70.0, 'subsample': 1.0},\n",
              "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
              "              nthread=None, objective='binary:logistic', random_state=0,\n",
              "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "              silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaFe4_nXdj96",
        "colab_type": "code",
        "outputId": "a9616d42-3e4f-49b5-9a1d-07a9b904f5e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier(**a8['ExtraTreesClassifier'] )  )\n",
        "pipe3\n",
        "a8.keys(),list(sclf2.named_classifiers.keys())\n",
        "a8['XGBClassifier']\n",
        "pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier(**a8['XGBClassifier']))\n",
        "pipe1,a8['XGBClassifier']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Pipeline(memory=None,\n",
              "          steps=[('catboostencoder',\n",
              "                  CatBoostEncoder(a=1, cols=None, drop_invariant=False,\n",
              "                                  handle_missing='value', handle_unknown='value',\n",
              "                                  random_state=None, return_df=True, sigma=None,\n",
              "                                  verbose=0)),\n",
              "                 ('xgbclassifier',\n",
              "                  XGBClassifier(base_score=0.5, booster='gbtree',\n",
              "                                colsample_bylevel=1, colsample_bynode=1,\n",
              "                                colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
              "                                max_delta_step=0, max_depth=13,\n",
              "                                min_child_weight=3, missing=None,\n",
              "                                n_estimators=100, n_jobs=1, nthread=None,\n",
              "                                objective='binary:logistic', random_state=0,\n",
              "                                reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
              "                                seed=None, silent=None,\n",
              "                                subsample=0.9406880083709813, verbosity=1))],\n",
              "          verbose=False),\n",
              " {'max_depth': 13, 'min_child_weight': 3, 'subsample': 0.9406880083709813})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHU7-dKPdJKv",
        "colab_type": "code",
        "outputId": "51ca7629-9338-4a86-add8-74e741515f96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "list(pipe1.named_steps.keys())[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'xgbclassifier'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5TMP6e7ab6X",
        "colab_type": "code",
        "outputId": "016882b2-f986-4157-8671-fc3dcbf786bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for i,k in enumerate(list(sclf2.named_classifiers.keys())):\n",
        "  for j,val in enumerate(list(a8.keys())):\n",
        "    if type((sclf2.named_classifiers[k][1])).__name__ ==list(a8.keys())[j]:\n",
        "        print(list(a8.values())[j])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'criterion': 'entropy', 'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 30}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQa1FPcy3ucp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "{'ExtraTreesClassifier': {'criterion': 'gini', 'max_depth': 7, 'max_features': 7, 'min_samples_leaf': 4, 'min_samples_split': 4, 'n_estimators': 19}, \n",
        "'LGBMClassifier': {'colsample_bytree': 1.0, 'max_depth': 7, 'min_child_samples': 80, 'num_leave': 50, 'reg_alpha': 0, 'reg_lambda': 10, 'scale_pos_weight': 70.0, 'subsample': 1.0},\n",
        "'RandomForestClassifier': {'criterion': 'entropy', 'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 30},\n",
        "'XGBClassifier': {'max_depth': 13, 'min_child_weight': 3, 'subsample': 0.9406880083709813}}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwGKr5nNdGcp",
        "colab_type": "code",
        "outputId": "a9f7cd5b-9183-42f2-8736-1a16defcbdbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from hyperopt import hp, tpe, fmin, Trials, STATUS_OK\n",
        "from sklearn import datasets\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble.forest import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline \n",
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "import numpy as np\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "param_hyperopt={\n",
        "  'RandomForestClassifier': \n",
        "            {\n",
        "            'max_depth': ho_scope.int(hp.quniform('c2_max_depth', 5, 7, 1)),\n",
        "            'n_estimators': ho_scope.int(hp.quniform('c2_n_estimators', 5, 30, 1)),\n",
        "            'max_features': hp.choice('c2_max_features', ['auto', 'sqrt']),\n",
        "            'criterion': hp.choice('c2_criterion', ['gini', 'entropy']),\n",
        "            'min_samples_split': ho_scope.int(hp.quniform('c2_min_samples_split', 2, 5,1)),\n",
        "            'min_samples_leaf': ho_scope.int(hp.quniform('c2_min_samples_leaf', 2, 5,1))\n",
        "                          \n",
        "         },\n",
        "  'ExtraTreesClassifier':\n",
        "            {\n",
        "                'max_depth': ho_scope.int(hp.quniform('c4_max_depth', 5, 20, 1)),\n",
        "                'n_estimators': ho_scope.int(hp.quniform('c4_n_estimators', 5, 30, 1)),\n",
        "                'max_features': ho_scope.int(hp.quniform('c4_max_features', 5, 20, 1)),\n",
        "                'criterion': hp.choice('c4_criterion', ['gini', 'entropy']),\n",
        "                'min_samples_split': ho_scope.int(hp.quniform('c4_min_samples_split', 2, 5,1)),\n",
        "                'min_samples_leaf': ho_scope.int(hp.quniform('c4_min_samples_leaf', 2, 5,1))\n",
        "       },\n",
        "  'XGBClassifier':\n",
        "      {\n",
        "           'max_depth': hp.choice(\"x_max_depth\", np.arange(5, 25, dtype=int)),\n",
        "           'min_child_weight': hp.choice ('x_min_child',np.arange(1, 10, dtype=int)),\n",
        "           'subsample': hp.uniform ('x_subsample', 0.8, 1)\n",
        "       },\n",
        "  'LGBMClassifier':\n",
        "       {\n",
        "        'max_depth': ho_scope.int(hp.quniform('c6_max_depth', 3, 10, 1)),\n",
        "        'num_leave': ho_scope.int(hp.quniform('c6_num_leaves', 5, 50, 5)),\n",
        "        'min_child_samples': ho_scope.int(hp.quniform('c6_min_child_samples', 50, 100, 10)),\n",
        "        'scale_pos_weight': ho_scope.float(hp.quniform('c6_scale_pos_weight', 50, 100, 10)),\n",
        "        'subsample': ho_scope.float(hp.quniform('c6_subsample', 0.6, 0.9, 1)),\n",
        "        'colsample_bytree': ho_scope.float(hp.quniform('c6_colsample_bytree', 0.6, 0.9, 1)),\n",
        "        'reg_lambda': hp.choice(\"c6_reg_lambda\",[0, 1e-1, 1, 5, 10, 20, 50]),\n",
        "        'reg_alpha': hp.choice(\"c6_reg_alpha\",[0, 1e-1, 1, 5, 10, 20, 50])\n",
        "       }         \n",
        "       \n",
        "}  \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "def f_clf8(params):\n",
        "   model1 =Pipeline(steps=[('encoder',Accepted[0][1]()),\n",
        "             ('c4',ExtraTreesClassifier(**f_unpack_dict(params['ExtraTreesClassifier'])))])\n",
        "   model2 =Pipeline(steps=[('encoder',Accepted[1][1]()),('LGBMClassifier', lgb.LGBMClassifier(**f_unpack_dict(params['LGBMClassifier'])))]) \n",
        "   model3 =Pipeline(steps=[('encoder',Accepted[2][1]()),('XGBClassifier', xgb.XGBClassifier(**f_unpack_dict(params['XGBClassifier'])))])   \n",
        "   model4 =Pipeline(steps=[('encoder',Accepted[3][1]()),('LinearDiscriminantAnalysis', LinearDiscriminantAnalysis())]) \n",
        "   model5 =Pipeline(steps=[('encoder',Accepted[4][1]()),\n",
        "             ('c2',RandomForestClassifier(**f_unpack_dict(params['RandomForestClassifier'])))]) \n",
        "   model6 =Pipeline(steps=[('encoder',Accepted[5][1]()),('QuadraticDiscriminantAnalysis', QuadraticDiscriminantAnalysis())])\n",
        "   sclf2 = StackingClassifier(classifiers=[model1,model2,model3,model4,model5,model6],meta_classifier=LogisticRegression())\n",
        "   return sclf2\n",
        "  \n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf8(params)\n",
        "    #clf = RandomForestClassifier(**params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf8 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                param_hyperopt, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "\n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval\n",
        "from hyperopt.pyll import scope as ho_scope\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.utils import shuffle\n",
        "clf8 = f_clf8(space_eval(param_hyperopt, best_clf8)).fit(X_train, y_train)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf8_val_score = roc_auc_score(y_test, clf8.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf8_val_score))\n",
        "print('Best parameters:')\n",
        "print(space_eval(param_hyperopt, best_clf8))\n",
        "oo=-trials.best_trial['result']['loss']\n",
        "oo"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 10%|█         | 1/10 [29:27<4:25:04, 1767.20s/it, best loss: -0.8759822037775029]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd05nQ3EojhD",
        "colab_type": "code",
        "outputId": "7a2da0a9-b32c-4347-b9e2-3466c7cbaff0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "from hyperopt import hp, tpe, fmin, Trials, STATUS_OK\n",
        "from sklearn import datasets\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble.forest import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline \n",
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "import numpy as np\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "\n",
        "param_hyperopt= {\n",
        "    'AdaBoostClassifier':\n",
        "     {\n",
        "          'learning_rate': hp.loguniform('c5_learning_rate', np.log(0.01), np.log(1)),\n",
        "           'n_estimators': ho_scope.int(hp.quniform('c5_n_estimators', 5, 30, 1)),\n",
        "           'algorithm': hp.choice('c5_algorithm',[\"SAMME\"])  \n",
        "       }    \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "def f_clf7(params):\n",
        "   model2 =Pipeline(steps=[('encoder',Accepted[5][1]()),\n",
        "             ('c5',AdaBoostClassifier(**f_unpack_dict(params['AdaBoostClassifier'])))]) \n",
        "   return model2\n",
        "        \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf7(params)\n",
        "    #clf = RandomForestClassifier(**params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf7 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                 param_hyperopt, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "\n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval\n",
        "from hyperopt.pyll import scope as ho_scope\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.utils import shuffle\n",
        "clf7 = f_clf7(space_eval(param_hyperopt, best_clf7)).fit(X_train2, y_train2)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf7_val_score = roc_auc_score(y_test, clf7.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf7_val_score))\n",
        "print('Best parameters:')\n",
        "print(space_eval(param_hyperopt, best_clf7))\n",
        "cc=-trials.best_trial['result']['loss']\n",
        "cc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [07:45<00:00, 43.23s/it, best loss: -0.7034403411046589]\n",
            "Cross-val score: 0.70344; validation score: 0.70367\n",
            "Best parameters:\n",
            "{'AdaBoostClassifier': {'algorithm': 'SAMME', 'learning_rate': 0.30044074314959096, 'n_estimators': 13}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7034403411046589"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    }
  ]
}