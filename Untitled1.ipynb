{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarah127/udacity-data-analysis-with-r/blob/master/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgzROkNIH4do",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAV8exRXypkt",
        "colab_type": "code",
        "outputId": "15577a95-df28-4053-eb28-edb904ee19ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanizeii\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm() "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7410 sha256=abd7139d7cbe1327cee63be21590076ef57c73462ccbb8e09601b6097955d34b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement humanizeii (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for humanizeii\u001b[0m\n",
            "Gen RAM Free: 26.4 GB  | Proc size: 156.3 MB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh5NCoc8fsSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1R6gDJMND6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP8zG65mYk86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD03caqDhFhA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "from  pyspark import SparkConf\n",
        "#conf = SparkConf().setMaster(\"local\").setAppName(\"My app\").set(\"spark.executor.memory\", \"1g\")\n",
        "#sc = SparkContext()\n",
        "# ===============================read from csv==============================================\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "from pyspark.sql.types import StructType,StructField,StringType,TimestampType,IntegerType,DoubleType,LongType,FloatType\n",
        "Df5_Schema = StructType([StructField(\"Airline\", StringType(),True),StructField(\"Flight\", DoubleType()),\n",
        "StructField(\"AirportFrom\",StringType()),StructField(\"AirportTo\", StringType()),StructField(\"DayOfWeek\", IntegerType()),\n",
        "StructField(\"Time\",IntegerType()), StructField(\"Length\", IntegerType()),StructField(\"codrna_X1\",DoubleType()),\n",
        "StructField(\"codrna_X2\",DoubleType()),StructField(\"codrna_X3\",DoubleType()),StructField(\"codrna_X4\", DoubleType()),\n",
        "StructField(\"codrna_X5\", DoubleType()),StructField(\"codrna_X6\", DoubleType()),StructField(\"codrna_X7\", DoubleType()),\n",
        "StructField(\"codrna_X8\", DoubleType()),StructField(\"age\", IntegerType()),StructField(\"workclass\", StringType()),\n",
        "StructField(\"fnlwgt\", IntegerType()),StructField(\"education\", StringType()),StructField(\"education-num\", IntegerType()),\n",
        "StructField(\"marital-status\", StringType()),StructField(\"occupation\", StringType()),StructField(\"relationship\", StringType()),\n",
        "StructField(\"race\", StringType()),StructField(\"sex\", StringType()),StructField(\"capitalgain\", IntegerType()),\n",
        "StructField(\"capitalloss\", IntegerType()),StructField(\"hoursperweek\", IntegerType()),StructField(\"native-country\", StringType()),\n",
        "StructField(\"Delay\", IntegerType())])\n",
        "CSV_2007= '/content/drive/My Drive/AirlinesCodrnaAdult.csv'\n",
        "df5 =spark.read.schema(Df5_Schema).option(\"header\",\"true\").option(\"mode\", \"DROPMALFORMED\").csv(CSV_2007)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQvods40rPBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "names = [f.name for f in df5.schema.fields]\n",
        "dataTypes = [f.dataType for f in df5.schema.fields]\n",
        "ls=list(zip(names,dataTypes))\n",
        "str_cols=[]\n",
        "for i in range(len(ls)):\n",
        "     if type(ls[i][1])==StringType and len(df5.select(df5[ls[i][0]]).distinct().collect())<=20:\n",
        "         str_cols.append([i,ls[i][0]]) \n",
        "strings=[]\n",
        "for i in range(len(str_cols)):\n",
        "     strings.append(str_cols[i][1])  \n",
        "\n",
        "#the process of convertiong catogerical columns into numeric is not included in the machine learning operation \n",
        "#it will be left for future improvements !!!ان شاء الله      \n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df5) for column in strings]\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "df6 = pipeline.fit(df5).transform(df5)\n",
        "df6=df6.drop(*strings)\n",
        "df6_pd=df6.toPandas()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIM7xkiDsRJI",
        "colab_type": "code",
        "outputId": "c82ac03f-684c-44ed-8038-5d302159b2a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "! pip install category_encoders"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting category_encoders\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/52/c54191ad3782de633ea3d6ee3bb2837bda0cf3bc97644bb6375cf14150a0/category_encoders-2.1.0-py2.py3-none-any.whl (100kB)\n",
            "\r\u001b[K     |███▎                            | 10kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 51kB 2.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 61kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 81kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 92kB 3.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: statsmodels>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.10.2)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.17.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.22.1)\n",
            "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.25.3)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.5.1)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.4.1->category_encoders) (1.12.0)\n",
            "Installing collected packages: category-encoders\n",
            "Successfully installed category-encoders-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB96hBgnrflo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import f1_score\n",
        "import category_encoders as ce\n",
        "from sklearn.utils import resample\n",
        "\n",
        "encoder_list = [ce.backward_difference.BackwardDifferenceEncoder,ce.basen.BaseNEncoder,ce.binary.BinaryEncoder,\n",
        "                ce.cat_boost.CatBoostEncoder,ce.helmert.HelmertEncoder,\n",
        "                ce.james_stein.JamesSteinEncoder,ce.one_hot.OneHotEncoder,ce.leave_one_out.LeaveOneOutEncoder,\n",
        "                ce.m_estimate.MEstimateEncoder,ce.ordinal.OrdinalEncoder,\n",
        "                ce.sum_coding.SumEncoder,ce.target_encoder.TargetEncoder,ce.woe.WOEEncoder]\n",
        "#numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "#categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X = df6_pd.drop('Delay', axis=1)\n",
        "y = df6_pd['Delay']\n",
        "#le = preprocessing.LabelEncoder()\n",
        "#label_encoder = le.fit(y)\n",
        "#y = label_encoder.transform(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "block_size=int(round(X_train.shape[0]/len(encoder_list)))\n",
        "n = block_size  #chunk row size\n",
        "list_X_train = [X_train[i:i+n] for i in range(0,X_train.shape[0],n)]\n",
        "list_y_train = [y_train[i:i+n] for i in range(0,y_train.shape[0],n)]\n",
        "list_X_test = [X_test[i:i+n] for i in range(0,X_test.shape[0],n)]\n",
        "list_y_test = [y_test[i:i+n] for i in range(0,y_test.shape[0],n)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "486lmPRrsNS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_numeric_dataset=[]\n",
        "intermidate=[]\n",
        "testing_numeric_dataset=[]\n",
        "for i,encoder in enumerate(encoder_list):\n",
        "  #(encoder(handle_unknown='ignore',cols=['AirportFrom','AirportTo','native-country'])).fit_transform(boot1[i])\n",
        "   training_numeric_dataset.append(encoder_list[i](cols=['AirportFrom','AirportTo','native-country']).fit(list_X_train[i],list_y_train[i]))\n",
        "   intermidate.append((training_numeric_dataset[i]).transform(list_X_train[i],list_y_train[i]))\n",
        "for i in range(len(list_X_test)):\n",
        "    for j,encoder in enumerate(encoder_list):\n",
        "        testing_numeric_dataset.append((training_numeric_dataset[j]).transform(list_X_test[i])) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KMjFk6msm2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xx=[]\n",
        "for i in range(len(encoder_list)):\n",
        "    #xx.append([i,min(intermidate[i].columns)] )\n",
        "    xx.append([encoder_list[i],len(intermidate[i].columns)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XrUFnRFs2ZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import category_encoders as ce\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "block_size=int(round(X_train2.shape[0]/12))\n",
        "n = block_size  #chunk row size\n",
        "list_X_train2 = [X_train2[i:i+n] for i in range(0,X_train2.shape[0],n)]\n",
        "list_y_train2 = [y_train2[i:i+n] for i in range(0,y_train2.shape[0],n)]\n",
        "list_X_test2 = [X_test2[i:i+n] for i in range(0,X_test2.shape[0],n)]\n",
        "list_y_test2 = [y_test2[i:i+n] for i in range(0,y_test2.shape[0],n)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAV2AZs2tE00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import category_encoders as ce\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from heapq import nlargest\n",
        "\n",
        "encoder_list = [ce.backward_difference.BackwardDifferenceEncoder(),ce.basen.BaseNEncoder(),\n",
        "                ce.binary.BinaryEncoder(),ce.cat_boost.CatBoostEncoder(),\n",
        "                ce.helmert.HelmertEncoder(),ce.james_stein.JamesSteinEncoder(),\n",
        "                ce.one_hot.OneHotEncoder(),ce.leave_one_out.LeaveOneOutEncoder()\n",
        "                ,ce.m_estimate.MEstimateEncoder(),ce.ordinal.OrdinalEncoder(),ce.sum_coding.SumEncoder(),\n",
        "                ce.target_encoder.TargetEncoder(),ce.woe.WOEEncoder()]\n",
        "my_dict={}\n",
        "for encoder in encoder_list:\n",
        "    model = Pipeline(steps=[('preprocessor', encoder),('classifier',RandomForestClassifier())])               \n",
        "    modelOpt = model.fit(X_train, y_train)\n",
        "    y_true,y_pred =y_test, model.predict(X_test)\n",
        "    clf2_val_score = roc_auc_score(y_test, modelOpt.predict_proba(X_test)[:, 1])\n",
        "    my_dict[type(encoder)]= [modelOpt.score(X_train, y_train),clf2_val_score]\n",
        "\n",
        "num=int(round(len(encoder_list)*0.5))    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRf0C2IxszAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNP-3BVuIy8l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z={}\n",
        "for i in range(len(my_dict)):\n",
        "   z[list(my_dict.keys())[i]]=list(my_dict.items())[i][1][0] \n",
        "Highest1 = nlargest(num, z, key = z.get)  \n",
        "\n",
        "zz={}\n",
        "for i in range(len(my_dict)):\n",
        "   zz[list(my_dict.keys())[i]]=list(my_dict.items())[i][1][1] \n",
        "Highest2 = nlargest(num, zz, key = zz.get)  \n",
        "Highest3=list(set(Highest1+Highest2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN3Vj_8VJDhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Accepted=[]\n",
        "keys = my_dict.keys()\n",
        "for i,each in enumerate(xx):\n",
        " for j,val in enumerate(Highest3): \n",
        "   if xx[i][0]==val and  xx[i][1]==len(X_train.columns)  : \n",
        "           #  Accepted.append(\"%s :%s: %s\" % (i,type(each), my_dict.get(each))) \n",
        "           Accepted.append((i,val))\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20rpUs7-EfrN",
        "colab_type": "code",
        "outputId": "00d78ce4-5fb4-47fa-b8ae-37e17c253015",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "Accepted      "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(3, category_encoders.cat_boost.CatBoostEncoder),\n",
              " (5, category_encoders.james_stein.JamesSteinEncoder),\n",
              " (8, category_encoders.m_estimate.MEstimateEncoder),\n",
              " (9, category_encoders.ordinal.OrdinalEncoder),\n",
              " (11, category_encoders.target_encoder.TargetEncoder),\n",
              " (12, category_encoders.woe.WOEEncoder)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AWt8EE1Jl76",
        "colab_type": "code",
        "outputId": "fc230191-4d00-4507-d988-385ebfee5ca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "pipe1 = make_pipeline(Accepted[0][1](),RandomForestClassifier())\n",
        "pipe2 = make_pipeline(Accepted[1][1](), KNeighborsClassifier())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2], meta_classifier=LogisticRegression())\n",
        "\n",
        "model2=sclf2.fit(X_train, y_train)\n",
        "y_true3, y_pred3  =y_test, model2.predict(X_test)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train, y_train))\n",
        "clf2_val_score = roc_auc_score(y_test, sclf2.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P6auG6ahBf-",
        "colab_type": "code",
        "outputId": "ac10a693-047d-4a76-ee7d-90d0a37bc0e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier())\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier())\n",
        "pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier())\n",
        "pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "pipe5 = make_pipeline(Accepted[4][1](),AdaBoostClassifier())\n",
        "pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe5,pipe6], meta_classifier=LogisticRegression())\n",
        "\n",
        "model2=sclf2.fit(X_train, y_train)\n",
        "y_true3, y_pred3  =y_test, model2.predict(X_test)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train, y_train))\n",
        "clf2_val_score = roc_auc_score(y_test, sclf2.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.78      0.77     94227\n",
            "           1       0.82      0.80      0.81    121131\n",
            "\n",
            "    accuracy                           0.79    215358\n",
            "   macro avg       0.79      0.79      0.79    215358\n",
            "weighted avg       0.79      0.79      0.79    215358\n",
            "\n",
            "StackingClassifier score: 0.912733\n",
            "Cross-val score: 0.86497\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZZsULnlhHEM",
        "colab_type": "code",
        "outputId": "727c340d-7afb-48ac-a20b-85dd5bb70801",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier())\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier())\n",
        "pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier())\n",
        "pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "#pipe5 = make_pipeline(Accepted[4][1](),AdaBoostClassifier())\n",
        "pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe6], meta_classifier=LogisticRegression())\n",
        "\n",
        "model2=sclf2.fit(X_train, y_train)\n",
        "y_true3, y_pred3  =y_test, model2.predict(X_test)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train, y_train))\n",
        "clf2_val_score = roc_auc_score(y_test, sclf2.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.78      0.77     94227\n",
            "           1       0.82      0.80      0.81    121131\n",
            "\n",
            "    accuracy                           0.79    215358\n",
            "   macro avg       0.79      0.79      0.79    215358\n",
            "weighted avg       0.79      0.79      0.79    215358\n",
            "\n",
            "StackingClassifier score: 0.912755\n",
            "Cross-val score: 0.86483\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuYcc4nnZkp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from hyperopt import fmin, hp, tpe, Trials, space_eval\n",
        "from hyperopt.pyll import scope as ho_scope\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.utils import shuffle\n",
        "clf1 = f_clf1(space_eval(param_hyperopt, best_clf3)).fit(X_train, y_train)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf1_val_score = roc_auc_score(y_test, clf1.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf1_val_score))\n",
        "print('Best parameters:')\n",
        "print(space_eval(param_hyperopt, best_clf3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyBAgEvxakfh",
        "colab_type": "code",
        "outputId": "b82d30a2-80bf-4dd6-c7bc-36d44c6485ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "y_true3, y_pred3  =y_test, model2.predict(X_test)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"RandomForestClassifier score: %f\" % clf1.score(X_train, y_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.79      0.77     94676\n",
            "           1       0.83      0.78      0.80    120682\n",
            "\n",
            "    accuracy                           0.79    215358\n",
            "   macro avg       0.78      0.79      0.78    215358\n",
            "weighted avg       0.79      0.79      0.79    215358\n",
            "\n",
            "RandomForestClassifier score: 0.731440\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "13701896-f9ea-4707-c543-e1927773389d",
        "id": "GoT4rgyqbNPr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "import warnings\n",
        "pipe1 = make_pipeline(Accepted[0][1](),RandomForestClassifier(n_estimators= 21,\n",
        "                max_depth= 7,min_samples_split= 4,min_samples_leaf= 4,max_features= 'sqrt'))\n",
        "pipe2 = make_pipeline(Accepted[1][1](),AdaBoostClassifier(DecisionTreeClassifier(max_depth=1)\n",
        ",n_estimators=28,learning_rate=0.8051561754791255)) \n",
        "pipe3 = make_pipeline(Accepted[2][1](),XGBClassifier())\n",
        "pipe4 = make_pipeline(Accepted[3][1](),ExtraTreesClassifier(criterion= 'entropy'))\n",
        "pipe5 = make_pipeline(Accepted[4][1](),GradientBoostingClassifier())\n",
        "pipe6 = make_pipeline(Accepted[5][1](),GaussianNB())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe5,pipe6],\n",
        "                           meta_classifier=LogisticRegression())\n",
        "\n",
        "model2=sclf2.fit(X_train, y_train)\n",
        "y_true3, y_pred3  =y_test, model2.predict(X_test)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"RandomForestClassifier score: %f\" % model2.score(X_train, y_train))\n",
        "clf2_val_score = roc_auc_score(y_test, sclf2.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.79      0.77     94676\n",
            "           1       0.83      0.78      0.80    120682\n",
            "\n",
            "    accuracy                           0.79    215358\n",
            "   macro avg       0.78      0.79      0.78    215358\n",
            "weighted avg       0.79      0.79      0.79    215358\n",
            "\n",
            "RandomForestClassifier score: 0.913660\n",
            "Cross-val score: 0.84756\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APRcNVp5qCWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive \n",
        "from google.colab import auth \n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7jAi0SyqJ6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGO4odcgVfWt",
        "colab_type": "code",
        "outputId": "3112ce65-6548-4e63-8012-d8963f82873d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Compare Algorithms\n",
        "import pandas\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis,QuadraticDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.pipeline import Pipeline \n",
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "# explicitly require this experimental feature\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
        "# now you can import normally from ensemble\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import xgboost\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import lightgbm as lgb\n",
        "# load dataset\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "#le = preprocessing.LabelEncoder()\n",
        "#label_encoder = le.fit(y)\n",
        "#y = label_encoder.transform(y)\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2)\n",
        "block_size=int(round(X_train2.shape[0]/13))\n",
        "n = block_size  #chunk row size\n",
        "list_X_train = [X_train2[i:i+n] for i in range(0,X_train2.shape[0],n)]\n",
        "list_y_train = [y_train2[i:i+n] for i in range(0,y_train2.shape[0],n)]\n",
        "list_X_test = [X_test2[i:i+n] for i in range(0,X_test2.shape[0],n)]\n",
        "list_y_test = [y_test2[i:i+n] for i in range(0,y_test2.shape[0],n)]\n",
        "test_data = lgb.Dataset(list_X_test[0], label=list_y_test[0])\n",
        "# Build the encoder\n",
        "# prepare configuration for cross validation test harness\n",
        "seed = 7\n",
        "# prepare models\n",
        "models = []\n",
        "parameters = {\n",
        "    'application': 'binary','objective': 'binary','metric': 'auc','is_unbalance': 'true',\n",
        "    'boosting': 'gbdt','num_leaves': 31,'feature_fraction': 0.5,'bagging_fraction': 0.5,\n",
        "    'bagging_freq': 20,'learning_rate': 0.05,'verbose': 0\n",
        "}\n",
        "fit_params={\"early_stopping_rounds\":10,\"eval_metric\" : 'auc',\"eval_set\" : [(X_test,y_test)],\n",
        "            'eval_names': ['valid'],'verbose': 100,\n",
        "            'feature_name': 'auto', # that's actually the default\n",
        "            'categorical_feature': 'auto' # that's actually the default\n",
        "           }\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                               ('RandomForestClassifier',RandomForestClassifier(n_estimators=20))])) \n",
        "                               .fit(list_X_train[0], list_y_train[0])) \n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                               ('AdaBoostClassifier',AdaBoostClassifier(n_estimators=25))]))\n",
        "                               .fit(list_X_train[1], list_y_train[1])) \n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                       ('HistGradientBoostingClassifier', HistGradientBoostingClassifier())])).fit(list_X_train[2], list_y_train[2]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                              ('XGBClassifier',XGBClassifier(max_depth=2,n_estimators=10,objective='binary:logistic'))]))\n",
        "                            .fit(list_X_train[3], list_y_train[3]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                              ('ExtraTreesClassifier',ExtraTreesClassifier(criterion= 'entropy'))]))\n",
        "                               .fit(list_X_train[4], list_y_train[4]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "              ('lightgbm',lgb.LGBMClassifier(n_estimators=5 , num_leaves= 15,\n",
        "           max_depth=-1,colsample_bytree=0.9,subsample=0.9,learning_rate=0.1))])).fit(list_X_train[5], list_y_train[5]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                     ('LDA', LinearDiscriminantAnalysis())])).fit(list_X_train[6], list_y_train[6]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                     ('MLPClassifier',\n",
        "                      MLPClassifier(hidden_layer_sizes=(50,30,30), max_iter=5, alpha=0.0001,\n",
        "                     solver='sgd', verbose=10,  random_state=1))])).fit(list_X_train[7], list_y_train[7]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                     ('LR', LogisticRegression())])).fit(list_X_train[8], list_y_train[8]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                     ('QDA', QuadraticDiscriminantAnalysis())])).fit(list_X_train[9], list_y_train[9]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                      ('CART', DecisionTreeClassifier())])).fit(list_X_train[10], list_y_train[10]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                       ('NB', GaussianNB())])).fit(list_X_train[11], list_y_train[11]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "   ('BaggingClassifier',BaggingClassifier( DecisionTreeClassifier(random_state=42), n_estimators=100,\n",
        "    max_samples=100, bootstrap=True, n_jobs=-1, random_state=42,oob_score = True))])).fit(list_X_train[12], list_y_train[12]))\n",
        "\n",
        "\n",
        "   \n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "for name, model in enumerate(models):\n",
        "    #kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
        "    print('1')\n",
        "    cv_results = model_selection.cross_val_score(model, list_X_train[name], \n",
        "                            list_y_train[name], cv=5, scoring=scoring)\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "    print(msg)\n",
        "# boxplot algorithm comparison\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Algorithm Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 1612078158.51448345\n",
            "Iteration 3, loss = 1612072772.16362143\n",
            "Iteration 4, loss = 1612067385.83355808\n",
            "Iteration 5, loss = 1612061999.52204514\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "0: 0.778930 (0.002659)\n",
            "1\n",
            "1: 0.750920 (0.004233)\n",
            "1\n",
            "2: 0.795983 (0.002509)\n",
            "1\n",
            "3: 0.707609 (0.003880)\n",
            "1\n",
            "4: 0.780077 (0.004421)\n",
            "1\n",
            "5: 0.747767 (0.003142)\n",
            "1\n",
            "6: 0.746771 (0.002790)\n",
            "1\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 1267230397.72934413\n",
            "Iteration 3, loss = 1267226809.16302061\n",
            "Iteration 4, loss = 1267223220.61068892\n",
            "Iteration 5, loss = 1267219632.06952500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 695365468.18954909\n",
            "Iteration 3, loss = 695363499.03847551\n",
            "Iteration 4, loss = 695361529.89686799\n",
            "Iteration 5, loss = 695359560.76183057\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 941134803.07201493\n",
            "Iteration 3, loss = 941132137.94786501\n",
            "Iteration 4, loss = 941129472.83512342\n",
            "Iteration 5, loss = 941126807.73089886\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 2331605719.44067049\n",
            "Iteration 3, loss = 2331599116.76134348\n",
            "Iteration 4, loss = 2331592514.10431385\n",
            "Iteration 5, loss = 2331585911.46693993\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 362975790.93156517\n",
            "Iteration 3, loss = 362974768.54610741\n",
            "Iteration 4, loss = 362973746.16728216\n",
            "Iteration 5, loss = 362972723.79229373\n",
            "7: 0.558508 (0.000013)\n",
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "8: 0.617681 (0.012610)\n",
            "1\n",
            "9: 0.730095 (0.004813)\n",
            "1\n",
            "10: 0.750000 (0.003971)\n",
            "1\n",
            "11: 0.608928 (0.004319)\n",
            "1\n",
            "12: 0.745669 (0.003346)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEVCAYAAADuAi4fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGSR_9eGDZB4",
        "colab_type": "code",
        "outputId": "82c7802a-39c0-49ba-abe0-122af2ed9c17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "last=[]\n",
        "for name, model in enumerate(models):\n",
        "  last.append([type(model[1]),results[name].mean()])\n",
        "last\n",
        "def Sort(sub_li): \n",
        "   return(sorted(sub_li, key = lambda x: x[1], reverse=True))     \n",
        "bb=(Sort(last))\n",
        "try1=[]\n",
        "for i in range(len(bb)):\n",
        "  if bb[i][1]>=0.70:\n",
        "     try1.append(bb[i])\n",
        "try1"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier,\n",
              "  0.7959826886207633],\n",
              " [sklearn.ensemble._forest.ExtraTreesClassifier, 0.7800767113558555],\n",
              " [sklearn.ensemble._forest.RandomForestClassifier, 0.7789297408677198],\n",
              " [sklearn.ensemble._weight_boosting.AdaBoostClassifier, 0.7509204711328707],\n",
              " [sklearn.tree._classes.DecisionTreeClassifier, 0.7499999806409834],\n",
              " [lightgbm.sklearn.LGBMClassifier, 0.7477665570893185],\n",
              " [sklearn.discriminant_analysis.LinearDiscriminantAnalysis,\n",
              "  0.7467705038006419],\n",
              " [sklearn.ensemble._bagging.BaggingClassifier, 0.7456688015258731],\n",
              " [sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis,\n",
              "  0.7300947402941236],\n",
              " [xgboost.sklearn.XGBClassifier, 0.7076088838208248]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09XV0e_yx2rJ",
        "colab_type": "code",
        "outputId": "09dfa82b-760c-4dcc-a6ad-6a7c3d71bb8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "bb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier,\n",
              "  0.7945943590239859],\n",
              " [sklearn.ensemble._forest.ExtraTreesClassifier, 0.7808462675667986],\n",
              " [sklearn.ensemble._forest.RandomForestClassifier, 0.7805897845111422],\n",
              " [sklearn.ensemble._weight_boosting.AdaBoostClassifier, 0.7512525215403791],\n",
              " [sklearn.ensemble._bagging.BaggingClassifier, 0.7480078888675838],\n",
              " [sklearn.tree._classes.DecisionTreeClassifier, 0.747223233762471],\n",
              " [sklearn.discriminant_analysis.LinearDiscriminantAnalysis,\n",
              "  0.7454878038309398],\n",
              " [lightgbm.sklearn.LGBMClassifier, 0.7381382966693678],\n",
              " [sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis,\n",
              "  0.7266539484001128],\n",
              " [xgboost.sklearn.XGBClassifier, 0.7097669734676023],\n",
              " [sklearn.linear_model._logistic.LogisticRegression, 0.6158701938309009],\n",
              " [sklearn.naive_bayes.GaussianNB, 0.6143154689315227],\n",
              " [sklearn.neural_network._multilayer_perceptron.MLPClassifier,\n",
              "  0.5603495113294544]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz0WsbNDCAMr",
        "colab_type": "code",
        "outputId": "f9df4ae5-dc88-4727-94fe-03d514b7ff89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from hyperopt import hp, tpe, fmin, Trials, STATUS_OK\n",
        "from sklearn import datasets\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble.forest import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline \n",
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "# explicitly require this experimental feature\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
        "# now you can import normally from ensemble\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "import numpy as np\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "import xgboost as xgb\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.utils import shuffle\n",
        "param_hyperopt= {\n",
        "        'DecisionTreeClassifier':\n",
        "            {\n",
        "            'max_depth': ho_scope.int(hp.quniform('c0_max_depth', 3, 7, 1)),\n",
        "            'max_features': hp.choice('c0_max_features', ['auto', 'sqrt', 'log2']),\n",
        "            'min_samples_split': ho_scope.int(hp.quniform('c0_min_samples_split', 3, 15,3)),\n",
        "            'min_samples_leaf': ho_scope.int(hp.quniform('c0_min_samples_leaf', 3, 15,3))                                   \n",
        "          \n",
        "         },\n",
        "        'RandomForestClassifier': \n",
        "            {\n",
        "             'max_depth': ho_scope.int(hp.quniform('c2_max_depth', 2, 14, 2)),\n",
        "            'n_estimators': ho_scope.int(hp.quniform('c2_n_estimators', 10, 50, 5)),\n",
        "            'max_features': hp.choice('c2_max_features', ['auto', 'sqrt']),\n",
        "            'criterion': hp.choice('c2_criterion', ['gini', 'entropy']),\n",
        "            'min_samples_split': ho_scope.int(hp.quniform('c2_min_samples_split', 2, 14,2)),\n",
        "            'min_samples_leaf': ho_scope.int(hp.quniform('c2_min_samples_leaf', 2, 14,2))\n",
        "                          \n",
        "         } ,                                             \n",
        "          'HistGradientBoostingClassifier':\n",
        "            {'learning_rate': hp.loguniform('c3_learning_rate', np.log(0.01), np.log(0.1)),\n",
        "                'max_iter': ho_scope.int(hp.quniform('c3_max_iter', 10, 200, 5)),\n",
        "                'max_depth': ho_scope.int(hp.quniform('c3_max_depth', 2,14, 1)),\n",
        "                'max_leaf_nodes': ho_scope.int(hp.quniform('c3_max_leaf_nodes', 5, 35,5)),\n",
        "                'min_samples_leaf': ho_scope.int(hp.quniform('c3_min_samples_leaf', 5, 25,5)),\n",
        "                'max_bins': ho_scope.int(hp.quniform('c3_max_bins', 20, 200,20)),\n",
        "                'validation_fraction':0.1,\n",
        "                'n_iter_no_change':None,\n",
        "                 'tol':1e-07,\n",
        "                'l2_regularization':0.0 \n",
        "           },\n",
        "  'ExtraTreesClassifier':\n",
        "            {\n",
        "                'max_depth': ho_scope.int(hp.quniform('c4_max_depth', 5, 20, 5)),\n",
        "                'n_estimators': ho_scope.int(hp.quniform('c4_n_estimators', 5, 50, 5)),\n",
        "                'max_features': ho_scope.int(hp.quniform('c4_max_features', 5, 20, 5)),\n",
        "                'criterion': hp.choice('c4_criterion', ['gini', 'entropy']),\n",
        "                'min_samples_split': ho_scope.int(hp.quniform('c4_min_samples_split', 2, 14,2)),\n",
        "                'min_samples_leaf': ho_scope.int(hp.quniform('c4_min_samples_leaf', 2, 14,2))    \n",
        "       },\n",
        " 'AdaBoostClassifier':\n",
        "   {\n",
        "          'base_estimator':hp.choice('base_estimator', [DecisionTreeClassifier(max_depth=1)]),\n",
        "          'learning_rate': hp.loguniform('c5_learning_rate', np.log(0.01), np.log(1)),\n",
        "          'n_estimators': ho_scope.int(hp.quniform('c5_n_estimators', 40, 140, 40)),\n",
        "          'algorithm': hp.choice('c5_algorithm',[\"SAMME\"]) \n",
        "       },\n",
        "       'XGBClassifier':\n",
        "    {   \n",
        "        'max_depth': ho_scope.int(hp.quniform('c2_max_depth', 5, 15, 5)),\n",
        "           'min_child_weight': hp.choice ('c2_min_child',np.arange(1, 10, dtype=int)),\n",
        "           'subsample': hp.uniform ('c2_subsample', 0.2, 1),\n",
        "           'n_estimators': ho_scope.int(hp.quniform('c2_n_estimators', 5, 45, 15)),\n",
        "          'eta':ho_scope.float(hp.quniform('c2_eta', 0.025, 0.5, 0.025)) ,\n",
        "          'gamma':ho_scope.float( hp.quniform('c2_gamma', 0.5, 1, 0.05)) ,\n",
        "        'colsample_bytree': ho_scope.float(hp.quniform('c2_colsample_bytree', 0.5, 1, 0.05)),\n",
        "         'objective': 'binary:logistic',\n",
        "          'n_iter_no_change':5\n",
        "       },\n",
        "   'LinearDiscriminantAnalysis':\n",
        "            {    }  ,\n",
        "  'QuadraticDiscriminantAnalysis' :\n",
        "        {  },\n",
        " 'LGBMClassifier':\n",
        " {\n",
        "                    'max_depth': ho_scope.int(hp.quniform('c6_max_depth', 2, 14, 2)),\n",
        "                   'num_leave': ho_scope.int(hp.quniform('c6_num_leaves', 5, 30, 5)),\n",
        "                   'min_child_samples': ho_scope.int(hp.quniform('c6_min_child_samples', 50, 100, 10)),\n",
        "                   'scale_pos_weight': ho_scope.float(hp.quniform('c6_scale_pos_weight', 50, 100, 10)),\n",
        "                   'subsample': ho_scope.float(hp.quniform('c6_subsample', 0.2, 0.9, 0.1)),\n",
        "                   'colsample_bytree': ho_scope.float(hp.quniform('c6_colsample_bytree', 0.2, 0.9, 0.1)),\n",
        "                   'reg_lambda': hp.choice(\"c6_reg_lambda\",[0, 1e-1, 1, 5, 10, 20, 50]),\n",
        "                   'reg_alpha': hp.choice(\"c6_reg_alpha\",[0, 1e-1, 1, 5, 10, 20, 50]) ,\n",
        "                   'max_bin': ho_scope.int(hp.quniform('c6_max_bin', 10, 200, 10)),\n",
        "                   'learning_rate': ho_scope.float(hp.quniform('c6_learning_rate', 0.001, 0.09, 0.005)),\n",
        "                   'num_iterations': ho_scope.int(hp.quniform('c6_num_iterations', 100, 2000, 100)),\n",
        "                    'objective': 'binary',\n",
        "                    'metric' : 'binary_error',\n",
        "                    'n_iter_no_change':10\n",
        "     },     \n",
        "  'LogisticRegression ':\n",
        "  { \n",
        "      ' classifier__penalty': hp.choice(' classifier__penalty', ['l1', 'l2']),\n",
        "     'classifier__C' :hp.loguniform('classifier__C', np.log(0.1), np.log(1)) ,\n",
        "    'classifier__solver' : 'liblinear'\n",
        "    }   ,\n",
        "  'MLPClassifier':\n",
        "  {\n",
        "      'hidden_layer_sizes': hp.choice(' c8_hidden_layer_sizes',[(50,50,50), (50,100,50), (100,)]),\n",
        "    'activation':hp.choice ('c8_activation',['tanh', 'relu']),\n",
        "    'solver': hp.choice('c8_solver',['sgd', 'adam']) ,\n",
        "    'alpha':hp.loguniform('c8_alpha',np.log(0.0001), np.log(0.05) ),\n",
        "    'learning_rate':hp.choice('c8_learning_rate', ['constant','adaptive'],)\n",
        "    },\n",
        "  'GaussianNB':\n",
        "  {  },\n",
        "  'BaggingClassifier':\n",
        "            {      \n",
        "              'base_estimator': hp.choice('base_estimator',[DecisionTreeClassifier()]),\n",
        "              'n_estimators': ho_scope.int(hp.quniform('n_estimators', 5, 25, 15)),\n",
        "              'oob_score': hp.choice('oob_score', [True,False]),\n",
        "               'random_state': 1\n",
        "            }\n",
        "                         \n",
        "       \n",
        "} \n",
        "\n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2,random_state = 0)\n",
        "\n",
        "best_clf3=[]\n",
        "final_clf=[]\n",
        "clf1_val_score=[]\n",
        "y_score=[]\n",
        "Fr_classifier=[]\n",
        "a1=[]\n",
        "classifiers=[]\n",
        "for i in range(len(try1)):\n",
        "    param_hyperopt3={try1[i][0].__name__:param_hyperopt[try1[i][0].__name__]}\n",
        "    clf_name=try1[i][0].__name__\n",
        "    cls=try1[i][0]\n",
        "    def f_clf1(params):\n",
        "       if not (params[clf_name]):   \n",
        "           model6 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),(clf_name,cls(**f_unpack_dict(params)))])\n",
        "           return model6\n",
        "       else:\n",
        "            model6 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),\n",
        "                                    (clf_name,cls(**f_unpack_dict(params[clf_name])))])\n",
        "            return model6\n",
        "    def objective_function(params,X_train2, y_train2):\n",
        "        model=f_clf1(params)\n",
        "        shuffle = KFold(n_splits=5, shuffle=True)\n",
        "        score = cross_val_score(model, X_train2, y_train2, cv=shuffle,scoring='roc_auc', n_jobs=-1).mean()\n",
        "        return {'loss': -score, 'status': STATUS_OK}  \n",
        "    trials = Trials()\n",
        "    best_clf3.append(fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                    param_hyperopt3, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))) \n",
        "    final_clf.append(f_clf1(space_eval(param_hyperopt3, best_clf3[i])).fit(X_train2, y_train2)) \n",
        "    # Calculating performance on validation set\n",
        "    y_score.append(final_clf[i].predict_proba(X_test2))  \n",
        "    clf1_val_score.append(roc_auc_score(y_test2, y_score[i][:,1])) \n",
        "    print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.format(-trials.best_trial['result']['loss'],\n",
        "                                                                       clf1_val_score[i]))\n",
        "    a1.append(space_eval(param_hyperopt3, best_clf3[i]))\n",
        "    classifiers.append({'classifier':cls,'Cross-val score':-trials.best_trial['result']['loss'],\n",
        "                        'validation score':clf1_val_score[i],'parameters':list(a1[i].values())[0]})\n",
        "classifiers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 20%|██        | 2/10 [02:34<12:12, 91.59s/it, best loss: -0.8811879851963225] "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [10:12<00:00, 77.07s/it, best loss: -0.889145641582062]\n",
            "Cross-val score: 0.88915; validation score: 0.88904\n",
            "  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [15:19<00:00, 95.05s/it, best loss: -0.8862456887587555]\n",
            "Cross-val score: 0.88625; validation score: 0.88182\n",
            " 50%|█████     | 5/10 [09:18<09:24, 112.82s/it, best loss: -0.8713451934772849]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [17:07<00:00, 80.95s/it, best loss: -0.8846250315039053]\n",
            "Cross-val score: 0.88463; validation score: 0.88352\n",
            "100%|██████████| 10/10 [41:46<00:00, 229.90s/it, best loss: -0.7629741618747085]\n",
            "Cross-val score: 0.76297; validation score: 0.76448\n",
            "  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [1:08:22<00:00, 392.89s/it, best loss: -0.8961590358829536]\n",
            "Cross-val score: 0.89616; validation score: 0.89891\n",
            "100%|██████████| 10/10 [02:34<00:00, 15.39s/it, best loss: -0.7520561338733224]\n",
            "Cross-val score: 0.75206; validation score: 0.78413\n",
            "  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [04:05<00:00, 24.51s/it, best loss: -0.8268494098728028]\n",
            "Cross-val score: 0.82685; validation score: 0.82928\n",
            "100%|██████████| 10/10 [38:50<00:00, 253.82s/it, best loss: -0.9013294538476375]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cross-val score: 0.90133; validation score: 0.90214\n",
            "100%|██████████| 10/10 [03:18<00:00, 19.86s/it, best loss: -0.7867791499718135]\n",
            "Cross-val score: 0.78678; validation score: 0.78877\n",
            "  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [19:24<00:00, 146.92s/it, best loss: -0.9030990105443255]\n",
            "Cross-val score: 0.90310; validation score: 0.90485\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Cross-val score': 0.889145641582062,\n",
              "  'classifier': sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier,\n",
              "  'parameters': {'l2_regularization': 0.0,\n",
              "   'learning_rate': 0.029641454778718972,\n",
              "   'max_bins': 100,\n",
              "   'max_depth': 10,\n",
              "   'max_iter': 175,\n",
              "   'max_leaf_nodes': 20,\n",
              "   'min_samples_leaf': 15,\n",
              "   'n_iter_no_change': None,\n",
              "   'tol': 1e-07,\n",
              "   'validation_fraction': 0.1},\n",
              "  'validation score': 0.8890387314781489},\n",
              " {'Cross-val score': 0.8862456887587555,\n",
              "  'classifier': sklearn.ensemble._forest.ExtraTreesClassifier,\n",
              "  'parameters': {'criterion': 'entropy',\n",
              "   'max_depth': 15,\n",
              "   'max_features': 15,\n",
              "   'min_samples_leaf': 12,\n",
              "   'min_samples_split': 10,\n",
              "   'n_estimators': 20},\n",
              "  'validation score': 0.8818171380065061},\n",
              " {'Cross-val score': 0.8846250315039053,\n",
              "  'classifier': sklearn.ensemble._forest.RandomForestClassifier,\n",
              "  'parameters': {'criterion': 'entropy',\n",
              "   'max_depth': 14,\n",
              "   'max_features': 'auto',\n",
              "   'min_samples_leaf': 2,\n",
              "   'min_samples_split': 6,\n",
              "   'n_estimators': 45},\n",
              "  'validation score': 0.8835185140616291},\n",
              " {'Cross-val score': 0.7629741618747085,\n",
              "  'classifier': sklearn.ensemble._weight_boosting.AdaBoostClassifier,\n",
              "  'parameters': {'algorithm': 'SAMME',\n",
              "   'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                          max_depth=1, max_features=None, max_leaf_nodes=None,\n",
              "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                          min_samples_leaf=1, min_samples_split=2,\n",
              "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                          random_state=None, splitter='best'),\n",
              "   'learning_rate': 0.30044074314959096,\n",
              "   'n_estimators': 80},\n",
              "  'validation score': 0.7644778116050867},\n",
              " {'Cross-val score': 0.8961590358829536,\n",
              "  'classifier': sklearn.ensemble._bagging.BaggingClassifier,\n",
              "  'parameters': {'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                          min_samples_leaf=1, min_samples_split=2,\n",
              "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                          random_state=None, splitter='best'),\n",
              "   'n_estimators': 30,\n",
              "   'oob_score': True,\n",
              "   'random_state': 1},\n",
              "  'validation score': 0.8989057692956279},\n",
              " {'Cross-val score': 0.7520561338733224,\n",
              "  'classifier': sklearn.tree._classes.DecisionTreeClassifier,\n",
              "  'parameters': {'max_depth': 6,\n",
              "   'max_features': 'sqrt',\n",
              "   'min_samples_leaf': 9,\n",
              "   'min_samples_split': 9},\n",
              "  'validation score': 0.7841260149694038},\n",
              " {'Cross-val score': 0.8268494098728028,\n",
              "  'classifier': sklearn.discriminant_analysis.LinearDiscriminantAnalysis,\n",
              "  'parameters': {},\n",
              "  'validation score': 0.8292819955028902},\n",
              " {'Cross-val score': 0.9013294538476375,\n",
              "  'classifier': lightgbm.sklearn.LGBMClassifier,\n",
              "  'parameters': {'colsample_bytree': 0.5,\n",
              "   'learning_rate': 0.09,\n",
              "   'max_bin': 80,\n",
              "   'max_depth': 10,\n",
              "   'min_child_samples': 80,\n",
              "   'n_iter_no_change': 10,\n",
              "   'num_iterations': 1000,\n",
              "   'num_leave': 10,\n",
              "   'reg_alpha': 50,\n",
              "   'reg_lambda': 5,\n",
              "   'scale_pos_weight': 50.0,\n",
              "   'subsample': 0.8},\n",
              "  'validation score': 0.9021447915521081},\n",
              " {'Cross-val score': 0.7867791499718135,\n",
              "  'classifier': sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis,\n",
              "  'parameters': {},\n",
              "  'validation score': 0.7887682267477347},\n",
              " {'Cross-val score': 0.9030990105443255,\n",
              "  'classifier': xgboost.sklearn.XGBClassifier,\n",
              "  'parameters': {'colsample_bytree': 1.0,\n",
              "   'eta': 0.125,\n",
              "   'gamma': 0.75,\n",
              "   'max_depth': 15,\n",
              "   'min_child_weight': 3,\n",
              "   'n_estimators': 15,\n",
              "   'n_iter_no_change': 5,\n",
              "   'objective': 'binary:logistic',\n",
              "   'subsample': 0.7468459518994333},\n",
              "  'validation score': 0.9048467840317365}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae7JMNadmIqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifiers=[{'Cross-val score': 0.889145641582062,\n",
        "  'classifier': try1[0][0],\n",
        "  'parameters': {'l2_regularization': 0.0,\n",
        "   'learning_rate': 0.029641454778718972,\n",
        "   'max_bins': 100,\n",
        "   'max_depth': 10,\n",
        "   'max_iter': 175,\n",
        "   'max_leaf_nodes': 20,\n",
        "   'min_samples_leaf': 15,\n",
        "   'n_iter_no_change': None,\n",
        "   'tol': 1e-07,\n",
        "   'validation_fraction': 0.1},\n",
        "  'validation score': 0.8890387314781489},\n",
        " {'Cross-val score': 0.8862456887587555,\n",
        "  'classifier': try1[1][0],\n",
        "  'parameters': {'criterion': 'entropy',\n",
        "   'max_depth': 15,\n",
        "   'max_features': 15,\n",
        "   'min_samples_leaf': 12,\n",
        "   'min_samples_split': 10,\n",
        "   'n_estimators': 20},\n",
        "  'validation score': 0.8818171380065061},\n",
        " {'Cross-val score': 0.8846250315039053,\n",
        "  'classifier': try1[2][0],\n",
        "  'parameters': {'criterion': 'entropy',\n",
        "   'max_depth': 14,\n",
        "   'max_features': 'auto',\n",
        "   'min_samples_leaf': 2,\n",
        "   'min_samples_split': 6,\n",
        "   'n_estimators': 45},\n",
        "  'validation score': 0.8835185140616291},\n",
        " {'Cross-val score': 0.7629741618747085,\n",
        "  'classifier': try1[3][0],\n",
        "  'parameters': {'algorithm': 'SAMME',\n",
        "   'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
        "                          max_depth=1, max_features=None, max_leaf_nodes=None,\n",
        "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                          min_samples_leaf=1, min_samples_split=2,\n",
        "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
        "                          random_state=None, splitter='best'),\n",
        "   'learning_rate': 0.30044074314959096,\n",
        "   'n_estimators': 80},\n",
        "  'validation score': 0.7644778116050867},\n",
        " {'Cross-val score': 0.8961590358829536,\n",
        "  'classifier': try1[4][0],\n",
        "  'parameters': {'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
        "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
        "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                          min_samples_leaf=1, min_samples_split=2,\n",
        "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
        "                          random_state=None, splitter='best'),\n",
        "   'n_estimators': 30,\n",
        "   'oob_score': True,\n",
        "   'random_state': 1},\n",
        "  'validation score': 0.8989057692956279},\n",
        " {'Cross-val score': 0.7520561338733224,\n",
        "  'classifier': try1[5][0],\n",
        "  'parameters': {'max_depth': 6,\n",
        "   'max_features': 'sqrt',\n",
        "   'min_samples_leaf': 9,\n",
        "   'min_samples_split': 9},\n",
        "  'validation score': 0.7841260149694038},\n",
        " {'Cross-val score': 0.8268494098728028,\n",
        "  'classifier': try1[6][0],\n",
        "  'parameters': {},\n",
        "  'validation score': 0.8292819955028902},\n",
        " {'Cross-val score': 0.9013294538476375,\n",
        "  'classifier': try1[7][0],\n",
        "  'parameters': {'colsample_bytree': 0.5,\n",
        "   'learning_rate': 0.09,\n",
        "   'max_bin': 80,\n",
        "   'max_depth': 10,\n",
        "   'min_child_samples': 80,\n",
        "   'n_iter_no_change': 10,\n",
        "   'num_iterations': 1000,\n",
        "   'num_leave': 10,\n",
        "   'reg_alpha': 50,\n",
        "   'reg_lambda': 5,\n",
        "   'scale_pos_weight': 50.0,\n",
        "   'subsample': 0.8},\n",
        "  'validation score': 0.9021447915521081},\n",
        " {'Cross-val score': 0.7867791499718135,\n",
        "  'classifier': try1[8][0],\n",
        "  'parameters': {},\n",
        "  'validation score': 0.7887682267477347},\n",
        " {'Cross-val score': 0.9030990105443255,\n",
        "  'classifier': try1[9][0],\n",
        "  'parameters': {'colsample_bytree': 1.0,\n",
        "   'eta': 0.125,\n",
        "   'gamma': 0.75,\n",
        "   'max_depth': 15,\n",
        "   'min_child_weight': 3,\n",
        "   'n_estimators': 15,\n",
        "   'n_iter_no_change': 5,\n",
        "   'objective': 'binary:logistic',\n",
        "   'subsample': 0.7468459518994333},\n",
        "  'validation score': 0.9048467840317365}]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmquMMt0cyx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[{'Cross-val score': 0.889145641582062,\n",
        "  'classifier': sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier,\n",
        "  'parameters': {'l2_regularization': 0.0,\n",
        "   'learning_rate': 0.029641454778718972,\n",
        "   'max_bins': 100,\n",
        "   'max_depth': 10,\n",
        "   'max_iter': 175,\n",
        "   'max_leaf_nodes': 20,\n",
        "   'min_samples_leaf': 15,\n",
        "   'n_iter_no_change': None,\n",
        "   'tol': 1e-07,\n",
        "   'validation_fraction': 0.1},\n",
        "  'validation score': 0.8890387314781489},\n",
        " {'Cross-val score': 0.8862456887587555,\n",
        "  'classifier': sklearn.ensemble._forest.ExtraTreesClassifier,\n",
        "  'parameters': {'criterion': 'entropy',\n",
        "   'max_depth': 15,\n",
        "   'max_features': 15,\n",
        "   'min_samples_leaf': 12,\n",
        "   'min_samples_split': 10,\n",
        "   'n_estimators': 20},\n",
        "  'validation score': 0.8818171380065061},\n",
        " {'Cross-val score': 0.8846250315039053,\n",
        "  'classifier': sklearn.ensemble._forest.RandomForestClassifier,\n",
        "  'parameters': {'criterion': 'entropy',\n",
        "   'max_depth': 14,\n",
        "   'max_features': 'auto',\n",
        "   'min_samples_leaf': 2,\n",
        "   'min_samples_split': 6,\n",
        "   'n_estimators': 45},\n",
        "  'validation score': 0.8835185140616291},\n",
        " {'Cross-val score': 0.7629741618747085,\n",
        "  'classifier': sklearn.ensemble._weight_boosting.AdaBoostClassifier,\n",
        "  'parameters': {'algorithm': 'SAMME',\n",
        "   'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
        "                          max_depth=1, max_features=None, max_leaf_nodes=None,\n",
        "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                          min_samples_leaf=1, min_samples_split=2,\n",
        "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
        "                          random_state=None, splitter='best'),\n",
        "   'learning_rate': 0.30044074314959096,\n",
        "   'n_estimators': 80},\n",
        "  'validation score': 0.7644778116050867},\n",
        " {'Cross-val score': 0.8961590358829536,\n",
        "  'classifier': sklearn.ensemble._bagging.BaggingClassifier,\n",
        "  'parameters': {'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
        "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
        "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                          min_samples_leaf=1, min_samples_split=2,\n",
        "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
        "                          random_state=None, splitter='best'),\n",
        "   'n_estimators': 30,\n",
        "   'oob_score': True,\n",
        "   'random_state': 1},\n",
        "  'validation score': 0.8989057692956279},\n",
        " {'Cross-val score': 0.7520561338733224,\n",
        "  'classifier': sklearn.tree._classes.DecisionTreeClassifier,\n",
        "  'parameters': {'max_depth': 6,\n",
        "   'max_features': 'sqrt',\n",
        "   'min_samples_leaf': 9,\n",
        "   'min_samples_split': 9},\n",
        "  'validation score': 0.7841260149694038},\n",
        " {'Cross-val score': 0.8268494098728028,\n",
        "  'classifier': sklearn.discriminant_analysis.LinearDiscriminantAnalysis,\n",
        "  'parameters': {},\n",
        "  'validation score': 0.8292819955028902},\n",
        " {'Cross-val score': 0.9013294538476375,\n",
        "  'classifier': lightgbm.sklearn.LGBMClassifier,\n",
        "  'parameters': {'colsample_bytree': 0.5,\n",
        "   'learning_rate': 0.09,\n",
        "   'max_bin': 80,\n",
        "   'max_depth': 10,\n",
        "   'min_child_samples': 80,\n",
        "   'n_iter_no_change': 10,\n",
        "   'num_iterations': 1000,\n",
        "   'num_leave': 10,\n",
        "   'reg_alpha': 50,\n",
        "   'reg_lambda': 5,\n",
        "   'scale_pos_weight': 50.0,\n",
        "   'subsample': 0.8},\n",
        "  'validation score': 0.9021447915521081},\n",
        " {'Cross-val score': 0.7867791499718135,\n",
        "  'classifier': sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis,\n",
        "  'parameters': {},\n",
        "  'validation score': 0.7887682267477347},\n",
        " {'Cross-val score': 0.9030990105443255,\n",
        "  'classifier': xgboost.sklearn.XGBClassifier,\n",
        "  'parameters': {'colsample_bytree': 1.0,\n",
        "   'eta': 0.125,\n",
        "   'gamma': 0.75,\n",
        "   'max_depth': 15,\n",
        "   'min_child_weight': 3,\n",
        "   'n_estimators': 15,\n",
        "   'n_iter_no_change': 5,\n",
        "   'objective': 'binary:logistic',\n",
        "   'subsample': 0.7468459518994333},\n",
        "  'validation score': 0.9048467840317365}]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-ZPMwgETLqx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "655d17a7-408a-4a61-e9c8-4b1c0fdf1d92"
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "from operator import itemgetter\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "newlist = sorted(classifiers, key=itemgetter('Cross-val score'), reverse=True)\n",
        "fiv=[]\n",
        "for i in range(len(newlist)):\n",
        "     if round(newlist[i]['Cross-val score'],2)>0.85:\n",
        "        fiv.append(newlist[i])  \n",
        "        \n",
        "names=[]\n",
        "for i, _ in enumerate(Accepted):\n",
        "    names.append(\"pipeline\"+str(i+1))  \n",
        "AcceptedR=[]\n",
        "for i in range(len(Accepted)):\n",
        "    AcceptedR.append(Accepted[i][1])\n",
        "dict = {}\n",
        "for name, Accepted_val in zip(names, AcceptedR ):\n",
        "    dict[name] =make_pipeline(Accepted_val(),fiv[AcceptedR.index(Accepted_val)]['classifier'](**fiv[AcceptedR.index(Accepted_val)]['parameters']))\n",
        "\n",
        "sclf2 = StackingClassifier(classifiers=list(dict.values()), meta_classifier=LogisticRegression()) \n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test2, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test2, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.77      0.76     94841\n",
            "           1       0.82      0.81      0.81    120517\n",
            "\n",
            "    accuracy                           0.79    215358\n",
            "   macro avg       0.79      0.79      0.79    215358\n",
            "weighted avg       0.79      0.79      0.79    215358\n",
            "\n",
            "StackingClassifier score: 0.913056\n",
            "Cross-val score: 0.87470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH6WnTQ0iK2I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "794339d3-e862-44d0-acda-f410ed60c834"
      },
      "source": [
        "fiv"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Cross-val score': 0.9030990105443255,\n",
              "  'classifier': xgboost.sklearn.XGBClassifier,\n",
              "  'parameters': {'colsample_bytree': 1.0,\n",
              "   'eta': 0.125,\n",
              "   'gamma': 0.75,\n",
              "   'max_depth': 15,\n",
              "   'min_child_weight': 3,\n",
              "   'n_estimators': 15,\n",
              "   'n_iter_no_change': 5,\n",
              "   'objective': 'binary:logistic',\n",
              "   'subsample': 0.7468459518994333},\n",
              "  'validation score': 0.9048467840317365},\n",
              " {'Cross-val score': 0.9013294538476375,\n",
              "  'classifier': lightgbm.sklearn.LGBMClassifier,\n",
              "  'parameters': {'colsample_bytree': 0.5,\n",
              "   'learning_rate': 0.09,\n",
              "   'max_bin': 80,\n",
              "   'max_depth': 10,\n",
              "   'min_child_samples': 80,\n",
              "   'n_iter_no_change': 10,\n",
              "   'num_iterations': 1000,\n",
              "   'num_leave': 10,\n",
              "   'reg_alpha': 50,\n",
              "   'reg_lambda': 5,\n",
              "   'scale_pos_weight': 50.0,\n",
              "   'subsample': 0.8},\n",
              "  'validation score': 0.9021447915521081},\n",
              " {'Cross-val score': 0.8961590358829536,\n",
              "  'classifier': sklearn.ensemble._bagging.BaggingClassifier,\n",
              "  'parameters': {'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                          max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                          min_samples_leaf=1, min_samples_split=2,\n",
              "                          min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                          random_state=None, splitter='best'),\n",
              "   'n_estimators': 30,\n",
              "   'oob_score': True,\n",
              "   'random_state': 1},\n",
              "  'validation score': 0.8989057692956279},\n",
              " {'Cross-val score': 0.889145641582062,\n",
              "  'classifier': sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier,\n",
              "  'parameters': {'l2_regularization': 0.0,\n",
              "   'learning_rate': 0.029641454778718972,\n",
              "   'max_bins': 100,\n",
              "   'max_depth': 10,\n",
              "   'max_iter': 175,\n",
              "   'max_leaf_nodes': 20,\n",
              "   'min_samples_leaf': 15,\n",
              "   'n_iter_no_change': None,\n",
              "   'tol': 1e-07,\n",
              "   'validation_fraction': 0.1},\n",
              "  'validation score': 0.8890387314781489},\n",
              " {'Cross-val score': 0.8862456887587555,\n",
              "  'classifier': sklearn.ensemble._forest.ExtraTreesClassifier,\n",
              "  'parameters': {'criterion': 'entropy',\n",
              "   'max_depth': 15,\n",
              "   'max_features': 15,\n",
              "   'min_samples_leaf': 12,\n",
              "   'min_samples_split': 10,\n",
              "   'n_estimators': 20},\n",
              "  'validation score': 0.8818171380065061},\n",
              " {'Cross-val score': 0.8846250315039053,\n",
              "  'classifier': sklearn.ensemble._forest.RandomForestClassifier,\n",
              "  'parameters': {'criterion': 'entropy',\n",
              "   'max_depth': 14,\n",
              "   'max_features': 'auto',\n",
              "   'min_samples_leaf': 2,\n",
              "   'min_samples_split': 6,\n",
              "   'n_estimators': 45},\n",
              "  'validation score': 0.8835185140616291}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro64pK_AQBNh",
        "colab_type": "code",
        "outputId": "9ce97d55-8e2a-4665-8a04-8e99d0026d2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "param_hyperopt1= {\n",
        "  'HistGradientBoostingClassifier':\n",
        "            {'learning_rate': hp.loguniform('c3_learning_rate', np.log(0.01), np.log(0.1)),\n",
        "                'max_iter': ho_scope.int(hp.quniform('c3_max_iter', 10, 200, 5)),\n",
        "                'max_depth': ho_scope.int(hp.quniform('c3_max_depth', 2,14, 1)),\n",
        "                'max_leaf_nodes': ho_scope.int(hp.quniform('c3_max_leaf_nodes', 5, 35,5)),\n",
        "                'min_samples_leaf': ho_scope.int(hp.quniform('c3_min_samples_leaf', 5, 25,5)),\n",
        "                'max_bins': ho_scope.int(hp.quniform('c3_max_bins', 20, 200,20)),\n",
        "                'validation_fraction':0.1,\n",
        "                'n_iter_no_change':None,\n",
        "                 'tol':1e-07,\n",
        "                'l2_regularization':0.0 \n",
        "           }\n",
        "       \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2,random_state = 0)\n",
        "def f_clf1(params):\n",
        "  model6 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),\n",
        "             ('HistGradientBoostingClassifier',HistGradientBoostingClassifier(**f_unpack_dict(params['HistGradientBoostingClassifier'])))])\n",
        "  return model6    \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf1(params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf1 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2), param_hyperopt1, algo=tpe.suggest, max_evals=10,\n",
        "                 trials=trials, rstate=np.random.RandomState(1))\n",
        "clf1 = f_clf1(space_eval(param_hyperopt1, best_clf1)).fit(X_train2, y_train2)\n",
        "# Calculating performance on validation set\n",
        "y_score = clf1.predict_proba(X_test2)\n",
        "clf1_val_score = roc_auc_score(y_test2, y_score[:,1])\n",
        "#clf2_val_score = model_selection.cross_val_score(f_clf2, X_train2,  y_train2, cv=5, scoring='roc_auc')\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf1_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(space_eval(param_hyperopt, best_clf1))\n",
        "a1=space_eval(param_hyperopt1, best_clf1)\n",
        "Fr_classifier={'classifier':list(a1.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf1_val_score,'parameters':list(a1.values())[0]}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 10%|█         | 1/10 [01:40<15:07, 100.83s/it, best loss: -0.880866960072799]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [02:18<10:55, 81.89s/it, best loss: -0.880866960072799] "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 40%|████      | 4/10 [03:31<06:01, 60.24s/it, best loss: -0.880866960072799]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [04:00<04:13, 50.73s/it, best loss: -0.880866960072799]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [04:19<02:45, 41.38s/it, best loss: -0.880866960072799]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [05:14<02:15, 45.22s/it, best loss: -0.880866960072799]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [06:10<01:37, 48.52s/it, best loss: -0.8876534360829765]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [07:40<01:00, 60.89s/it, best loss: -0.8876534360829765]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 10/10 [09:10<00:00, 69.80s/it, best loss: -0.8890878006805651]\n",
            "Cross-val score: 0.88909; validation score: 0.88939\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Azd6FkhgUPtB",
        "colab_type": "code",
        "outputId": "744052d9-c38d-49a5-f323-fd39eee31fb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "Fr_classifier"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Cross-val score': 0.8890878006805651,\n",
              " 'classifier': 'HistGradientBoostingClassifier',\n",
              " 'parameters': {'l2_regularization': 0.0,\n",
              "  'learning_rate': 0.029641454778718972,\n",
              "  'max_bins': 100,\n",
              "  'max_depth': 10,\n",
              "  'max_iter': 175,\n",
              "  'max_leaf_nodes': 20,\n",
              "  'min_samples_leaf': 15,\n",
              "  'n_iter_no_change': None,\n",
              "  'tol': 1e-07,\n",
              "  'validation_fraction': 0.1},\n",
              " 'validation score': 0.889389222120687}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bkp-7STuMRJ7",
        "colab_type": "code",
        "outputId": "fe558f46-1e7e-4af1-a4ca-a52ef80b9b84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "param_hyperopt1= {\n",
        "   'ExtraTreesClassifier':\n",
        "            {\n",
        "                'max_depth': ho_scope.int(hp.quniform('c4_max_depth', 5, 20, 5)),\n",
        "                'n_estimators': ho_scope.int(hp.quniform('c4_n_estimators', 5, 50, 5)),\n",
        "                'max_features': ho_scope.int(hp.quniform('c4_max_features', 5, 20, 5)),\n",
        "                'criterion': hp.choice('c4_criterion', ['gini', 'entropy']),\n",
        "                'min_samples_split': ho_scope.int(hp.quniform('c4_min_samples_split', 2, 14,2)),\n",
        "                'min_samples_leaf': ho_scope.int(hp.quniform('c4_min_samples_leaf', 2, 14,2))      \n",
        "             }     \n",
        "       \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2,random_state = 0)\n",
        "def f_clf1(params):\n",
        "  model6 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),\n",
        "             ('ExtraTreesClassifier',ExtraTreesClassifier(**f_unpack_dict(params['ExtraTreesClassifier'])))])\n",
        "  return model6    \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf1(params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf1 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2), param_hyperopt1, algo=tpe.suggest, max_evals=10,\n",
        "                 trials=trials, rstate=np.random.RandomState(1))\n",
        "clf1 = f_clf1(space_eval(param_hyperopt1, best_clf1)).fit(X_train2, y_train2)\n",
        "# Calculating performance on validation set\n",
        "y_score = clf1.predict_proba(X_test2)\n",
        "clf1_val_score = roc_auc_score(y_test2, y_score[:,1])\n",
        "#clf2_val_score = model_selection.cross_val_score(f_clf2, X_train2,  y_train2, cv=5, scoring='roc_auc')\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf1_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(space_eval(param_hyperopt, best_clf1))\n",
        "a1=space_eval(param_hyperopt1, best_clf1)\n",
        "Fr_classifier={'classifier':list(a1.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf1_val_score,'parameters':list(a1.values())[0]}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-e777f8d0d739>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m best_clf1 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2), param_hyperopt1, algo=tpe.suggest, max_evals=10,\n\u001b[0;32m---> 55\u001b[0;31m                  trials=trials, rstate=np.random.RandomState(1))\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0mclf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_clf1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_hyperopt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_clf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Calculating performance on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             show_progressbar=show_progressbar)\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    405\u001b[0m                     show_progressbar=show_progressbar)\n\u001b[1;32m    406\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                         \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-e777f8d0d739>\u001b[0m in \u001b[0;36mobjective_function\u001b[0;34m(params, X_train2, y_train2)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mobjective_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf_clf1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'status'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSTATUS_OK\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    388\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    391\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 236\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha8jZBOPyWoB",
        "colab_type": "code",
        "outputId": "e944662d-c7b0-4e5d-d2be-df9c050f7649",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "Fr_classifier"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Cross-val score': 0.8865282784493205,\n",
              " 'classifier': 'ExtraTreesClassifier',\n",
              " 'parameters': {'criterion': 'entropy',\n",
              "  'max_depth': 15,\n",
              "  'max_features': 15,\n",
              "  'min_samples_leaf': 12,\n",
              "  'min_samples_split': 10,\n",
              "  'n_estimators': 20},\n",
              " 'validation score': 0.8852764300265377}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiM49RkuqGq3",
        "colab_type": "code",
        "outputId": "f891bf2d-4b27-4fd5-9030-6626cb1d9710",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "from sklearn.ensemble.forest import RandomForestClassifier\n",
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "param_hyperopt2= {\n",
        "  'RandomForestClassifier': \n",
        "            {\n",
        "            'max_depth': ho_scope.int(hp.quniform('c2_max_depth', 2, 14, 2)),\n",
        "            'n_estimators': ho_scope.int(hp.quniform('c2_n_estimators', 10, 50, 5)),\n",
        "            'max_features': hp.choice('c2_max_features', ['auto', 'sqrt']),\n",
        "            'criterion': hp.choice('c2_criterion', ['gini', 'entropy']),\n",
        "            'min_samples_split': ho_scope.int(hp.quniform('c2_min_samples_split', 2, 14,2)),\n",
        "            'min_samples_leaf': ho_scope.int(hp.quniform('c2_min_samples_leaf', 2, 14,2))\n",
        "                          \n",
        "         } \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "def f_clf2(params):\n",
        "  model4 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),('RandomForestClassifier',RandomForestClassifier(**f_unpack_dict(params['RandomForestClassifier'])))])\n",
        "  return model4    \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf2(params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf2 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                 param_hyperopt2, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "clf2 = f_clf2(space_eval(param_hyperopt2, best_clf2)).fit(X_train2, y_train2)\n",
        "# Calculating performance on validation set\n",
        "y_score = clf2.predict_proba(X_test2)\n",
        "clf2_val_score = roc_auc_score(y_test2, y_score[:,1])\n",
        "#clf2_val_score = model_selection.cross_val_score(f_clf2, X_train2,  y_train2, cv=5, scoring='roc_auc')\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf2_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(\n",
        "a2=space_eval(param_hyperopt2, best_clf2)\n",
        "Se_classifier={'classifier':list(a2.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf2_val_score,'parameters':list(a2.values())[0]}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 20%|██        | 2/10 [03:24<12:12, 91.55s/it, best loss: -0.8579616542577266]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 50%|█████     | 5/10 [09:25<09:31, 114.23s/it, best loss: -0.8739648796765478]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [17:46<00:00, 86.68s/it, best loss: -0.8838360349561534]\n",
            "Cross-val score: 0.88384; validation score: 0.87969\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_cLstq3kPbq",
        "colab_type": "code",
        "outputId": "9ece8ef3-8154-406c-8665-d58c02a1c226",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        " RandomForestClassifier()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_Qo9loEyJ9x",
        "colab_type": "code",
        "outputId": "484342be-24f6-446d-a6e1-24526623b584",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "best_clf2\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'c2_criterion': 1,\n",
              " 'c2_max_depth': 10.0,\n",
              " 'c2_max_features': 0,\n",
              " 'c2_min_samples_leaf': 2.0,\n",
              " 'c2_min_samples_split': 4.0,\n",
              " 'c2_n_estimators': 35.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKMWqZQ8L3JV",
        "colab_type": "code",
        "outputId": "9f698df3-bb1a-4140-a311-58fa44b0117b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.utils import shuffle\n",
        "param_hyperopt3= {      \n",
        "       'BaggingClassifier':\n",
        "            {      \n",
        "             'base_estimator': hp.choice('base_estimator',[DecisionTreeClassifier()]),\n",
        "              'n_estimators': ho_scope.int(hp.quniform('n_estimators', 10, 50, 10)),\n",
        "              'oob_score': hp.choice('oob_score', [True,False]),\n",
        "               'random_state': 1\n",
        "            }\n",
        "                  \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "def f_clf3(params):\n",
        "   model1 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),('BaggingClassifier', BaggingClassifier(**f_unpack_dict(params['BaggingClassifier'])))])   \n",
        "   return model1\n",
        "        \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf3(params)\n",
        "    shuffle = KFold(n_splits=5, shuffle=True)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=shuffle,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf3 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                param_hyperopt3, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "clf3 = f_clf3(space_eval(param_hyperopt3, best_clf3)).fit(X_train2, y_train2)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf3_val_score = roc_auc_score(y_test, clf3.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf3_val_score))\n",
        "print('Best parameters:')\n",
        "print(space_eval(param_hyperopt3, best_clf3))\n",
        "# Calculating performance on validation set\n",
        "clf3_val_score = roc_auc_score(y_test, clf3.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf3_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(\n",
        "a3=space_eval(param_hyperopt3, best_clf3)\n",
        "Thrid_classifier={'classifier':list(a3.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf3_val_score,'parameters':list(a3.values())[0]}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 50%|█████     | 5/10 [1:04:13<1:04:12, 770.49s/it, best loss: -0.8983805004954327]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [2:00:02<00:00, 687.90s/it, best loss: -0.8983805004954327]\n",
            "Cross-val score: 0.89838; validation score: 0.93051\n",
            "Best parameters:\n",
            "{'BaggingClassifier': {'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
            "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
            "                       random_state=None, splitter='best'), 'n_estimators': 50, 'oob_score': True, 'random_state': 1}}\n",
            "Cross-val score: 0.89838; validation score: 0.93051\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-AC9k5VLzSY",
        "colab_type": "code",
        "outputId": "70ecde8f-4e17-4b5f-9b6e-b36238505ec4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "Thrid_classifier\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Cross-val score': 0.8983805004954327,\n",
              " 'classifier': 'BaggingClassifier',\n",
              " 'parameters': {'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                         max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                         min_samples_leaf=1, min_samples_split=2,\n",
              "                         min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                         random_state=None, splitter='best'),\n",
              "  'n_estimators': 50,\n",
              "  'oob_score': True,\n",
              "  'random_state': 1},\n",
              " 'validation score': 0.9305090729151654}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB0p-1daEvuD",
        "colab_type": "code",
        "outputId": "df30d798-b8ae-45ff-c27b-c7c91840591b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.utils import shuffle\n",
        "param_hyperopt3= {      \n",
        "       'BaggingClassifier':\n",
        "            {      \n",
        "             'base_estimator': hp.choice('base_estimator',[DecisionTreeClassifier()]),\n",
        "              'n_estimators': ho_scope.int(hp.quniform('n_estimators', 5, 25, 15)),\n",
        "              'oob_score': hp.choice('oob_score', [True,False]),\n",
        "               'random_state': 1\n",
        "            }\n",
        "                \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "def f_clf3(params):\n",
        "   model1 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),('BaggingClassifier', BaggingClassifier(**f_unpack_dict(params['BaggingClassifier'])))])   \n",
        "   return model1\n",
        "        \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf3(params)\n",
        "    shuffle = KFold(n_splits=5, shuffle=True)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=shuffle,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf3 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                param_hyperopt3, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "clf3 = f_clf3(space_eval(param_hyperopt3, best_clf3)).fit(X_train2, y_train2)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf3_val_score = roc_auc_score(y_test, clf3.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf3_val_score))\n",
        "print('Best parameters:')\n",
        "print(space_eval(param_hyperopt3, best_clf3))\n",
        "# Calculating performance on validation set\n",
        "clf3_val_score = roc_auc_score(y_test, clf3.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf3_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(\n",
        "a3=space_eval(param_hyperopt3, best_clf3)\n",
        "Thrid_classifier={'classifier':list(a3.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf3_val_score,'parameters':list(a3.values())[0]}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 10%|█         | 1/10 [11:53<1:46:59, 713.31s/it, best loss: -0.895919376299091]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-234a05933d03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m best_clf3 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n\u001b[0;32m---> 56\u001b[0;31m                 param_hyperopt3, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mclf3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_clf3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_hyperopt3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_clf3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             show_progressbar=show_progressbar)\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    405\u001b[0m                     show_progressbar=show_progressbar)\n\u001b[1;32m    406\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                         \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-234a05933d03>\u001b[0m in \u001b[0;36mobjective_function\u001b[0;34m(params, X_train2, y_train2)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf_clf3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'status'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSTATUS_OK\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    388\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    391\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 236\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j37o6YxZZgrL",
        "colab_type": "code",
        "outputId": "b6dca997-d289-4f4b-8d30-2221de374c28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "Thrid_classifier"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Cross-val score': 0.8956732554663308,\n",
              " 'classifier': 'BaggingClassifier',\n",
              " 'parameters': {'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                         max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                         min_samples_leaf=1, min_samples_split=2,\n",
              "                         min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                         random_state=None, splitter='best'),\n",
              "  'n_estimators': 30,\n",
              "  'oob_score': True,\n",
              "  'random_state': 1},\n",
              " 'validation score': 0.9280463385108305}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O2unkUj6im8",
        "colab_type": "code",
        "outputId": "e7f447e1-0d90-49ba-a52a-ccc3ee1e549f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.utils import shuffle\n",
        "param_hyperopt3= {      \n",
        "       'AdaBoostClassifier':\n",
        "   {\n",
        "         'base_estimator':hp.choice('base_estimator', [DecisionTreeClassifier(max_depth=1)]),\n",
        "          'learning_rate': hp.loguniform('c5_learning_rate', np.log(0.01), np.log(1)),\n",
        "          'n_estimators': ho_scope.int(hp.quniform('c5_n_estimators', 40, 140, 40)),\n",
        "          'algorithm': hp.choice('c5_algorithm',[\"SAMME\"])  \n",
        "       }\n",
        "                \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "def f_clf3(params):\n",
        "   model1 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),('AdaBoostClassifier', AdaBoostClassifier(**f_unpack_dict(params['AdaBoostClassifier'])))])   \n",
        "   return model1\n",
        "        \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf3(params)\n",
        "    shuffle = KFold(n_splits=5, shuffle=True)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=shuffle,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf3 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                param_hyperopt3, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "clf3 = f_clf3(space_eval(param_hyperopt3, best_clf3)).fit(X_train2, y_train2)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf3_val_score = roc_auc_score(y_test, clf3.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf3_val_score))\n",
        "print('Best parameters:')\n",
        "print(space_eval(param_hyperopt3, best_clf3))\n",
        "# Calculating performance on validation set\n",
        "clf3_val_score = roc_auc_score(y_test, clf3.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf3_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(\n",
        "a3=space_eval(param_hyperopt3, best_clf3)\n",
        "fifth_classifier={'classifier':list(a3.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf3_val_score,'parameters':list(a3.values())[0]}\n",
        "fifth_classifier"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [45:42<00:00, 250.16s/it, best loss: -0.7632769733847417]\n",
            "Cross-val score: 0.76328; validation score: 0.76264\n",
            "Best parameters:\n",
            "{'AdaBoostClassifier': {'algorithm': 'SAMME', 'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
            "                       max_depth=1, max_features=None, max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
            "                       random_state=None, splitter='best'), 'learning_rate': 0.30044074314959096, 'n_estimators': 80}}\n",
            "Cross-val score: 0.76328; validation score: 0.76264\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Cross-val score': 0.7632769733847417,\n",
              " 'classifier': 'AdaBoostClassifier',\n",
              " 'parameters': {'algorithm': 'SAMME',\n",
              "  'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                         max_depth=1, max_features=None, max_leaf_nodes=None,\n",
              "                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                         min_samples_leaf=1, min_samples_split=2,\n",
              "                         min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                         random_state=None, splitter='best'),\n",
              "  'learning_rate': 0.30044074314959096,\n",
              "  'n_estimators': 80},\n",
              " 'validation score': 0.7626419534117789}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6ZkDVZl8N3H",
        "colab_type": "code",
        "outputId": "f5be2124-01bf-415b-b075-7ec9250df240",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "AdaBoostClassifier(),DecisionTreeClassifier()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
              "                    n_estimators=50, random_state=None),\n",
              " DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                        max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                        min_samples_leaf=1, min_samples_split=2,\n",
              "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                        random_state=None, splitter='best'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv4WPj9XX5OS",
        "colab_type": "code",
        "outputId": "574f1cb6-df71-4c2e-de27-2b844774672f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.utils import shuffle\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "import xgboost as xgb\n",
        "\n",
        "param_hyperopt4= {\n",
        "  'XGBClassifier':\n",
        "      {\n",
        "           'max_depth': ho_scope.int(hp.quniform('c16_max_depth', 5, 15, 5)),\n",
        "           'min_child_weight': hp.choice ('x_min_child',np.arange(1, 10, dtype=int)),\n",
        "           'subsample': hp.uniform ('x_subsample', 0.2, 1),\n",
        "           'n_estimators': ho_scope.int(hp.quniform('c2_n_estimators', 5, 45, 15)),\n",
        "       }         \n",
        "       \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "def f_clf4(params):\n",
        "   model1 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),('XGBClassifier', xgb.XGBClassifier())])   \n",
        "   return model1  \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf4(params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf4 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                param_hyperopt4, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "clf4 = f_clf4(space_eval(param_hyperopt4, best_clf4)).fit(X_train2, y_train2)\n",
        "# Calculating performance on validation set\n",
        "clf4_val_score = roc_auc_score(y_test, clf4.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf4_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(\n",
        "a4=space_eval(param_hyperopt4, best_clf4)\n",
        "Four_classifier={'classifier':list(a4.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf4_val_score,'parameters':list(a4.values())[0]}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [03:11<28:42, 191.36s/it, best loss: -0.8774978395308153]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [06:26<25:39, 192.48s/it, best loss: -0.8774978395308153]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [09:38<22:25, 192.27s/it, best loss: -0.8774978395308153]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [12:51<19:15, 192.63s/it, best loss: -0.8774978395308153]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [16:03<16:01, 192.33s/it, best loss: -0.8774978395308153]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [19:15<12:48, 192.16s/it, best loss: -0.8774978395308153]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 80%|████████  | 8/10 [25:44<06:27, 193.70s/it, best loss: -0.8774978395308153]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [29:01<03:14, 194.72s/it, best loss: -0.8774978395308153]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 10/10 [32:21<00:00, 196.06s/it, best loss: -0.8774978395308153]\n",
            "Cross-val score: 0.87750; validation score: 0.87816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmuOPHfZZL9T",
        "colab_type": "code",
        "outputId": "56963711-3dbd-4674-e8d5-b749c222d5ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "Four_classifier\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Cross-val score': 0.8774978395308153,\n",
              " 'classifier': 'XGBClassifier',\n",
              " 'parameters': {'max_depth': 10,\n",
              "  'min_child_weight': 8,\n",
              "  'n_estimators': 15,\n",
              "  'subsample': 0.3916405019248916},\n",
              " 'validation score': 0.8781637598970042}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6WfRi1-5dev",
        "colab_type": "code",
        "outputId": "5a058986-ccc6-4f86-c8ee-3964b150784f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll import scope as ho_scope\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.utils import shuffle\n",
        "import lightgbm as lgb\n",
        "from functools import partial\n",
        "\n",
        "param_hyperopt5= {\n",
        "   'LGBMClassifier1':\n",
        "                  {\n",
        "                   'max_depth': ho_scope.int(hp.quniform('c16_max_depth', -2, 10, 1)),\n",
        "                   'num_leave': ho_scope.int(hp.quniform('c16_num_leaves', 10, 200, 10)),\n",
        "                   'min_child_samples': ho_scope.int(hp.quniform('c16_min_child_samples', 10, 90, 10)),\n",
        "                   'scale_pos_weight': ho_scope.int(hp.quniform('c16_scale_pos_weight', 10, 90, 10)),\n",
        "                   'subsample': ho_scope.float(hp.quniform('c16_subsample', 0.1, 0.9, 0.1)),\n",
        "                   'colsample_bytree': ho_scope.float(hp.quniform('c16_colsample_bytree', 0.1, 0.9, 0.1)),\n",
        "                   'reg_lambda': hp.choice(\"c16_reg_lambda\",[0, 1e-1, 1, 5, 10, 20, 50]),\n",
        "                   'reg_alpha': hp.choice(\"c16_reg_alpha\",[0, 1e-1, 1, 5, 10, 20, 50]),\n",
        "                   'bagging_fraction': ho_scope.float(hp.quniform('c16_bagging_fraction', 0.1, 0.9, 0.05)),\n",
        "                    'bagging_freq': ho_scope.int(hp.quniform('c6_bagging_freq', 1, 8, 1)),\n",
        "                   'min_data_in_leaf': ho_scope.int(hp.quniform('c16_min_data_in_leaf', 100, 1000, 100)),\n",
        "                  'min_sum_hessian_in_leaf': ho_scope.int(hp.quniform('c16_min_sum_hessian_in_leaf', 5, 20, 5)),\n",
        "                   'max_bin': ho_scope.int(hp.quniform('c16_max_bin', 10, 100, 10)),\n",
        "                   'learning_rate': ho_scope.float(hp.quniform('c16_learning_rate', 0.001, 0.09, 0.005)),\n",
        "                   'num_iterations': ho_scope.int(hp.quniform('c16_num_iterations', 100, 10000, 100)),\n",
        "                   'n_iter_no_change':10\n",
        "                       },\n",
        "                  \n",
        "                   'LGBMClassifier':\n",
        "                  {\n",
        "                     'max_depth': ho_scope.int(hp.quniform('c6_max_depth', 2, 14, 2)),\n",
        "                   'num_leave': ho_scope.int(hp.quniform('c6_num_leaves', 5, 30, 5)),\n",
        "                   'min_child_samples': ho_scope.int(hp.quniform('c6_min_child_samples', 50, 100, 10)),\n",
        "                   'scale_pos_weight': ho_scope.float(hp.quniform('c6_scale_pos_weight', 50, 100, 10)),\n",
        "                   'subsample': ho_scope.float(hp.quniform('c6_subsample', 0.2, 0.9, 0.1)),\n",
        "                   'colsample_bytree': ho_scope.float(hp.quniform('c6_colsample_bytree', 0.2, 0.9, 0.1)),\n",
        "                   'reg_lambda': hp.choice(\"c6_reg_lambda\",[0, 1e-1, 1, 5, 10, 20, 50]),\n",
        "                   'reg_alpha': hp.choice(\"c6_reg_alpha\",[0, 1e-1, 1, 5, 10, 20, 50]) ,\n",
        "                   'max_bin': ho_scope.int(hp.quniform('c6_max_bin', 10, 200, 10)),\n",
        "                   'learning_rate': ho_scope.float(hp.quniform('c6_learning_rate', 0.001, 0.09, 0.005)),\n",
        "                   'num_iterations': ho_scope.int(hp.quniform('c6_num_iterations', 100, 2000, 100)),\n",
        "                    'n_iter_no_change':10,\n",
        "                   'metric' : 'binary_error'\n",
        "                   }     \n",
        "} \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "def f_clf5(params):\n",
        "   model1 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),('LGBMClassifier', \n",
        "                                                         lgb.LGBMClassifier(**f_unpack_dict(params['LGBMClassifier'])))])   \n",
        "   return model1  \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf5(params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf5 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                param_hyperopt5, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "clf5 = f_clf5(space_eval(param_hyperopt5, best_clf5)).fit(X_train2, y_train2)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf5_val_score = roc_auc_score(y_test, clf5.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf5_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(\n",
        "a5=space_eval(param_hyperopt5, best_clf5)\n",
        "fith_classifier={'classifier':list(a5.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf5_val_score,'parameters':list(a5.values())[0]}"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 10%|█         | 1/10 [04:31<40:42, 271.35s/it, best loss: -0.8997379383173877]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [34:46<00:00, 226.51s/it, best loss: -0.9012358759310123]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cross-val score: 0.90124; validation score: 0.90342\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn4Xx2mh_YYt",
        "colab_type": "code",
        "outputId": "f1b46ade-1411-4dae-fb74-d66cbaf587ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "fith_classifier"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Cross-val score': 0.9012358759310123,\n",
              " 'classifier': 'LGBMClassifier',\n",
              " 'parameters': {'colsample_bytree': 0.5,\n",
              "  'learning_rate': 0.09,\n",
              "  'max_bin': 80,\n",
              "  'max_depth': 10,\n",
              "  'min_child_samples': 80,\n",
              "  'n_iter_no_change': 10,\n",
              "  'num_iterations': 1000,\n",
              "  'num_leave': 10,\n",
              "  'reg_alpha': 50,\n",
              "  'reg_lambda': 5,\n",
              "  'scale_pos_weight': 50.0,\n",
              "  'subsample': 0.8},\n",
              " 'validation score': 0.9032920414079589}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3If0WbQtPcQM",
        "colab_type": "code",
        "outputId": "b9cd8772-57f9-4646-ad28-db0027affb3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "from hyperopt.pyll import scope as ho_scope\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.utils import shuffle\n",
        "param_hyperopt6= {\n",
        "    'QuadraticDiscriminantAnalysis':\n",
        "      {\n",
        "           \n",
        "      }      \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "def f_clf6(params):\n",
        "   model3 =Pipeline(steps=[('encoder',Accepted[5][1]()),('QuadraticDiscriminantAnalysis',QuadraticDiscriminantAnalysis())])\n",
        "   return model3 \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf6(params)\n",
        "    #clf = RandomForestClassifier(**params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf6 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                param_hyperopt6, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "\n",
        "clf6 = f_clf6(space_eval(param_hyperopt6, best_clf6)).fit(X_train2, y_train2)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf6_val_score = roc_auc_score(y_test, clf6.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.format(-trials.best_trial['result']['loss'], clf6_val_score))\n",
        "print('Best parameters:')\n",
        "print(space_eval(param_hyperopt6, best_clf6))\n",
        "# Calculating performance on validation set\n",
        "clf6_val_score = roc_auc_score(y_test, clf6.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf6_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(\n",
        "a6=space_eval(param_hyperopt6, best_clf6)\n",
        "six_classifier={'classifier':list(a6.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf6_val_score,'parameters':list(a6.values())[0]}"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [02:04<00:00, 12.22s/it, best loss: -0.7861718985031253]\n",
            "Cross-val score: 0.78617; validation score: 0.78727\n",
            "Best parameters:\n",
            "{'QuadraticDiscriminantAnalysis': {}}\n",
            "Cross-val score: 0.78617; validation score: 0.78727\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oUvZj4cvDNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
        "# now you can import normally from ensemble\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "pipe1 = make_pipeline(Accepted[3][1](),XGBClassifier())\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier())  \n",
        "pipe3 = make_pipeline(Accepted[0][1](),ExtraTreesClassifier()) \n",
        "pipe6 = make_pipeline(Accepted[4][1](),lgb.LGBMClassifier()) \n",
        "pipe4 = make_pipeline(Accepted[2][1](),HistGradientBoostingClassifier())\n",
        "pipe5 = make_pipeline(Accepted[5][1](),QuadraticDiscriminantAnalysis())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe5], meta_classifier=LogisticRegression()) \n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test2, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test2, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZRFjh3jzf8x",
        "colab_type": "code",
        "outputId": "31834de4-363c-4746-9188-bff9785af0a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
        "# now you can import normally from ensemble\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier())  \n",
        "pipe3 = make_pipeline(Accepted[0][1](),ExtraTreesClassifier()) \n",
        "pipe6 = make_pipeline(Accepted[3][1](),lgb.LGBMClassifier()) \n",
        "pipe4 = make_pipeline(Accepted[2][1](),HistGradientBoostingClassifier())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe2,pipe3,pipe4,pipe6], meta_classifier=LogisticRegression()) \n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test2, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test2, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.77      0.77     94841\n",
            "           1       0.82      0.81      0.81    120517\n",
            "\n",
            "    accuracy                           0.79    215358\n",
            "   macro avg       0.79      0.79      0.79    215358\n",
            "weighted avg       0.79      0.79      0.79    215358\n",
            "\n",
            "StackingClassifier score: 0.913729\n",
            "Cross-val score: 0.86096\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DFF7PDGoM7j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
        "# now you can import normally from ensemble\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "pipe1 = make_pipeline(Accepted[3][1](),XGBClassifier())\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier())  \n",
        "pipe3 = make_pipeline(Accepted[0][1](),ExtraTreesClassifier()) \n",
        "pipe6 = make_pipeline(Accepted[4][1](),lgb.LGBMClassifier()) \n",
        "pipe4 = make_pipeline(Accepted[2][1](),HistGradientBoostingClassifier())\n",
        "pipe5 = make_pipeline(Accepted[5][1](), BaggingClassifier(DecisionTreeClassifier()) )\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe5], meta_classifier=LogisticRegression()) \n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test2, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test2, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA4P3yftv5VC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Fr_classifier={'Cross-val score': 0.8755075622861487,\n",
        " 'classifier': 'ExtraTreesClassifier',\n",
        " 'parameters': {'criterion': 'entropy',\n",
        "  'max_depth': 15,\n",
        "  'max_features': 13,\n",
        "  'min_samples_leaf': 4,\n",
        "  'min_samples_split': 4,\n",
        "  'n_estimators': 13},\n",
        " 'validation score': 0.884938764808794}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72q7zxSvwQK6",
        "colab_type": "code",
        "outputId": "367c1766-0447-4db5-c048-a6e2f9b376a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "Fr_classifier['parameters']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'criterion': 'entropy',\n",
              " 'max_depth': 15,\n",
              " 'max_features': 13,\n",
              " 'min_samples_leaf': 4,\n",
              " 'min_samples_split': 4,\n",
              " 'n_estimators': 13}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT1x5xQ-vWFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Se_classifier={'Cross-val score': 0.8273926245659394,\n",
        " 'classifier': 'RandomForestClassifier',\n",
        " 'parameters': {'criterion': 'gini',\n",
        "  'max_depth': 7,\n",
        "  'max_features': 'auto',\n",
        "  'min_samples_leaf': 2,\n",
        "  'min_samples_split': 3,\n",
        "  'n_estimators': 23},\n",
        " 'validation score': 0.7973701565847332}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jb9jiievacj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Thrid_classifier={'Cross-val score': 0.8268064630182487,\n",
        " 'classifier': 'LinearDiscriminantAnalysis',\n",
        " 'parameters': {},\n",
        " 'validation score': 0.827194350739109}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWLcgpDlvecC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Four_classifier={'Cross-val score': 0.8761819849027198,\n",
        " 'classifier': 'XGBClassifier',\n",
        " 'parameters': {'max_depth': 7,\n",
        "  'min_child_weight': 1,\n",
        "  'n_estimators': 38,\n",
        "  'subsample': 0.8479101254812229},\n",
        " 'validation score': 0.8770024548105813}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT7GCH_evh_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fith_classifier={'Cross-val score': 0.8931760781045686,\n",
        " 'classifier': 'LGBMClassifier',\n",
        " 'parameters': {'colsample_bytree': 1.0,\n",
        "  'max_depth': 9,\n",
        "  'min_child_samples': 70,\n",
        "  'num_leave': 10,\n",
        "  'reg_alpha': 5,\n",
        "  'reg_lambda': 20,\n",
        "  'scale_pos_weight': 50.0,\n",
        "  'subsample': 1.0},\n",
        " 'validation score': 0.8944420319608248}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOeAy9aqvmT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "six_classifier={'Cross-val score': 0.786171897798788,\n",
        " 'classifier': 'QuadraticDiscriminantAnalysis',\n",
        " 'parameters': {},\n",
        " 'validation score': 0.787235957146868}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoY5YxNhSY0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a8={'ExtraTreesClassifier': {'criterion': 'gini',\n",
        "  'max_depth': 7,\n",
        "  'max_features': 7,\n",
        "  'min_samples_leaf': 4,\n",
        "  'min_samples_split': 4,\n",
        "  'n_estimators': 19},\n",
        " 'LGBMClassifier': {'colsample_bytree': 1.0,\n",
        "  'max_depth': 7,\n",
        "  'min_child_samples': 80,\n",
        "  'num_leave': 50,\n",
        "  'reg_alpha': 0,\n",
        "  'reg_lambda': 10,\n",
        "  'scale_pos_weight': 70.0,\n",
        "  'subsample': 1.0},\n",
        " 'RandomForestClassifier': {'criterion': 'entropy',\n",
        "  'max_depth': 5,\n",
        "  'max_features': 'sqrt',\n",
        "  'min_samples_leaf': 3,\n",
        "  'min_samples_split': 2,\n",
        "  'n_estimators': 30},\n",
        " 'XGBClassifier': {'max_depth': 13,\n",
        "  'min_child_weight': 3,\n",
        "  'subsample': 0.9406880083709813}}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQZrDLCO4tzY",
        "colab_type": "code",
        "outputId": "c582b68c-85a5-4e0b-8489-5f14779fcd65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "{'ExtraTreesClassifier': {'criterion': 'gini',\n",
        "  'max_depth': 7,\n",
        "  'max_features': 7,\n",
        "  'min_samples_leaf': 4,\n",
        "  'min_samples_split': 4,\n",
        "  'n_estimators': 19},\n",
        " 'LGBMClassifier': {'colsample_bytree': 1.0,\n",
        "  'max_depth': 7,\n",
        "  'min_child_samples': 80,\n",
        "  'num_leave': 50,\n",
        "  'reg_alpha': 0,\n",
        "  'reg_lambda': 10,\n",
        "  'scale_pos_weight': 70.0,\n",
        "  'subsample': 1.0},\n",
        " 'RandomForestClassifier': {'criterion': 'entropy',\n",
        "  'max_depth': 5,\n",
        "  'max_features': 'sqrt',\n",
        "  'min_samples_leaf': 3,\n",
        "  'min_samples_split': 2,\n",
        "  'n_estimators': 30},\n",
        " 'XGBClassifier': {'max_depth': 13,\n",
        "  'min_child_weight': 3,\n",
        "  'subsample': 0.9406880083709813}}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e735912700ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mFr_classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'Fr_classifier' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc6sc6QfbVhx",
        "colab_type": "code",
        "outputId": "29f521ad-9c86-49e6-8b04-a46f03df8b63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier())\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier())\n",
        "pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier())\n",
        "pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe6], meta_classifier=LogisticRegression())\n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test2, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test2, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.78      0.77     94841\n",
            "           1       0.82      0.80      0.81    120517\n",
            "\n",
            "    accuracy                           0.79    215358\n",
            "   macro avg       0.79      0.79      0.79    215358\n",
            "weighted avg       0.79      0.79      0.79    215358\n",
            "\n",
            "StackingClassifier score: 0.913023\n",
            "Cross-val score: 0.86650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxWwikwQVEhi",
        "colab_type": "code",
        "outputId": "b3d5b3a5-0fec-48c2-c0b2-1451a54da5ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier(**a8['RandomForestClassifier']))  \n",
        "pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier(**a8['ExtraTreesClassifier'])) \n",
        "pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier(**a8['LGBMClassifier'])) \n",
        "pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe2,pipe3,pipe4,pipe6], meta_classifier=LogisticRegression()) \n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test2, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.48      0.46     94433\n",
            "           1       0.56      0.52      0.54    120925\n",
            "\n",
            "    accuracy                           0.50    215358\n",
            "   macro avg       0.50      0.50      0.50    215358\n",
            "weighted avg       0.51      0.50      0.50    215358\n",
            "\n",
            "StackingClassifier score: 0.809074\n",
            "Cross-val score: 0.50016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_NjQ3a2n3HR",
        "colab_type": "code",
        "outputId": "6020a5fe-fbf9-484b-8fac-728a0220d551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "for clf, label in zip([list(sclf2.named_classifiers.keys()),sclf2],[list(a8.keys())]):\n",
        "  for i in range(len(list(a8.keys()))):\n",
        "    if list(a8.keys())[i]  in label:\n",
        "      if (type((sclf2.named_classifiers[clf[0]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe1 = make_pipeline(Accepted[0][1](),\n",
        "                                XGBClassifier(**a8[(type((sclf2.named_classifiers[clf[0]][1])).__name__)]))\n",
        "      if (type((sclf2.named_classifiers[clf[1]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe2 = make_pipeline(Accepted[1][1](),\n",
        "                                RandomForestClassifier(**a8[(type((sclf2.named_classifiers[clf[1]][1])).__name__)]))  \n",
        "      if (type((sclf2.named_classifiers[clf[2]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe3 = make_pipeline(Accepted[2][1](),\n",
        "                                ExtraTreesClassifier(**a8[(type((sclf2.named_classifiers[clf[2]][1])).__name__)])) \n",
        "      if (type((sclf2.named_classifiers[clf[5]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe6 = make_pipeline(Accepted[5][1](),\n",
        "                                lgb.LGBMClassifier(**a8[(type((sclf2.named_classifiers[clf[5]][1])).__name__)])) \n",
        "    else:  \n",
        "            pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "            pipe5 = make_pipeline(Accepted[4][1](),QuadraticDiscriminantAnalysis())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe5,pipe6], meta_classifier=LogisticRegression()) \n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test2, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test2, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.84      0.79     94841\n",
            "           1       0.86      0.78      0.82    120517\n",
            "\n",
            "    accuracy                           0.81    215358\n",
            "   macro avg       0.81      0.81      0.81    215358\n",
            "weighted avg       0.81      0.81      0.81    215358\n",
            "\n",
            "StackingClassifier score: 0.813905\n",
            "Cross-val score: 0.87589\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmXql2VSupkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier(**a8['XGBClassifier']))\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier(**a8['RandomForestClassifier']))  \n",
        "pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier(**a8['ExtraTreesClassifier'])) \n",
        "pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier(**a8['LGBMClassifier'])) \n",
        "pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "pipe5 = make_pipeline(Accepted[4][1](),QuadraticDiscriminantAnalysis())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe5,pipe6], meta_classifier=LogisticRegression()) \n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVt5r7R6nsh3",
        "colab_type": "code",
        "outputId": "e6169280-bbd4-43fe-e1bc-db87afbce428",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "a8"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ExtraTreesClassifier': {'criterion': 'gini',\n",
              "  'max_depth': 7,\n",
              "  'max_features': 7,\n",
              "  'min_samples_leaf': 4,\n",
              "  'min_samples_split': 4,\n",
              "  'n_estimators': 19},\n",
              " 'LGBMClassifier': {'colsample_bytree': 1.0,\n",
              "  'max_depth': 7,\n",
              "  'min_child_samples': 80,\n",
              "  'num_leave': 50,\n",
              "  'reg_alpha': 0,\n",
              "  'reg_lambda': 10,\n",
              "  'scale_pos_weight': 70.0,\n",
              "  'subsample': 1.0},\n",
              " 'RandomForestClassifier': {'criterion': 'entropy',\n",
              "  'max_depth': 5,\n",
              "  'max_features': 'sqrt',\n",
              "  'min_samples_leaf': 3,\n",
              "  'min_samples_split': 2,\n",
              "  'n_estimators': 30},\n",
              " 'XGBClassifier': {'max_depth': 13,\n",
              "  'min_child_weight': 3,\n",
              "  'subsample': 0.9406880083709813}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tsh4A3eZtNDm",
        "colab_type": "code",
        "outputId": "ce2a2c8c-e6da-4b64-e765-8c1c6e23887b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "list(pipe1.named_steps.keys())[1],list(a8.keys())[0].lower()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('xgbclassifier', 'extratreesclassifier')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyjoLr3s4OzU",
        "colab_type": "code",
        "outputId": "febab780-ac65-4655-aec1-4f82539af121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "a8"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ExtraTreesClassifier': {'criterion': 'gini',\n",
              "  'max_depth': 7,\n",
              "  'max_features': 7,\n",
              "  'min_samples_leaf': 4,\n",
              "  'min_samples_split': 4,\n",
              "  'n_estimators': 19},\n",
              " 'LGBMClassifier': {'colsample_bytree': 1.0,\n",
              "  'max_depth': 7,\n",
              "  'min_child_samples': 80,\n",
              "  'num_leave': 50,\n",
              "  'reg_alpha': 0,\n",
              "  'reg_lambda': 10,\n",
              "  'scale_pos_weight': 70.0,\n",
              "  'subsample': 1.0},\n",
              " 'RandomForestClassifier': {'criterion': 'entropy',\n",
              "  'max_depth': 5,\n",
              "  'max_features': 'sqrt',\n",
              "  'min_samples_leaf': 3,\n",
              "  'min_samples_split': 2,\n",
              "  'n_estimators': 30},\n",
              " 'XGBClassifier': {'max_depth': 13,\n",
              "  'min_child_weight': 3,\n",
              "  'subsample': 0.9406880083709813}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yS_rBX750WcG",
        "colab_type": "code",
        "outputId": "684e1d1d-3603-4dcd-d668-e03588a3b3e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "for clf, label in zip([list(sclf2.named_classifiers.keys()),sclf2],[list(a8.keys())]):\n",
        "  for i in range(len(list(a8.keys()))):\n",
        "    if list(a8.keys())[i]  in label:\n",
        "      if (type((sclf2.named_classifiers[clf[0]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier(**a8[(type((sclf2.named_classifiers[clf[0]][1])).__name__)]))\n",
        "          pipe2\n",
        "      if (type((sclf2.named_classifiers[clf[1]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier(**a8[(type((sclf2.named_classifiers[clf[1]][1])).__name__)]))  \n",
        "          pipe2   \n",
        "      if (type((sclf2.named_classifiers[clf[2]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier(**a8[(type((sclf2.named_classifiers[clf[2]][1])).__name__)])) \n",
        "      if (type((sclf2.named_classifiers[clf[3]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier(**a8[(type((sclf2.named_classifiers[clf[5]][1])).__name__)])) \n",
        "    else:  \n",
        "            pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "            pipe5 = make_pipeline(Accepted[4][1](),QuadraticDiscriminantAnalysis())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe5,pipe6], meta_classifier=LogisticRegression())  \n",
        "model2=sclf2.fit(X_train, y_train)\n",
        "y_true3, y_pred3  =y_test, model2.predict(X_test)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train, y_train))\n",
        "clf2_val_score = roc_auc_score(y_test, sclf2.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('catboostencoder',\n",
              "                 CatBoostEncoder(a=1, cols=None, drop_invariant=False,\n",
              "                                 handle_missing='value', handle_unknown='value',\n",
              "                                 random_state=None, return_df=True, sigma=None,\n",
              "                                 verbose=0)),\n",
              "                ('xgbclassifier',\n",
              "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
              "                               colsample_bylevel=1, colsample_bynode=1,\n",
              "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
              "                               max_delta_step=0, max_depth=13,\n",
              "                               min_child_weight=3, missing=None,\n",
              "                               n_estimators=100, n_jobs=1, nthread=None,\n",
              "                               objective='binary:logistic', random_state=0,\n",
              "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
              "                               seed=None, silent=None,\n",
              "                               subsample=0.9406880083709813, verbosity=1))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVFICfs6qliC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for clf, label in zip([list(sclf2.named_classifiers.keys()),sclf2],[list(a8.keys())]):\n",
        "  if label[0].lower()==list(pipe1.named_steps.keys())[1]:\n",
        "    print(label)\n",
        "\n",
        "pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier())\n",
        "\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier())\n",
        "pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier())\n",
        "pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "pipe5 = make_pipeline(Accepted[4][1](),QuadraticDiscriminantAnalysis())\n",
        "pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiGbF94Mg6XY",
        "colab_type": "code",
        "outputId": "5aa1680c-f325-446f-a5be-4e03b7543f2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "sclf2.named_classifiers['pipeline-1'][1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.1, max_delta_step=0,\n",
              "              max_depth={'colsample_bytree': 1.0, 'max_depth': 7,\n",
              "                         'min_child_samples': 80, 'num_leave': 50,\n",
              "                         'reg_alpha': 0, 'reg_lambda': 10,\n",
              "                         'scale_pos_weight': 70.0, 'subsample': 1.0},\n",
              "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
              "              nthread=None, objective='binary:logistic', random_state=0,\n",
              "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "              silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaFe4_nXdj96",
        "colab_type": "code",
        "outputId": "a9616d42-3e4f-49b5-9a1d-07a9b904f5e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier(**a8['ExtraTreesClassifier'] )  )\n",
        "pipe3\n",
        "a8.keys(),list(sclf2.named_classifiers.keys())\n",
        "a8['XGBClassifier']\n",
        "pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier(**a8['XGBClassifier']))\n",
        "pipe1,a8['XGBClassifier']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Pipeline(memory=None,\n",
              "          steps=[('catboostencoder',\n",
              "                  CatBoostEncoder(a=1, cols=None, drop_invariant=False,\n",
              "                                  handle_missing='value', handle_unknown='value',\n",
              "                                  random_state=None, return_df=True, sigma=None,\n",
              "                                  verbose=0)),\n",
              "                 ('xgbclassifier',\n",
              "                  XGBClassifier(base_score=0.5, booster='gbtree',\n",
              "                                colsample_bylevel=1, colsample_bynode=1,\n",
              "                                colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
              "                                max_delta_step=0, max_depth=13,\n",
              "                                min_child_weight=3, missing=None,\n",
              "                                n_estimators=100, n_jobs=1, nthread=None,\n",
              "                                objective='binary:logistic', random_state=0,\n",
              "                                reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
              "                                seed=None, silent=None,\n",
              "                                subsample=0.9406880083709813, verbosity=1))],\n",
              "          verbose=False),\n",
              " {'max_depth': 13, 'min_child_weight': 3, 'subsample': 0.9406880083709813})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHU7-dKPdJKv",
        "colab_type": "code",
        "outputId": "51ca7629-9338-4a86-add8-74e741515f96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "list(pipe1.named_steps.keys())[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'xgbclassifier'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5TMP6e7ab6X",
        "colab_type": "code",
        "outputId": "016882b2-f986-4157-8671-fc3dcbf786bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for i,k in enumerate(list(sclf2.named_classifiers.keys())):\n",
        "  for j,val in enumerate(list(a8.keys())):\n",
        "    if type((sclf2.named_classifiers[k][1])).__name__ ==list(a8.keys())[j]:\n",
        "        print(list(a8.values())[j])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'criterion': 'entropy', 'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 30}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQa1FPcy3ucp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "{'ExtraTreesClassifier': {'criterion': 'gini', 'max_depth': 7, 'max_features': 7, 'min_samples_leaf': 4, 'min_samples_split': 4, 'n_estimators': 19}, \n",
        "'LGBMClassifier': {'colsample_bytree': 1.0, 'max_depth': 7, 'min_child_samples': 80, 'num_leave': 50, 'reg_alpha': 0, 'reg_lambda': 10, 'scale_pos_weight': 70.0, 'subsample': 1.0},\n",
        "'RandomForestClassifier': {'criterion': 'entropy', 'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 30},\n",
        "'XGBClassifier': {'max_depth': 13, 'min_child_weight': 3, 'subsample': 0.9406880083709813}}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwGKr5nNdGcp",
        "colab_type": "code",
        "outputId": "a9f7cd5b-9183-42f2-8736-1a16defcbdbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from hyperopt import hp, tpe, fmin, Trials, STATUS_OK\n",
        "from sklearn import datasets\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble.forest import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline \n",
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "import numpy as np\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "param_hyperopt={\n",
        "  'RandomForestClassifier': \n",
        "            {\n",
        "            'max_depth': ho_scope.int(hp.quniform('c2_max_depth', 5, 7, 1)),\n",
        "            'n_estimators': ho_scope.int(hp.quniform('c2_n_estimators', 5, 30, 1)),\n",
        "            'max_features': hp.choice('c2_max_features', ['auto', 'sqrt']),\n",
        "            'criterion': hp.choice('c2_criterion', ['gini', 'entropy']),\n",
        "            'min_samples_split': ho_scope.int(hp.quniform('c2_min_samples_split', 2, 5,1)),\n",
        "            'min_samples_leaf': ho_scope.int(hp.quniform('c2_min_samples_leaf', 2, 5,1))\n",
        "                          \n",
        "         },\n",
        "  'ExtraTreesClassifier':\n",
        "            {\n",
        "                'max_depth': ho_scope.int(hp.quniform('c4_max_depth', 5, 20, 1)),\n",
        "                'n_estimators': ho_scope.int(hp.quniform('c4_n_estimators', 5, 30, 1)),\n",
        "                'max_features': ho_scope.int(hp.quniform('c4_max_features', 5, 20, 1)),\n",
        "                'criterion': hp.choice('c4_criterion', ['gini', 'entropy']),\n",
        "                'min_samples_split': ho_scope.int(hp.quniform('c4_min_samples_split', 2, 5,1)),\n",
        "                'min_samples_leaf': ho_scope.int(hp.quniform('c4_min_samples_leaf', 2, 5,1))\n",
        "       },\n",
        "  'XGBClassifier':\n",
        "      {\n",
        "           'max_depth': hp.choice(\"x_max_depth\", np.arange(5, 25, dtype=int)),\n",
        "           'min_child_weight': hp.choice ('x_min_child',np.arange(1, 10, dtype=int)),\n",
        "           'subsample': hp.uniform ('x_subsample', 0.8, 1)\n",
        "       },\n",
        "  'LGBMClassifier':\n",
        "       {\n",
        "        'max_depth': ho_scope.int(hp.quniform('c6_max_depth', 3, 10, 1)),\n",
        "        'num_leave': ho_scope.int(hp.quniform('c6_num_leaves', 5, 50, 5)),\n",
        "        'min_child_samples': ho_scope.int(hp.quniform('c6_min_child_samples', 50, 100, 10)),\n",
        "        'scale_pos_weight': ho_scope.float(hp.quniform('c6_scale_pos_weight', 50, 100, 10)),\n",
        "        'subsample': ho_scope.float(hp.quniform('c6_subsample', 0.6, 0.9, 1)),\n",
        "        'colsample_bytree': ho_scope.float(hp.quniform('c6_colsample_bytree', 0.6, 0.9, 1)),\n",
        "        'reg_lambda': hp.choice(\"c6_reg_lambda\",[0, 1e-1, 1, 5, 10, 20, 50]),\n",
        "        'reg_alpha': hp.choice(\"c6_reg_alpha\",[0, 1e-1, 1, 5, 10, 20, 50])\n",
        "       }         \n",
        "       \n",
        "}  \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "def f_clf8(params):\n",
        "   model1 =Pipeline(steps=[('encoder',Accepted[0][1]()),\n",
        "             ('c4',ExtraTreesClassifier(**f_unpack_dict(params['ExtraTreesClassifier'])))])\n",
        "   model2 =Pipeline(steps=[('encoder',Accepted[1][1]()),('LGBMClassifier', lgb.LGBMClassifier(**f_unpack_dict(params['LGBMClassifier'])))]) \n",
        "   model3 =Pipeline(steps=[('encoder',Accepted[2][1]()),('XGBClassifier', xgb.XGBClassifier(**f_unpack_dict(params['XGBClassifier'])))])   \n",
        "   model4 =Pipeline(steps=[('encoder',Accepted[3][1]()),('LinearDiscriminantAnalysis', LinearDiscriminantAnalysis())]) \n",
        "   model5 =Pipeline(steps=[('encoder',Accepted[4][1]()),\n",
        "             ('c2',RandomForestClassifier(**f_unpack_dict(params['RandomForestClassifier'])))]) \n",
        "   model6 =Pipeline(steps=[('encoder',Accepted[5][1]()),('QuadraticDiscriminantAnalysis', QuadraticDiscriminantAnalysis())])\n",
        "   sclf2 = StackingClassifier(classifiers=[model1,model2,model3,model4,model5,model6],meta_classifier=LogisticRegression())\n",
        "   return sclf2\n",
        "  \n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf8(params)\n",
        "    #clf = RandomForestClassifier(**params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf8 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                param_hyperopt, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "\n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval\n",
        "from hyperopt.pyll import scope as ho_scope\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.utils import shuffle\n",
        "clf8 = f_clf8(space_eval(param_hyperopt, best_clf8)).fit(X_train, y_train)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf8_val_score = roc_auc_score(y_test, clf8.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf8_val_score))\n",
        "print('Best parameters:')\n",
        "print(space_eval(param_hyperopt, best_clf8))\n",
        "oo=-trials.best_trial['result']['loss']\n",
        "oo"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 10%|█         | 1/10 [29:27<4:25:04, 1767.20s/it, best loss: -0.8759822037775029]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd05nQ3EojhD",
        "colab_type": "code",
        "outputId": "7a2da0a9-b32c-4347-b9e2-3466c7cbaff0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "from hyperopt import hp, tpe, fmin, Trials, STATUS_OK\n",
        "from sklearn import datasets\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble.forest import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline \n",
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "import numpy as np\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "\n",
        "param_hyperopt= {\n",
        "    'AdaBoostClassifier':\n",
        "     {\n",
        "          'learning_rate': hp.loguniform('c5_learning_rate', np.log(0.01), np.log(1)),\n",
        "           'n_estimators': ho_scope.int(hp.quniform('c5_n_estimators', 5, 30, 1)),\n",
        "           'algorithm': hp.choice('c5_algorithm',[\"SAMME\"])  \n",
        "       }    \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "def f_clf7(params):\n",
        "   model2 =Pipeline(steps=[('encoder',Accepted[5][1]()),\n",
        "             ('c5',AdaBoostClassifier(**f_unpack_dict(params['AdaBoostClassifier'])))]) \n",
        "   return model2\n",
        "        \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf7(params)\n",
        "    #clf = RandomForestClassifier(**params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf7 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                 param_hyperopt, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "\n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval\n",
        "from hyperopt.pyll import scope as ho_scope\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.utils import shuffle\n",
        "clf7 = f_clf7(space_eval(param_hyperopt, best_clf7)).fit(X_train2, y_train2)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf7_val_score = roc_auc_score(y_test, clf7.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf7_val_score))\n",
        "print('Best parameters:')\n",
        "print(space_eval(param_hyperopt, best_clf7))\n",
        "cc=-trials.best_trial['result']['loss']\n",
        "cc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [07:45<00:00, 43.23s/it, best loss: -0.7034403411046589]\n",
            "Cross-val score: 0.70344; validation score: 0.70367\n",
            "Best parameters:\n",
            "{'AdaBoostClassifier': {'algorithm': 'SAMME', 'learning_rate': 0.30044074314959096, 'n_estimators': 13}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7034403411046589"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    }
  ]
}