{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarah127/udacity-data-analysis-with-r/blob/master/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgzROkNIH4do",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "6840ec33-9c7e-4dd2-a151-449a97e4e6ec"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGDCijX7ybgt",
        "colab_type": "code",
        "outputId": "89feeb1f-2c76-41bc-9d8d-c167fc8a78ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "#' ' means CPU whereas '/device:G:0' means GPU\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-SxmThUynsh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!df -h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAV8exRXypkt",
        "colab_type": "code",
        "outputId": "ffb58dca-6b81-424e-f613-e9b47cffa5cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanizeii\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm() "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7410 sha256=eef4a6792edb33673d403e3e206fa5168e89248c0f43dade334ba048cdce6f2c\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement humanizeii (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for humanizeii\u001b[0m\n",
            "Gen RAM Free: 26.4 GB  | Proc size: 158.2 MB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuFNEWi_19RH",
        "colab_type": "code",
        "outputId": "2ca37fbd-5b11-4c35-a62f-418339b7e336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh5NCoc8fsSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1R6gDJMND6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP8zG65mYk86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-IRoKKSfh4q",
        "colab_type": "code",
        "outputId": "e2febe11-ce2a-4429-d372-e152cf78447e",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 57
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-223422d2-77ab-4ed0-aa68-2b83b48bf8e2\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-223422d2-77ab-4ed0-aa68-2b83b48bf8e2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wEHXFWvfzTH",
        "colab_type": "code",
        "outputId": "5f000ca2-054a-47ff-b0cf-02497e93679a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!ls /content/drive/*"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'191021-234625 (1).png'\t\t   'How to get started with Drive.pdf'\n",
            " 191021-234625.png\t\t   'IELTS Grammar Chris'\n",
            "'٢٠١٨٠٢٠٥_٢١٤٣٥٥ (1).jpg'\t    IMG_20180427_151315.jpg\n",
            "'٢٠١٨٠٢٠٥_٢١٤٣٥٥ (2).jpg'\t   'IMG_20190909_205719[1] (1).jpg'\n",
            " ٢٠١٨٠٢٠٥_٢١٤٣٥٥.jpg\t\t   'IMG_20190909_205719[1].jpg'\n",
            "'٢٠١٨٠٢٠٥_٢١٤٣٥٥ - sarah ali.jpg'   prposal.docx\n",
            " ٢٠١٨٠٤٠٨_١٣٤٦٠٦.jpg\t\t   'Ryan Higgins - IELTS Course 2014'\n",
            " AirlinesCodrnaAdult.csv\t   'Study English IELTS Preparation Series'\n",
            "'Colab Notebooks'\t\t   'TED talks+scripts'\n",
            " dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD03caqDhFhA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "from  pyspark import SparkConf\n",
        "#conf = SparkConf().setMaster(\"local\").setAppName(\"My app\").set(\"spark.executor.memory\", \"1g\")\n",
        "#sc = SparkContext()\n",
        "# ===============================read from csv==============================================\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "from pyspark.sql.types import StructType,StructField,StringType,TimestampType,IntegerType,DoubleType,LongType,FloatType\n",
        "Df5_Schema = StructType([StructField(\"Airline\", StringType(),True),StructField(\"Flight\", DoubleType()),\n",
        "StructField(\"AirportFrom\",StringType()),StructField(\"AirportTo\", StringType()),StructField(\"DayOfWeek\", IntegerType()),\n",
        "StructField(\"Time\",IntegerType()), StructField(\"Length\", IntegerType()),StructField(\"codrna_X1\",DoubleType()),\n",
        "StructField(\"codrna_X2\",DoubleType()),StructField(\"codrna_X3\",DoubleType()),StructField(\"codrna_X4\", DoubleType()),\n",
        "StructField(\"codrna_X5\", DoubleType()),StructField(\"codrna_X6\", DoubleType()),StructField(\"codrna_X7\", DoubleType()),\n",
        "StructField(\"codrna_X8\", DoubleType()),StructField(\"age\", IntegerType()),StructField(\"workclass\", StringType()),\n",
        "StructField(\"fnlwgt\", IntegerType()),StructField(\"education\", StringType()),StructField(\"education-num\", IntegerType()),\n",
        "StructField(\"marital-status\", StringType()),StructField(\"occupation\", StringType()),StructField(\"relationship\", StringType()),\n",
        "StructField(\"race\", StringType()),StructField(\"sex\", StringType()),StructField(\"capitalgain\", IntegerType()),\n",
        "StructField(\"capitalloss\", IntegerType()),StructField(\"hoursperweek\", IntegerType()),StructField(\"native-country\", StringType()),\n",
        "StructField(\"Delay\", IntegerType())])\n",
        "CSV_2007= '/content/drive/My Drive/AirlinesCodrnaAdult.csv'\n",
        "df5 =spark.read.schema(Df5_Schema).option(\"header\",\"true\").option(\"mode\", \"DROPMALFORMED\").csv(CSV_2007)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQvods40rPBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "names = [f.name for f in df5.schema.fields]\n",
        "dataTypes = [f.dataType for f in df5.schema.fields]\n",
        "ls=list(zip(names,dataTypes))\n",
        "str_cols=[]\n",
        "for i in range(len(ls)):\n",
        "     if type(ls[i][1])==StringType and len(df5.select(df5[ls[i][0]]).distinct().collect())<=20:\n",
        "         str_cols.append([i,ls[i][0]]) \n",
        "strings=[]\n",
        "for i in range(len(str_cols)):\n",
        "     strings.append(str_cols[i][1])  \n",
        "\n",
        "#the process of convertiong catogerical columns into numeric is not included in the machine learning operation \n",
        "#it will be left for future improvements !!!ان شاء الله      \n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df5) for column in strings]\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "df6 = pipeline.fit(df5).transform(df5)\n",
        "df6=df6.drop(*strings)\n",
        "df6_pd=df6.toPandas()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIM7xkiDsRJI",
        "colab_type": "code",
        "outputId": "df323004-2095-4276-bf2a-1940872153dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "! pip install category_encoders"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting category_encoders\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/52/c54191ad3782de633ea3d6ee3bb2837bda0cf3bc97644bb6375cf14150a0/category_encoders-2.1.0-py2.py3-none-any.whl (100kB)\n",
            "\r\u001b[K     |███▎                            | 10kB 17.7MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 30kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 92kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.21.3)\n",
            "Requirement already satisfied: statsmodels>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.10.2)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.17.4)\n",
            "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.25.3)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.5.1)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.3.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.4.1->category_encoders) (1.12.0)\n",
            "Installing collected packages: category-encoders\n",
            "Successfully installed category-encoders-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB96hBgnrflo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import f1_score\n",
        "import category_encoders as ce\n",
        "from sklearn.utils import resample\n",
        "\n",
        "encoder_list = [ce.backward_difference.BackwardDifferenceEncoder,ce.basen.BaseNEncoder,ce.binary.BinaryEncoder,\n",
        "                ce.cat_boost.CatBoostEncoder,ce.helmert.HelmertEncoder,\n",
        "                ce.james_stein.JamesSteinEncoder,ce.one_hot.OneHotEncoder,ce.leave_one_out.LeaveOneOutEncoder,\n",
        "                ce.m_estimate.MEstimateEncoder,ce.ordinal.OrdinalEncoder,\n",
        "                ce.sum_coding.SumEncoder,ce.target_encoder.TargetEncoder,ce.woe.WOEEncoder]\n",
        "#numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "#categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X = df6_pd.drop('Delay', axis=1)\n",
        "y = df6_pd['Delay']\n",
        "#le = preprocessing.LabelEncoder()\n",
        "#label_encoder = le.fit(y)\n",
        "#y = label_encoder.transform(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "block_size=int(round(X_train.shape[0]/len(encoder_list)))\n",
        "n = block_size  #chunk row size\n",
        "list_X_train = [X_train[i:i+n] for i in range(0,X_train.shape[0],n)]\n",
        "list_y_train = [y_train[i:i+n] for i in range(0,y_train.shape[0],n)]\n",
        "list_X_test = [X_test[i:i+n] for i in range(0,X_test.shape[0],n)]\n",
        "list_y_test = [y_test[i:i+n] for i in range(0,y_test.shape[0],n)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "486lmPRrsNS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_numeric_dataset=[]\n",
        "intermidate=[]\n",
        "testing_numeric_dataset=[]\n",
        "for i,encoder in enumerate(encoder_list):\n",
        "  #(encoder(handle_unknown='ignore',cols=['AirportFrom','AirportTo','native-country'])).fit_transform(boot1[i])\n",
        "   training_numeric_dataset.append(encoder_list[i](cols=['AirportFrom','AirportTo','native-country']).fit(list_X_train[i],list_y_train[i]))\n",
        "   intermidate.append((training_numeric_dataset[i]).transform(list_X_train[i],list_y_train[i]))\n",
        "for i in range(len(list_X_test)):\n",
        "    for j,encoder in enumerate(encoder_list):\n",
        "        testing_numeric_dataset.append((training_numeric_dataset[j]).transform(list_X_test[i])) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KMjFk6msm2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xx=[]\n",
        "for i in range(len(encoder_list)):\n",
        "    #xx.append([i,min(intermidate[i].columns)] )\n",
        "    xx.append([encoder_list[i],len(intermidate[i].columns)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVO0TqN94DCM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = []\n",
        "while(1):\n",
        "    a.append('1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XrUFnRFs2ZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import category_encoders as ce\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "block_size=int(round(X_train2.shape[0]/12))\n",
        "n = block_size  #chunk row size\n",
        "list_X_train2 = [X_train2[i:i+n] for i in range(0,X_train2.shape[0],n)]\n",
        "list_y_train2 = [y_train2[i:i+n] for i in range(0,y_train2.shape[0],n)]\n",
        "list_X_test2 = [X_test2[i:i+n] for i in range(0,X_test2.shape[0],n)]\n",
        "list_y_test2 = [y_test2[i:i+n] for i in range(0,y_test2.shape[0],n)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAV2AZs2tE00",
        "colab_type": "code",
        "outputId": "b401b473-a6d5-45e0-a409-7fa3405ce269",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "import category_encoders as ce\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from heapq import nlargest\n",
        "\n",
        "encoder_list = [ce.backward_difference.BackwardDifferenceEncoder(),ce.basen.BaseNEncoder(),\n",
        "                ce.binary.BinaryEncoder(),ce.cat_boost.CatBoostEncoder(),\n",
        "                ce.helmert.HelmertEncoder(),ce.james_stein.JamesSteinEncoder(),\n",
        "                ce.one_hot.OneHotEncoder(),ce.leave_one_out.LeaveOneOutEncoder()\n",
        "                ,ce.m_estimate.MEstimateEncoder(),ce.ordinal.OrdinalEncoder(),ce.sum_coding.SumEncoder(),\n",
        "                ce.target_encoder.TargetEncoder(),ce.woe.WOEEncoder()]\n",
        "my_dict={}\n",
        "for encoder in encoder_list:\n",
        "    model = Pipeline(steps=[('preprocessor', encoder),('classifier',RandomForestClassifier())])               \n",
        "    modelOpt = model.fit(X_train, y_train)\n",
        "    y_true,y_pred =y_test, model.predict(X_test)\n",
        "    clf2_val_score = roc_auc_score(y_test, modelOpt.predict_proba(X_test)[:, 1])\n",
        "    my_dict[type(encoder)]= [modelOpt.score(X_train, y_train),clf2_val_score]\n",
        "\n",
        "num=int(round(len(encoder_list)*0.5))    "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNP-3BVuIy8l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z={}\n",
        "for i in range(len(my_dict)):\n",
        "   z[list(my_dict.keys())[i]]=list(my_dict.items())[i][1][0] \n",
        "Highest1 = nlargest(num, z, key = z.get)  \n",
        "\n",
        "zz={}\n",
        "for i in range(len(my_dict)):\n",
        "   zz[list(my_dict.keys())[i]]=list(my_dict.items())[i][1][1] \n",
        "Highest2 = nlargest(num, zz, key = zz.get)  \n",
        "Highest3=list(set(Highest1+Highest2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuhCUjfj_lc1",
        "colab_type": "code",
        "outputId": "2b77bb5f-d1d5-4224-d1a0-60476ed1eb6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "zz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{category_encoders.backward_difference.BackwardDifferenceEncoder: 0.8606833246206118,\n",
              " category_encoders.basen.BaseNEncoder: 0.8663549344139968,\n",
              " category_encoders.binary.BinaryEncoder: 0.8663393023681079,\n",
              " category_encoders.cat_boost.CatBoostEncoder: 0.8827985009757067,\n",
              " category_encoders.helmert.HelmertEncoder: 0.8592280236084026,\n",
              " category_encoders.james_stein.JamesSteinEncoder: 0.8683739506772159,\n",
              " category_encoders.leave_one_out.LeaveOneOutEncoder: 0.515101473852555,\n",
              " category_encoders.m_estimate.MEstimateEncoder: 0.8681464920689393,\n",
              " category_encoders.one_hot.OneHotEncoder: 0.8617009332062612,\n",
              " category_encoders.ordinal.OrdinalEncoder: 0.867874490461257,\n",
              " category_encoders.sum_coding.SumEncoder: 0.8616371273696918,\n",
              " category_encoders.target_encoder.TargetEncoder: 0.8682504198254399,\n",
              " category_encoders.woe.WOEEncoder: 0.8686003347631575}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN3Vj_8VJDhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Accepted=[]\n",
        "keys = my_dict.keys()\n",
        "for i,each in enumerate(xx):\n",
        " for j,val in enumerate(Highest3): \n",
        "   if xx[i][0]==val and  xx[i][1]==len(X_train.columns)  : \n",
        "           #  Accepted.append(\"%s :%s: %s\" % (i,type(each), my_dict.get(each))) \n",
        "           Accepted.append((i,val))\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20rpUs7-EfrN",
        "colab_type": "code",
        "outputId": "00d78ce4-5fb4-47fa-b8ae-37e17c253015",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "Accepted      "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(3, category_encoders.cat_boost.CatBoostEncoder),\n",
              " (5, category_encoders.james_stein.JamesSteinEncoder),\n",
              " (8, category_encoders.m_estimate.MEstimateEncoder),\n",
              " (9, category_encoders.ordinal.OrdinalEncoder),\n",
              " (11, category_encoders.target_encoder.TargetEncoder),\n",
              " (12, category_encoders.woe.WOEEncoder)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AWt8EE1Jl76",
        "colab_type": "code",
        "outputId": "fc230191-4d00-4507-d988-385ebfee5ca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "pipe1 = make_pipeline(Accepted[0][1](),RandomForestClassifier())\n",
        "pipe2 = make_pipeline(Accepted[1][1](), KNeighborsClassifier())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2], meta_classifier=LogisticRegression())\n",
        "\n",
        "model2=sclf2.fit(X_train, y_train)\n",
        "y_true3, y_pred3  =y_test, model2.predict(X_test)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train, y_train))\n",
        "clf2_val_score = roc_auc_score(y_test, sclf2.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P6auG6ahBf-",
        "colab_type": "code",
        "outputId": "ac10a693-047d-4a76-ee7d-90d0a37bc0e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier())\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier())\n",
        "pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier())\n",
        "pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "pipe5 = make_pipeline(Accepted[4][1](),AdaBoostClassifier())\n",
        "pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe5,pipe6], meta_classifier=LogisticRegression())\n",
        "\n",
        "model2=sclf2.fit(X_train, y_train)\n",
        "y_true3, y_pred3  =y_test, model2.predict(X_test)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train, y_train))\n",
        "clf2_val_score = roc_auc_score(y_test, sclf2.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.78      0.77     94227\n",
            "           1       0.82      0.80      0.81    121131\n",
            "\n",
            "    accuracy                           0.79    215358\n",
            "   macro avg       0.79      0.79      0.79    215358\n",
            "weighted avg       0.79      0.79      0.79    215358\n",
            "\n",
            "StackingClassifier score: 0.912733\n",
            "Cross-val score: 0.86497\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZZsULnlhHEM",
        "colab_type": "code",
        "outputId": "727c340d-7afb-48ac-a20b-85dd5bb70801",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier())\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier())\n",
        "pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier())\n",
        "pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "#pipe5 = make_pipeline(Accepted[4][1](),AdaBoostClassifier())\n",
        "pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe6], meta_classifier=LogisticRegression())\n",
        "\n",
        "model2=sclf2.fit(X_train, y_train)\n",
        "y_true3, y_pred3  =y_test, model2.predict(X_test)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train, y_train))\n",
        "clf2_val_score = roc_auc_score(y_test, sclf2.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.78      0.77     94227\n",
            "           1       0.82      0.80      0.81    121131\n",
            "\n",
            "    accuracy                           0.79    215358\n",
            "   macro avg       0.79      0.79      0.79    215358\n",
            "weighted avg       0.79      0.79      0.79    215358\n",
            "\n",
            "StackingClassifier score: 0.912755\n",
            "Cross-val score: 0.86483\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuYcc4nnZkp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from hyperopt import fmin, hp, tpe, Trials, space_eval\n",
        "from hyperopt.pyll import scope as ho_scope\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.utils import shuffle\n",
        "clf1 = f_clf1(space_eval(param_hyperopt, best_clf3)).fit(X_train, y_train)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf1_val_score = roc_auc_score(y_test, clf1.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf1_val_score))\n",
        "print('Best parameters:')\n",
        "print(space_eval(param_hyperopt, best_clf3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyBAgEvxakfh",
        "colab_type": "code",
        "outputId": "b82d30a2-80bf-4dd6-c7bc-36d44c6485ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "y_true3, y_pred3  =y_test, model2.predict(X_test)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"RandomForestClassifier score: %f\" % clf1.score(X_train, y_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.79      0.77     94676\n",
            "           1       0.83      0.78      0.80    120682\n",
            "\n",
            "    accuracy                           0.79    215358\n",
            "   macro avg       0.78      0.79      0.78    215358\n",
            "weighted avg       0.79      0.79      0.79    215358\n",
            "\n",
            "RandomForestClassifier score: 0.731440\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "13701896-f9ea-4707-c543-e1927773389d",
        "id": "GoT4rgyqbNPr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "import warnings\n",
        "pipe1 = make_pipeline(Accepted[0][1](),RandomForestClassifier(n_estimators= 21,\n",
        "                max_depth= 7,min_samples_split= 4,min_samples_leaf= 4,max_features= 'sqrt'))\n",
        "pipe2 = make_pipeline(Accepted[1][1](),AdaBoostClassifier(DecisionTreeClassifier(max_depth=1)\n",
        ",n_estimators=28,learning_rate=0.8051561754791255)) \n",
        "pipe3 = make_pipeline(Accepted[2][1](),XGBClassifier())\n",
        "pipe4 = make_pipeline(Accepted[3][1](),ExtraTreesClassifier(criterion= 'entropy'))\n",
        "pipe5 = make_pipeline(Accepted[4][1](),GradientBoostingClassifier())\n",
        "pipe6 = make_pipeline(Accepted[5][1](),GaussianNB())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe5,pipe6],\n",
        "                           meta_classifier=LogisticRegression())\n",
        "\n",
        "model2=sclf2.fit(X_train, y_train)\n",
        "y_true3, y_pred3  =y_test, model2.predict(X_test)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"RandomForestClassifier score: %f\" % model2.score(X_train, y_train))\n",
        "clf2_val_score = roc_auc_score(y_test, sclf2.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.79      0.77     94676\n",
            "           1       0.83      0.78      0.80    120682\n",
            "\n",
            "    accuracy                           0.79    215358\n",
            "   macro avg       0.78      0.79      0.78    215358\n",
            "weighted avg       0.79      0.79      0.79    215358\n",
            "\n",
            "RandomForestClassifier score: 0.913660\n",
            "Cross-val score: 0.84756\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APRcNVp5qCWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive \n",
        "from google.colab import auth \n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7jAi0SyqJ6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGO4odcgVfWt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a81bfdb4-5b42-4614-8ae4-97b72ede0c67"
      },
      "source": [
        "# Compare Algorithms\n",
        "import pandas\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis,QuadraticDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.pipeline import Pipeline \n",
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "# explicitly require this experimental feature\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
        "# now you can import normally from ensemble\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import xgboost\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import lightgbm as lgb\n",
        "# load dataset\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "#le = preprocessing.LabelEncoder()\n",
        "#label_encoder = le.fit(y)\n",
        "#y = label_encoder.transform(y)\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2)\n",
        "block_size=int(round(X_train2.shape[0]/13))\n",
        "n = block_size  #chunk row size\n",
        "list_X_train = [X_train2[i:i+n] for i in range(0,X_train2.shape[0],n)]\n",
        "list_y_train = [y_train2[i:i+n] for i in range(0,y_train2.shape[0],n)]\n",
        "list_X_test = [X_test2[i:i+n] for i in range(0,X_test2.shape[0],n)]\n",
        "list_y_test = [y_test2[i:i+n] for i in range(0,y_test2.shape[0],n)]\n",
        "test_data = lgb.Dataset(list_X_test[0], label=list_y_test[0])\n",
        "# Build the encoder\n",
        "# prepare configuration for cross validation test harness\n",
        "seed = 7\n",
        "# prepare models\n",
        "models = []\n",
        "parameters = {\n",
        "    'application': 'binary','objective': 'binary','metric': 'auc','is_unbalance': 'true',\n",
        "    'boosting': 'gbdt','num_leaves': 31,'feature_fraction': 0.5,'bagging_fraction': 0.5,\n",
        "    'bagging_freq': 20,'learning_rate': 0.05,'verbose': 0\n",
        "}\n",
        "fit_params={\"early_stopping_rounds\":10,\"eval_metric\" : 'auc',\"eval_set\" : [(X_test,y_test)],\n",
        "            'eval_names': ['valid'],'verbose': 100,\n",
        "            'feature_name': 'auto', # that's actually the default\n",
        "            'categorical_feature': 'auto' # that's actually the default\n",
        "           }\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                               ('RandomForestClassifier',RandomForestClassifier(n_estimators=20))])) \n",
        "                               .fit(list_X_train[0], list_y_train[0])) \n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                               ('AdaBoostClassifier',AdaBoostClassifier(n_estimators=25))]))\n",
        "                               .fit(list_X_train[1], list_y_train[1])) \n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                       ('HistGradientBoostingClassifier', HistGradientBoostingClassifier())])).fit(list_X_train[2], list_y_train[2]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                              ('XGBClassifier',XGBClassifier(max_depth=2,n_estimators=10,objective='binary:logistic'))]))\n",
        "                            .fit(list_X_train[3], list_y_train[3]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                              ('ExtraTreesClassifier',ExtraTreesClassifier(criterion= 'entropy'))]))\n",
        "                               .fit(list_X_train[4], list_y_train[4]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "              ('lightgbm',lgb.LGBMClassifier(n_estimators=5 , num_leaves= 15,\n",
        "           max_depth=-1,colsample_bytree=0.9,subsample=0.9,learning_rate=0.1))])).fit(list_X_train[5], list_y_train[5]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                     ('LDA', LinearDiscriminantAnalysis())])).fit(list_X_train[6], list_y_train[6]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                     ('MLPClassifier',\n",
        "                      MLPClassifier(hidden_layer_sizes=(10,10), max_iter=5, alpha=0.0001,\n",
        "                     solver='sgd', verbose=10,  random_state=1))])).fit(list_X_train[7], list_y_train[7]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                     ('LR', LogisticRegression())])).fit(list_X_train[8], list_y_train[8]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                     ('QDA', QuadraticDiscriminantAnalysis())])).fit(list_X_train[9], list_y_train[9]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                      ('CART', DecisionTreeClassifier())])).fit(list_X_train[10], list_y_train[10]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                       ('NB', GaussianNB())])).fit(list_X_train[11], list_y_train[11]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "   ('BaggingClassifier',BaggingClassifier( DecisionTreeClassifier(random_state=42), n_estimators=100,\n",
        "    max_samples=100, bootstrap=True, n_jobs=-1, random_state=42,oob_score = True))])).fit(list_X_train[12], list_y_train[12]))\n",
        "\n",
        "\n",
        "   \n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "for name, model in enumerate(models):\n",
        "    #kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
        "    print('1')\n",
        "    cv_results = model_selection.cross_val_score(model, list_X_train[name], \n",
        "                            list_y_train[name], cv=5, scoring=scoring)\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "    print(msg)\n",
        "# boxplot algorithm comparison\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Algorithm Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 154.68473779\n",
            "Iteration 3, loss = 154.68162101\n",
            "Iteration 4, loss = 154.68092260\n",
            "Iteration 5, loss = 154.68039048\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "0: 0.778809 (0.003580)\n",
            "1\n",
            "1: 0.746077 (0.008504)\n",
            "1\n",
            "2: 0.797054 (0.002299)\n",
            "1\n",
            "3: 0.706809 (0.001283)\n",
            "1\n",
            "4: 0.768336 (0.005092)\n",
            "1\n",
            "5: 0.735316 (0.003705)\n",
            "1\n",
            "6: 0.747857 (0.005341)\n",
            "1\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 174.60193104\n",
            "Iteration 3, loss = 174.59684839\n",
            "Iteration 4, loss = 174.59574496\n",
            "Iteration 5, loss = 174.59516886\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 150.10485081\n",
            "Iteration 3, loss = 150.09977374\n",
            "Iteration 4, loss = 150.09873735\n",
            "Iteration 5, loss = 150.09824071\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 126.60992167\n",
            "Iteration 3, loss = 126.60485670\n",
            "Iteration 4, loss = 126.60387514\n",
            "Iteration 5, loss = 126.60344846\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 87.13738182\n",
            "Iteration 3, loss = 87.13236894\n",
            "Iteration 4, loss = 87.13149922\n",
            "Iteration 5, loss = 87.13119949\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 112.90700101\n",
            "Iteration 3, loss = 112.90185012\n",
            "Iteration 4, loss = 112.90094284\n",
            "Iteration 5, loss = 112.90057349\n",
            "7: 0.562281 (0.000013)\n",
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "8: 0.634809 (0.003531)\n",
            "1\n",
            "9: 0.729703 (0.004259)\n",
            "1\n",
            "10: 0.747419 (0.002300)\n",
            "1\n",
            "11: 0.613440 (0.003380)\n",
            "1\n",
            "12: 0.746303 (0.003873)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEVCAYAAADuAi4fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niXPQm5kl7v8DqvXI+dPpteoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGSR_9eGDZB4",
        "colab_type": "code",
        "outputId": "6900d555-3c13-4165-a82a-25b7ba7a6f41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "last=[]\n",
        "for name, model in enumerate(models):\n",
        "  last.append([type(model[1]),results[name].mean()])\n",
        "last\n",
        "def Sort(sub_li): \n",
        "   return(sorted(sub_li, key = lambda x: x[1], reverse=True))     \n",
        "bb=(Sort(last))\n",
        "try1=[]\n",
        "for i in range(len(bb)):\n",
        "  if bb[i][1]>=0.70:\n",
        "     try1.append(bb[i])\n",
        "try1"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier,\n",
              "  0.7970541516552283],\n",
              " [sklearn.ensemble.forest.RandomForestClassifier, 0.7788089166901575],\n",
              " [sklearn.ensemble.forest.ExtraTreesClassifier, 0.768335647199329],\n",
              " [sklearn.discriminant_analysis.LinearDiscriminantAnalysis,\n",
              "  0.7478571709337324],\n",
              " [sklearn.tree.tree.DecisionTreeClassifier, 0.7474194812327859],\n",
              " [sklearn.ensemble.bagging.BaggingClassifier, 0.7463027034137849],\n",
              " [sklearn.ensemble.weight_boosting.AdaBoostClassifier, 0.7460766561038177],\n",
              " [lightgbm.sklearn.LGBMClassifier, 0.7353164791144129],\n",
              " [sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis,\n",
              "  0.7297026091166999],\n",
              " [xgboost.sklearn.XGBClassifier, 0.7068091234578924]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvwjMnT6oJ42",
        "colab_type": "code",
        "outputId": "cd3b7bae-ca9f-4f1d-ffeb-43aac80bd5d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "for name, model in enumerate(models):\n",
        "  print(type(model[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
            "<class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'>\n",
            "<class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>\n",
            "<class 'xgboost.sklearn.XGBClassifier'>\n",
            "<class 'sklearn.ensemble.forest.ExtraTreesClassifier'>\n",
            "<class 'lightgbm.sklearn.LGBMClassifier'>\n",
            "<class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>\n",
            "<class 'sklearn.neural_network.multilayer_perceptron.MLPClassifier'>\n",
            "<class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
            "<class 'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis'>\n",
            "<class 'sklearn.tree.tree.DecisionTreeClassifier'>\n",
            "<class 'sklearn.naive_bayes.GaussianNB'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t72Demkh8sp",
        "colab_type": "code",
        "outputId": "085924e6-e2d5-42d3-98f1-2acc65487200",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "bb"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier,\n",
              "  0.7954545889830228],\n",
              " [sklearn.ensemble.forest.RandomForestClassifier, 0.7787336670539304],\n",
              " [sklearn.ensemble.forest.ExtraTreesClassifier, 0.7728480216418386],\n",
              " [sklearn.ensemble.weight_boosting.AdaBoostClassifier, 0.7506790659098037],\n",
              " [sklearn.ensemble.bagging.BaggingClassifier, 0.7494263980324497],\n",
              " [sklearn.tree.tree.DecisionTreeClassifier, 0.7472082396142955],\n",
              " [sklearn.discriminant_analysis.LinearDiscriminantAnalysis,\n",
              "  0.7438428732919431],\n",
              " [lightgbm.sklearn.LGBMClassifier, 0.7356634490994174],\n",
              " [sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis,\n",
              "  0.726850368903699],\n",
              " [xgboost.sklearn.XGBClassifier, 0.7061601871165106],\n",
              " [sklearn.linear_model.logistic.LogisticRegression, 0.6394572586734208],\n",
              " [sklearn.naive_bayes.GaussianNB, 0.6099540603979843],\n",
              " [sklearn.neural_network.multilayer_perceptron.MLPClassifier,\n",
              "  0.5616171682045052]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz0WsbNDCAMr",
        "colab_type": "code",
        "outputId": "d289b2df-69ea-4d06-98c7-770b9f6a0158",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "from hyperopt import hp, tpe, fmin, Trials, STATUS_OK\n",
        "from sklearn import datasets\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble.forest import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline \n",
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "# explicitly require this experimental feature\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
        "# now you can import normally from ensemble\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "import numpy as np\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "import xgboost as xgb\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.utils import shuffle\n",
        "param_hyperopt= {\n",
        "        'DecisionTreeClassifier':\n",
        "            {\n",
        "            'max_depth': ho_scope.int(hp.quniform('c0_max_depth', 3, 7, 1)),\n",
        "            'max_features': hp.choice('c0_max_features', ['auto', 'sqrt', 'log2']),\n",
        "            'min_samples_split': ho_scope.int(hp.quniform('c0_min_samples_split', 3, 15,3)),\n",
        "            'min_samples_leaf': ho_scope.int(hp.quniform('c0_min_samples_leaf', 3, 15,3))                                   \n",
        "          \n",
        "         },\n",
        "         'RandomForestClassifier': \n",
        "            {\n",
        "            'max_depth': ho_scope.int(hp.quniform('c2_max_depth', 5, 7, 1)),\n",
        "            'n_estimators': ho_scope.int(hp.quniform('c2_n_estimators', 5, 30, 1)),\n",
        "            'max_features': hp.choice('c2_max_features', ['auto', 'sqrt']),\n",
        "            'criterion': hp.choice('c2_criterion', ['gini', 'entropy']),\n",
        "            'min_samples_split': ho_scope.int(hp.quniform('c2_min_samples_split', 2, 5,1)),\n",
        "            'min_samples_leaf': ho_scope.int(hp.quniform('c2_min_samples_leaf', 2, 5,1))\n",
        "                          \n",
        "         },                                               \n",
        "          'HistGradientBoostingClassifier':\n",
        "            {\n",
        "               'learning_rate': hp.loguniform('c3_learning_rate', np.log(0.01), np.log(0.1)),\n",
        "                'max_iter': ho_scope.int(hp.quniform('c3_max_iter', 10, 80, 5)),\n",
        "                'max_depth': ho_scope.int(hp.quniform('c3_max_depth', 2,7, 1)),\n",
        "                'max_leaf_nodes': ho_scope.int(hp.quniform('c3_max_leaf_nodes', 5, 35,5)),\n",
        "                'min_samples_leaf': ho_scope.int(hp.quniform('c3_min_samples_leaf', 5, 25,5)),\n",
        "                'max_bins': ho_scope.int(hp.quniform('c3_max_bins', 20, 100,20)),\n",
        "                'validation_fraction':0.1,\n",
        "                'n_iter_no_change':None,\n",
        "                 'tol':1e-07,\n",
        "                'l2_regularization':0.0 \n",
        "           },\n",
        "      'ExtraTreesClassifier':\n",
        "            {\n",
        "                'max_depth': ho_scope.int(hp.quniform('c4_max_depth', 5, 20, 1)),\n",
        "                'n_estimators': ho_scope.int(hp.quniform('c4_n_estimators', 5, 30, 1)),\n",
        "                'max_features': ho_scope.int(hp.quniform('c4_max_features', 5, 20, 1)),\n",
        "                'criterion': hp.choice('c4_criterion', ['gini', 'entropy']),\n",
        "                'min_samples_split': ho_scope.int(hp.quniform('c4_min_samples_split', 2, 5,1)),\n",
        "                'min_samples_leaf': ho_scope.int(hp.quniform('c4_min_samples_leaf', 2, 5,1))\n",
        "       },\n",
        "\n",
        "     'AdaBoostClassifier':\n",
        "   {\n",
        "         'base_estimator':hp.choice('base_estimator', [DecisionTreeClassifier()]),\n",
        "          'learning_rate': hp.loguniform('c5_learning_rate', np.log(0.01), np.log(1)),\n",
        "          'n_estimators': ho_scope.int(hp.quniform('c5_n_estimators', 10, 80, 10)),\n",
        "          'algorithm': hp.choice('c5_algorithm',[\"SAMME\"])  \n",
        "       },\n",
        "       'XGBClassifier':\n",
        "    {\n",
        "         'n_estimators': ho_scope.int(hp.quniform('n_estimators', 10, 200, 10)),\n",
        "        'eta':ho_scope.float(hp.quniform('eta', 0.025, 0.5, 0.025)) ,\n",
        "        'max_depth':ho_scope.int( hp.quniform('max_depth', 2, 14,2)) ,\n",
        "        'min_child_weight':ho_scope.int(hp.quniform('min_child_weight', 1, 6, 1)) ,\n",
        "        'subsample':ho_scope.float(hp.quniform('subsample', 0.5, 1.0, 0.05)),\n",
        "        'gamma':ho_scope.float( hp.quniform('gamma', 0.5, 1, 0.05)) ,\n",
        "        'colsample_bytree': ho_scope.float(hp.quniform('colsample_bytree', 0.5, 1, 0.05)),\n",
        "        'eval_metric': 'auc',\n",
        "        'objective': 'binary:logistic',\n",
        "         'nthread': 10,\n",
        "        'booster': 'gbtree',\n",
        "        'tree_method': 'exact',\n",
        "        'silent': 1,\n",
        "       'n_iter_no_change':5\n",
        "       },\n",
        "   'LinearDiscriminantAnalysis':\n",
        "            {      \n",
        "            \n",
        "            }  ,\n",
        "  'QuadraticDiscriminantAnalysis' :\n",
        "        {\n",
        "      \n",
        "        },\n",
        " 'LGBMClassifier':\n",
        "                  {\n",
        "                   'max_depth': ho_scope.int(hp.quniform('c6_max_depth', 3, 10, 1)),\n",
        "                   'num_leave': ho_scope.int(hp.quniform('c6_num_leaves', 5, 50, 5)),\n",
        "                   'min_child_samples': ho_scope.int(hp.quniform('c6_min_child_samples', 50, 100, 10)),\n",
        "                   'scale_pos_weight': ho_scope.float(hp.quniform('c6_scale_pos_weight', 50, 100, 10)),\n",
        "                   'subsample': ho_scope.float(hp.quniform('c6_subsample', 0.6, 0.9, 1)),\n",
        "                   'colsample_bytree': ho_scope.float(hp.quniform('c6_colsample_bytree', 0.6, 0.9, 1)),\n",
        "                   'reg_lambda': hp.choice(\"c6_reg_lambda\",[0, 1e-1, 1, 5, 10, 20, 50]),\n",
        "                   'reg_alpha': hp.choice(\"c6_reg_alpha\",[0, 1e-1, 1, 5, 10, 20, 50])\n",
        "                   },     \n",
        "  'LogisticRegression ':\n",
        "  {\n",
        "      ' classifier__penalty': hp.choice(' classifier__penalty', ['l1', 'l2']),\n",
        "     'classifier__C' :hp.loguniform('classifier__C', np.log(0.1), np.log(1)) ,\n",
        "    'classifier__solver' : 'liblinear'\n",
        "  }   ,\n",
        "  'MLPClassifier':{\n",
        "      'hidden_layer_sizes': hp.choice(' c8_hidden_layer_sizes',[(50,50,50), (50,100,50), (100,)]),\n",
        "    'activation':hp.choice ('c8_activation',['tanh', 'relu']),\n",
        "    'solver': hp.choice('c8_solver',['sgd', 'adam']) ,\n",
        "    'alpha':hp.loguniform('c8_alpha',np.log(0.0001), np.log(0.05) ),\n",
        "    'learning_rate':hp.choice('c8_learning_rate', ['constant','adaptive'],)\n",
        "  },\n",
        "  'GaussianNB':\n",
        "  {\n",
        "      \n",
        "  },\n",
        "  'BaggingClassifier':\n",
        "            {      \n",
        "             'base_estimator': hp.choice('base_estimator',[DecisionTreeClassifier()]),\n",
        "              'n_estimators': ho_scope.int(hp.quniform('n_estimators', 10, 50, 10)),\n",
        "              'oob_score': hp.choice('oob_score', [True,False]),\n",
        "               'random_state': 1\n",
        "            }\n",
        "                         \n",
        "       \n",
        "} \n",
        "\n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2,random_state = 0)\n",
        "q=[2,3]\n",
        "model=[]\n",
        "best_clf3=[]\n",
        "final_clf=[]\n",
        "clf1_val_score=[]\n",
        "y_score=[]\n",
        "Fr_classifier=[]\n",
        "a1=[]\n",
        "Fr_classifier=[]\n",
        "for i in range(len(try1)):\n",
        "    param_hyperopt3={try1[i][0].__name__:param_hyperopt[try1[i][0].__name__]}\n",
        "    clf_name=try1[i][0].__name__\n",
        "    cls=try1[i][0]\n",
        "    def f_clf1(params):\n",
        "       if not (params[clf_name]):   \n",
        "           model6 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),(clf_name,cls(**f_unpack_dict(params)))])\n",
        "           return model6\n",
        "       else:\n",
        "            model6 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),(clf_name,cls(**f_unpack_dict(params[clf_name])))])\n",
        "            return model6\n",
        "    def objective_function(params,X_train2, y_train2):\n",
        "        model=f_clf1(params)\n",
        "        shuffle = KFold(n_splits=5, shuffle=True)\n",
        "        score = cross_val_score(model, X_train2, y_train2, cv=shuffle,scoring='roc_auc', n_jobs=-1).mean()\n",
        "        return {'loss': -score, 'status': STATUS_OK}  \n",
        "    trials = Trials()\n",
        "    best_clf3.append(fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                    param_hyperopt3, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))) \n",
        "    final_clf.append(f_clf1(space_eval(param_hyperopt3, best_clf3[i])).fit(X_train2, y_train2)) \n",
        "    # Calculating performance on validation set\n",
        "    y_score.append(final_clf[i].predict_proba(X_test2))  \n",
        "    clf1_val_score.append(roc_auc_score(y_test2, y_score[i][:,1])) \n",
        "    print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.format(-trials.best_trial['result']['loss'], clf1_val_score[i]))\n",
        "    a1.append(space_eval(param_hyperopt3, best_clf3[i]))\n",
        "    Fr_classifier.append({'classifier':cls,'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf1_val_score[i],'parameters':list(a1[i].values())[0]})\n",
        "    Fr_classifier"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [05:45<00:00, 40.66s/it, best loss: -0.8709960449970973]\n",
            "Cross-val score: 0.87100; validation score: 0.87230\n",
            " 60%|██████    | 6/10 [06:39<04:40, 70.20s/it, best loss: -0.8240602823420584]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [10:24<00:00, 56.27s/it, best loss: -0.8240602823420584]\n",
            "Cross-val score: 0.82406; validation score: 0.83465\n",
            "100%|██████████| 10/10 [12:20<00:00, 71.76s/it, best loss: -0.8747138258478401]\n",
            "Cross-val score: 0.87471; validation score: 0.88506\n",
            "  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [04:24<00:00, 26.48s/it, best loss: -0.8268467618939667]\n",
            "Cross-val score: 0.82685; validation score: 0.82928\n",
            "100%|██████████| 10/10 [02:44<00:00, 16.56s/it, best loss: -0.7697799769268939]\n",
            "Cross-val score: 0.76978; validation score: 0.66992\n",
            "  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [2:11:20<00:00, 746.18s/it, best loss: -0.8984435660132537]\n",
            "Cross-val score: 0.89844; validation score: 0.90127\n",
            "100%|██████████| 10/10 [09:42<00:00, 58.15s/it, best loss: -0.7604508228998579]\n",
            "Cross-val score: 0.76045; validation score: 0.76254\n",
            "  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [08:45<00:00, 50.43s/it, best loss: -0.8929875025198267]\n",
            "Cross-val score: 0.89299; validation score: 0.89377\n",
            "  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [03:32<00:00, 21.10s/it, best loss: -0.7867886912438091]\n",
            "Cross-val score: 0.78679; validation score: 0.78877\n",
            "  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [49:23<00:00, 399.05s/it, best loss: -0.9045777452981845]\n",
            "Cross-val score: 0.90458; validation score: 0.90585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95A2o2btQqdy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e8a3271f-c309-45f5-ad2d-49946e0aa37e"
      },
      "source": [
        "Fr_classifier"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Cross-val score': 0.8709960449970973,\n",
              "  'classifier': 'H',\n",
              "  'parameters': {'l2_regularization': 0.0,\n",
              "   'learning_rate': 0.029641454778718972,\n",
              "   'max_bins': 60,\n",
              "   'max_depth': 5,\n",
              "   'max_iter': 70,\n",
              "   'max_leaf_nodes': 20,\n",
              "   'min_samples_leaf': 15,\n",
              "   'n_iter_no_change': None,\n",
              "   'tol': 1e-07,\n",
              "   'validation_fraction': 0.1},\n",
              "  'validation score': 0.8723034213769378},\n",
              " {'Cross-val score': 0.8240602823420584,\n",
              "  'classifier': 'a',\n",
              "  'parameters': {'criterion': 'entropy',\n",
              "   'max_depth': 7,\n",
              "   'max_features': 'auto',\n",
              "   'min_samples_leaf': 2,\n",
              "   'min_samples_split': 3,\n",
              "   'n_estimators': 27},\n",
              "  'validation score': 0.834652607183466},\n",
              " {'Cross-val score': 0.8747138258478401,\n",
              "  'classifier': 't',\n",
              "  'parameters': {'criterion': 'entropy',\n",
              "   'max_depth': 15,\n",
              "   'max_features': 13,\n",
              "   'min_samples_leaf': 4,\n",
              "   'min_samples_split': 4,\n",
              "   'n_estimators': 13},\n",
              "  'validation score': 0.8850623181186879},\n",
              " {'Cross-val score': 0.8268467618939667,\n",
              "  'classifier': 'e',\n",
              "  'parameters': {},\n",
              "  'validation score': 0.8292819955028902},\n",
              " {'Cross-val score': 0.7697799769268939,\n",
              "  'classifier': 's',\n",
              "  'parameters': {'max_depth': 6,\n",
              "   'max_features': 'sqrt',\n",
              "   'min_samples_leaf': 9,\n",
              "   'min_samples_split': 9},\n",
              "  'validation score': 0.6699164900759476},\n",
              " {'Cross-val score': 0.8984435660132537,\n",
              "  'classifier': 'n',\n",
              "  'parameters': {'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
              "                          max_features=None, max_leaf_nodes=None,\n",
              "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                          min_samples_leaf=1, min_samples_split=2,\n",
              "                          min_weight_fraction_leaf=0.0, presort=False,\n",
              "                          random_state=None, splitter='best'),\n",
              "   'n_estimators': 50,\n",
              "   'oob_score': True,\n",
              "   'random_state': 1},\n",
              "  'validation score': 0.9012734481461568},\n",
              " {'Cross-val score': 0.7604508228998579,\n",
              "  'classifier': 's',\n",
              "  'parameters': {'algorithm': 'SAMME',\n",
              "   'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
              "                          max_features=None, max_leaf_nodes=None,\n",
              "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                          min_samples_leaf=1, min_samples_split=2,\n",
              "                          min_weight_fraction_leaf=0.0, presort=False,\n",
              "                          random_state=None, splitter='best'),\n",
              "   'learning_rate': 0.06654968297130961,\n",
              "   'n_estimators': 60},\n",
              "  'validation score': 0.7625440706795947},\n",
              " {'Cross-val score': 0.8929875025198267,\n",
              "  'classifier': 's',\n",
              "  'parameters': {'colsample_bytree': 1.0,\n",
              "   'max_depth': 9,\n",
              "   'min_child_samples': 70,\n",
              "   'num_leave': 10,\n",
              "   'reg_alpha': 5,\n",
              "   'reg_lambda': 20,\n",
              "   'scale_pos_weight': 50.0,\n",
              "   'subsample': 1.0},\n",
              "  'validation score': 0.8937718515059235},\n",
              " {'Cross-val score': 0.7867886912438091,\n",
              "  'classifier': 'c',\n",
              "  'parameters': {},\n",
              "  'validation score': 0.7887682267477347},\n",
              " {'Cross-val score': 0.9045777452981845,\n",
              "  'classifier': 'f',\n",
              "  'parameters': {'booster': 'gbtree',\n",
              "   'colsample_bytree': 0.65,\n",
              "   'eta': 0.25,\n",
              "   'eval_metric': 'auc',\n",
              "   'gamma': 0.7000000000000001,\n",
              "   'max_depth': 10,\n",
              "   'min_child_weight': 5,\n",
              "   'n_estimators': 110,\n",
              "   'n_iter_no_change': 5,\n",
              "   'nthread': 10,\n",
              "   'objective': 'binary:logistic',\n",
              "   'silent': 1,\n",
              "   'subsample': 0.7000000000000001,\n",
              "   'tree_method': 'exact'},\n",
              "  'validation score': 0.9058466258248712}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cStf2DxWSTIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifiers=[{'Cross-val score': 0.8709960449970973,\n",
        "  'classifier': 'H',\n",
        "  'parameters': {'l2_regularization': 0.0,\n",
        "   'learning_rate': 0.029641454778718972,\n",
        "   'max_bins': 60,\n",
        "   'max_depth': 5,\n",
        "   'max_iter': 70,\n",
        "   'max_leaf_nodes': 20,\n",
        "   'min_samples_leaf': 15,\n",
        "   'n_iter_no_change': None,\n",
        "   'tol': 1e-07,\n",
        "   'validation_fraction': 0.1},\n",
        "  'validation score': 0.8723034213769378},\n",
        " {'Cross-val score': 0.8240602823420584,\n",
        "  'classifier': 'a',\n",
        "  'parameters': {'criterion': 'entropy',\n",
        "   'max_depth': 7,\n",
        "   'max_features': 'auto',\n",
        "   'min_samples_leaf': 2,\n",
        "   'min_samples_split': 3,\n",
        "   'n_estimators': 27},\n",
        "  'validation score': 0.834652607183466},\n",
        " {'Cross-val score': 0.8747138258478401,\n",
        "  'classifier': 't',\n",
        "  'parameters': {'criterion': 'entropy',\n",
        "   'max_depth': 15,\n",
        "   'max_features': 13,\n",
        "   'min_samples_leaf': 4,\n",
        "   'min_samples_split': 4,\n",
        "   'n_estimators': 13},\n",
        "  'validation score': 0.8850623181186879},\n",
        " {'Cross-val score': 0.8268467618939667,\n",
        "  'classifier': 'e',\n",
        "  'parameters': {},\n",
        "  'validation score': 0.8292819955028902},\n",
        " {'Cross-val score': 0.7697799769268939,\n",
        "  'classifier': 's',\n",
        "  'parameters': {'max_depth': 6,\n",
        "   'max_features': 'sqrt',\n",
        "   'min_samples_leaf': 9,\n",
        "   'min_samples_split': 9},\n",
        "  'validation score': 0.6699164900759476},\n",
        " {'Cross-val score': 0.8984435660132537,\n",
        "  'classifier': 'n',\n",
        "  'parameters': {'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
        "                          max_features=None, max_leaf_nodes=None,\n",
        "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                          min_samples_leaf=1, min_samples_split=2,\n",
        "                          min_weight_fraction_leaf=0.0, presort=False,\n",
        "                          random_state=None, splitter='best'),\n",
        "   'n_estimators': 50,\n",
        "   'oob_score': True,\n",
        "   'random_state': 1},\n",
        "  'validation score': 0.9012734481461568},\n",
        " {'Cross-val score': 0.7604508228998579,\n",
        "  'classifier': 's',\n",
        "  'parameters': {'algorithm': 'SAMME',\n",
        "   'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
        "                          max_features=None, max_leaf_nodes=None,\n",
        "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                          min_samples_leaf=1, min_samples_split=2,\n",
        "                          min_weight_fraction_leaf=0.0, presort=False,\n",
        "                          random_state=None, splitter='best'),\n",
        "   'learning_rate': 0.06654968297130961,\n",
        "   'n_estimators': 60},\n",
        "  'validation score': 0.7625440706795947},\n",
        " {'Cross-val score': 0.8929875025198267,\n",
        "  'classifier': 's',\n",
        "  'parameters': {'colsample_bytree': 1.0,\n",
        "   'max_depth': 9,\n",
        "   'min_child_samples': 70,\n",
        "   'num_leave': 10,\n",
        "   'reg_alpha': 5,\n",
        "   'reg_lambda': 20,\n",
        "   'scale_pos_weight': 50.0,\n",
        "   'subsample': 1.0},\n",
        "  'validation score': 0.8937718515059235},\n",
        " {'Cross-val score': 0.7867886912438091,\n",
        "  'classifier': 'c',\n",
        "  'parameters': {},\n",
        "  'validation score': 0.7887682267477347},\n",
        " {'Cross-val score': 0.9045777452981845,\n",
        "  'classifier': 'f',\n",
        "  'parameters': {'booster': 'gbtree',\n",
        "   'colsample_bytree': 0.65,\n",
        "   'eta': 0.25,\n",
        "   'eval_metric': 'auc',\n",
        "   'gamma': 0.7000000000000001,\n",
        "   'max_depth': 10,\n",
        "   'min_child_weight': 5,\n",
        "   'n_estimators': 110,\n",
        "   'n_iter_no_change': 5,\n",
        "   'nthread': 10,\n",
        "   'objective': 'binary:logistic',\n",
        "   'silent': 1,\n",
        "   'subsample': 0.7000000000000001,\n",
        "   'tree_method': 'exact'},\n",
        "  'validation score': 0.9058466258248712}]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URDiSAjgTAGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "newlist = sorted(classifiers, key=itemgetter('Cross-val score'), reverse=True)\n",
        "fiv=[]\n",
        "for i in range(len(newlist)):\n",
        "     if round(newlist[i]['Cross-val score'],2)>0.77:\n",
        "        fiv.append(newlist[i])    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-ZPMwgETLqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "newlist = sorted(classifiers, key=itemgetter('Cross-val score'), reverse=True)\n",
        "fiv=[]\n",
        "for i in range(len(newlist)):\n",
        "     if round(newlist[i]['Cross-val score'],2)>0.77:\n",
        "        fiv.append(newlist[i])  \n",
        "        \n",
        "names=[]\n",
        "for i, _ in enumerate(Accepted):\n",
        "    names.append(\"pipeline\"+str(i+1))  \n",
        "AcceptedR=[]\n",
        "for i in range(len(Accepted2)):\n",
        "    AcceptedR.append(Accepted[i][1])\n",
        "dict = {}\n",
        "for name, Accepted_val in zip(names, AcceptedR ):\n",
        "    dict[name] =make_pipeline(Accepted_val(),fiv[AcceptedR.index(Accepted_val)]['classifier'](*fiv[AcceptedR.index(Accepted_val)]['parameters']))\n",
        "\n",
        "sclf2 = StackingClassifier(classifiers=list(dict.values()), meta_classifier=LogisticRegression()) \n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test2, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test2, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faKetaE3uN4I",
        "colab_type": "code",
        "outputId": "8c0df53c-d1a1-4b42-9e8e-1bbe46533862",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "q=[2,3]\n",
        "model=[]\n",
        "best_clf3=[]\n",
        "for i in range(len(q)):\n",
        "    param_hyperopt3={try1[i][0].__name__:param_hyperopt[try1[i][0].__name__]}\n",
        "    clf_name=try1[i][0].__name__\n",
        "    cls=try1[i][0]\n",
        "    def f_clf1(params):\n",
        "            model6 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),(clf_name,cls(**f_unpack_dict(params)))])\n",
        "            return model6\n",
        "    def objective_function(params,X_train2, y_train2):\n",
        "        model=f_clf1(params)\n",
        "        shuffle = KFold(n_splits=5, shuffle=True)\n",
        "        score = cross_val_score(model, X_train2, y_train2, cv=shuffle,scoring='roc_auc', n_jobs=-1).mean()\n",
        "        return {'loss': -score, 'status': STATUS_OK}  \n",
        "    trials = Trials()\n",
        "    best_clf3.append(fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                    param_hyperopt[try1[i][0].__name__], algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))) \n",
        "    clf1.append(f_clf1(space_eval(param_hyperopt[try1[i][0].__name__], best_clf1[i])).fit(X_train2, y_train2)) \n",
        "  # Calculating performance on validation set\n",
        "  y_score = clf1.predict_proba(X_test2)\n",
        "  clf1_val_score = roc_auc_score(y_test2, y_score[:,1])\n",
        "  print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.format(-trials.best_trial['result']['loss'], clf1_val_score))\n",
        "  #print('Best parameters:')\n",
        "  #print(space_eval(param_hyperopt, best_clf1))\n",
        "  a1=space_eval(param_hyperopt3, best_clf1)\n",
        "  Fr_classifier={'classifier':clf_name,'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf1_val_score,'parameters':list(a1.values())[0]}\n",
        "  Fr_classifier"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 20%|██        | 2/10 [00:46<03:16, 24.52s/it, best loss: -0.8486475203626231]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [03:20<00:00, 19.81s/it, best loss: -0.8533252929083949]\n",
            "100%|██████████| 10/10 [08:10<00:00, 44.42s/it, best loss: -0.8205002267315795]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0VoEBEovJIe",
        "colab_type": "code",
        "outputId": "ce9f526f-8f54-4808-abdc-96e7e3e176a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "best_clf3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'c3_learning_rate': 0.029641454778718972,\n",
              "  'c3_max_bins': 60.0,\n",
              "  'c3_max_depth': 5.0,\n",
              "  'c3_max_iter': 70.0,\n",
              "  'c3_max_leaf_nodes': 20.0,\n",
              "  'c3_min_samples_leaf': 15.0},\n",
              " {'c2_criterion': 0,\n",
              "  'c2_max_depth': 7.0,\n",
              "  'c2_max_features': 0,\n",
              "  'c2_min_samples_leaf': 2.0,\n",
              "  'c2_min_samples_split': 3.0,\n",
              "  'c2_n_estimators': 23.0},\n",
              " {'c4_criterion': 1,\n",
              "  'c4_max_depth': 15.0,\n",
              "  'c4_max_features': 13.0,\n",
              "  'c4_min_samples_leaf': 4.0,\n",
              "  'c4_min_samples_split': 4.0,\n",
              "  'c4_n_estimators': 13.0},\n",
              " {'base_estimator': 0,\n",
              "  'c5_algorithm': 0,\n",
              "  'c5_learning_rate': 0.010638933664917378,\n",
              "  'c5_n_estimators': 70.0},\n",
              " {},\n",
              " {'c0_max_depth': 6.0,\n",
              "  'c0_max_features': 1,\n",
              "  'c0_min_samples_leaf': 9.0,\n",
              "  'c0_min_samples_split': 9.0}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bkp-7STuMRJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "\n",
        "param_hyperopt1= {\n",
        "   'ExtraTreesClassifier':\n",
        "            {\n",
        "                'max_depth': ho_scope.int(hp.quniform('c4_max_depth', 5, 20, 1)),\n",
        "                'n_estimators': ho_scope.int(hp.quniform('c4_n_estimators', 5, 30, 1)),\n",
        "                'max_features': ho_scope.int(hp.quniform('c4_max_features', 5, 20, 1)),\n",
        "                'criterion': hp.choice('c4_criterion', ['gini', 'entropy']),\n",
        "                'min_samples_split': ho_scope.int(hp.quniform('c4_min_samples_split', 2, 5,1)),\n",
        "                'min_samples_leaf': ho_scope.int(hp.quniform('c4_min_samples_leaf', 2, 5,1))      \n",
        "             }     \n",
        "       \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2,random_state = 0)\n",
        "def f_clf1(params):\n",
        "  model6 =Pipeline(steps=[('encoder',Accepted[0][1]()),\n",
        "             ('c4',ExtraTreesClassifier(**f_unpack_dict(params['ExtraTreesClassifier'])))])\n",
        "  return model6    \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf1(params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf1 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2), param_hyperopt1, algo=tpe.suggest, max_evals=10,\n",
        "                 trials=trials, rstate=np.random.RandomState(1))\n",
        "clf1 = f_clf1(space_eval(param_hyperopt1, best_clf1)).fit(X_train2, y_train2)\n",
        "# Calculating performance on validation set\n",
        "y_score = clf1.predict_proba(X_test2)\n",
        "clf1_val_score = roc_auc_score(y_test2, y_score[:,1])\n",
        "#clf2_val_score = model_selection.cross_val_score(f_clf2, X_train2,  y_train2, cv=5, scoring='roc_auc')\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf1_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(space_eval(param_hyperopt, best_clf1))\n",
        "a1=space_eval(param_hyperopt1, best_clf1)\n",
        "Fr_classifier={'classifier':list(a1.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf1_val_score,'parameters':list(a1.values())[0]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiM49RkuqGq3",
        "colab_type": "code",
        "outputId": "1c464dd0-b943-40f7-e719-edd1caebb12e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.ensemble.forest import RandomForestClassifier\n",
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "\n",
        "param_hyperopt2= {\n",
        "  'RandomForestClassifier': \n",
        "            {\n",
        "            'max_depth': ho_scope.int(hp.quniform('c2_max_depth', 5, 7, 1)),\n",
        "            'n_estimators': ho_scope.int(hp.quniform('c2_n_estimators', 5, 30, 1)),\n",
        "            'max_features': hp.choice('c2_max_features', ['auto', 'sqrt']),\n",
        "            'criterion': hp.choice('c2_criterion', ['gini', 'entropy']),\n",
        "            'min_samples_split': ho_scope.int(hp.quniform('c2_min_samples_split', 2, 5,1)),\n",
        "            'min_samples_leaf': ho_scope.int(hp.quniform('c2_min_samples_leaf', 2, 5,1))\n",
        "                          \n",
        "         } \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "def f_clf2(params):\n",
        "  model4 =Pipeline(steps=[('encoder',Accepted[1][1]()),('c2',try1[1][0](**f_unpack_dict(params[try1[1][0].__name__])))])\n",
        "  return model4    \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf2(params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf2 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                 param_hyperopt2, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "clf2 = f_clf2(space_eval(param_hyperopt2, best_clf2)).fit(X_train2, y_train2)\n",
        "# Calculating performance on validation set\n",
        "y_score = clf2.predict_proba(X_test2)\n",
        "clf2_val_score = roc_auc_score(y_test2, y_score[:,1])\n",
        "#clf2_val_score = model_selection.cross_val_score(f_clf2, X_train2,  y_train2, cv=5, scoring='roc_auc')\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf2_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(\n",
        "a2=space_eval(param_hyperopt2, best_clf2)\n",
        "Se_classifier={'classifier':list(a2.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf2_val_score,'parameters':list(a2.values())[0]}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [06:52<00:00, 37.99s/it, best loss: -0.8222612508481044]\n",
            "Cross-val score: 0.82226; validation score: 0.83310\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKMWqZQ8L3JV",
        "colab_type": "code",
        "outputId": "598978e8-4c35-4ba5-afb2-f7e0e000bc77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.utils import shuffle\n",
        "param_hyperopt3= {      \n",
        "       'LinearDiscriminantAnalysis':\n",
        "            {      \n",
        "            \n",
        "            }\n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "def f_clf3(params):\n",
        "   model1 =Pipeline(steps=[('encoder',Accepted[2][1]()),('LinearDiscriminantAnalysis', LinearDiscriminantAnalysis())])   \n",
        "   return model1\n",
        "        \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf3(params)\n",
        "    shuffle = KFold(n_splits=5, shuffle=True)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=shuffle,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf3 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                param_hyperopt3, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "clf3 = f_clf3(space_eval(param_hyperopt3, best_clf3)).fit(X_train2, y_train2)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf3_val_score = roc_auc_score(y_test, clf3.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf3_val_score))\n",
        "print('Best parameters:')\n",
        "print(space_eval(param_hyperopt3, best_clf3))\n",
        "# Calculating performance on validation set\n",
        "clf3_val_score = roc_auc_score(y_test, clf3.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf3_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(\n",
        "a3=space_eval(param_hyperopt3, best_clf3)\n",
        "Thrid_classifier={'classifier':list(a3.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf3_val_score,'parameters':list(a3.values())[0]}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [02:37<00:00, 15.45s/it, best loss: -0.8268064630182487]\n",
            "Cross-val score: 0.82681; validation score: 0.82719\n",
            "Best parameters:\n",
            "{'LinearDiscriminantAnalysis': {}}\n",
            "Cross-val score: 0.82681; validation score: 0.82719\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv4WPj9XX5OS",
        "colab_type": "code",
        "outputId": "465ba13b-a8b0-43bc-d282-2f3c166c1370",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.utils import shuffle\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "import xgboost as xgb\n",
        "\n",
        "param_hyperopt4= {\n",
        "  'XGBClassifier':\n",
        "      {\n",
        "           'max_depth': hp.choice(\"x_max_depth\", np.arange(5, 10, dtype=int)),\n",
        "           'min_child_weight': hp.choice ('x_min_child',np.arange(1, 5, dtype=int)),\n",
        "           'subsample': hp.uniform ('x_subsample', 0.8, 1),\n",
        "           'n_estimators': ho_scope.int(hp.quniform('c2_n_estimators', 5, 40, 1)),\n",
        "       }         \n",
        "       \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "def f_clf4(params):\n",
        "   model1 =Pipeline(steps=[('encoder',Accepted[3][1]()),('XGBClassifier', xgb.XGBClassifier())])   \n",
        "   return model1  \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf4(params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf4 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                param_hyperopt4, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "clf4 = f_clf4(space_eval(param_hyperopt4, best_clf4)).fit(X_train2, y_train2)\n",
        "# Calculating performance on validation set\n",
        "clf4_val_score = roc_auc_score(y_test, clf4.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf4_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(\n",
        "a4=space_eval(param_hyperopt4, best_clf4)\n",
        "Four_classifier={'classifier':list(a4.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf4_val_score,'parameters':list(a4.values())[0]}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [02:31<22:41, 151.31s/it, best loss: -0.8761819849027198]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [05:01<20:08, 151.10s/it, best loss: -0.8761819849027198]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [07:32<17:37, 151.07s/it, best loss: -0.8761819849027198]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [10:04<15:08, 151.36s/it, best loss: -0.8761819849027198]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [12:37<12:38, 151.68s/it, best loss: -0.8761819849027198]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [15:13<10:12, 153.06s/it, best loss: -0.8761819849027198]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [17:46<07:39, 153.05s/it, best loss: -0.8761819849027198]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [20:18<05:05, 152.79s/it, best loss: -0.8761819849027198]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [22:54<02:33, 153.75s/it, best loss: -0.8761819849027198]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 10/10 [25:29<00:00, 153.97s/it, best loss: -0.8761819849027198]\n",
            "Cross-val score: 0.87618; validation score: 0.87700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6WfRi1-5dev",
        "colab_type": "code",
        "outputId": "bf902d14-d717-44be-bbe6-74013e68ac1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll import scope as ho_scope\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.utils import shuffle\n",
        "import lightgbm as lgb\n",
        "from functools import partial\n",
        "\n",
        "param_hyperopt5= {\n",
        "   'LGBMClassifier1':\n",
        "                  {\n",
        "                   'max_depth': ho_scope.int(hp.quniform('c16_max_depth', -2, 10, 1)),\n",
        "                   'num_leave': ho_scope.int(hp.quniform('c16_num_leaves', 10, 200, 10)),\n",
        "                   'min_child_samples': ho_scope.int(hp.quniform('c16_min_child_samples', 10, 90, 10)),\n",
        "                   'scale_pos_weight': ho_scope.int(hp.quniform('c16_scale_pos_weight', 10, 90, 10)),\n",
        "                   'subsample': ho_scope.float(hp.quniform('c16_subsample', 0.1, 0.9, 0.1)),\n",
        "                   'colsample_bytree': ho_scope.float(hp.quniform('c16_colsample_bytree', 0.1, 0.9, 0.1)),\n",
        "                   'reg_lambda': hp.choice(\"c16_reg_lambda\",[0, 1e-1, 1, 5, 10, 20, 50]),\n",
        "                   'reg_alpha': hp.choice(\"c16_reg_alpha\",[0, 1e-1, 1, 5, 10, 20, 50]),\n",
        "                   'bagging_fraction': ho_scope.float(hp.quniform('c16_bagging_fraction', 0.1, 0.9, 0.05)),\n",
        "                    'bagging_freq': ho_scope.int(hp.quniform('c6_bagging_freq', 1, 8, 1)),\n",
        "                   'min_data_in_leaf': ho_scope.int(hp.quniform('c16_min_data_in_leaf', 100, 1000, 100)),\n",
        "                  'min_sum_hessian_in_leaf': ho_scope.int(hp.quniform('c16_min_sum_hessian_in_leaf', 5, 20, 5)),\n",
        "                   'max_bin': ho_scope.int(hp.quniform('c16_max_bin', 10, 100, 10)),\n",
        "                   'learning_rate': ho_scope.float(hp.quniform('c16_learning_rate', 0.001, 0.09, 0.005)),\n",
        "                   'num_iterations': ho_scope.int(hp.quniform('c16_num_iterations', 100, 10000, 100)),\n",
        "                   'n_iter_no_change':10\n",
        "                       },\n",
        "                  \n",
        "                   'LGBMClassifier':\n",
        "                  {\n",
        "                   'max_depth': ho_scope.int(hp.quniform('c6_max_depth', 3, 10, 1)),\n",
        "                   'num_leave': ho_scope.int(hp.quniform('c6_num_leaves', 5, 50, 5)),\n",
        "                   'min_child_samples': ho_scope.int(hp.quniform('c6_min_child_samples', 50, 100, 10)),\n",
        "                   'scale_pos_weight': ho_scope.float(hp.quniform('c6_scale_pos_weight', 50, 100, 10)),\n",
        "                   'subsample': ho_scope.float(hp.quniform('c6_subsample', 0.6, 0.9, 1)),\n",
        "                   'colsample_bytree': ho_scope.float(hp.quniform('c6_colsample_bytree', 0.6, 0.9, 1)),\n",
        "                   'reg_lambda': hp.choice(\"c6_reg_lambda\",[0, 1e-1, 1, 5, 10, 20, 50]),\n",
        "                   'reg_alpha': hp.choice(\"c6_reg_alpha\",[0, 1e-1, 1, 5, 10, 20, 50]),\n",
        "                    'max_bin': ho_scope.int(hp.quniform('c6_max_bin', 10, 100, 10)),\n",
        "                   'learning_rate': ho_scope.float(hp.quniform('c6_learning_rate', 0.001, 0.01, 0.005)),\n",
        "                   'num_iterations': ho_scope.int(hp.quniform('c6_num_iterations', 100, 1000, 100)),\n",
        "                   'n_iter_no_change':10 \n",
        "                   }     \n",
        "} \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "def f_clf5(params):\n",
        "   model1 =Pipeline(steps=[('encoder',ce.cat_boost.CatBoostEncoder()),('LGBMClassifier', \n",
        "                                                         lgb.LGBMClassifier(**f_unpack_dict(params['LGBMClassifier'])))])   \n",
        "   return model1  \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf5(params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf5 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                param_hyperopt5, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "clf5 = f_clf5(space_eval(param_hyperopt5, best_clf5)).fit(X_train2, y_train2)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf5_val_score = roc_auc_score(y_test, clf5.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf5_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(\n",
        "a5=space_eval(param_hyperopt5, best_clf5)\n",
        "fith_classifier={'classifier':list(a5.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf5_val_score,'parameters':list(a5.values())[0]}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 30%|███       | 3/10 [09:06<23:38, 202.65s/it, best loss: -0.862257055392101]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3If0WbQtPcQM",
        "colab_type": "code",
        "outputId": "db558e83-8156-45ed-91db-65fbd9e211cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "from hyperopt.pyll import scope as ho_scope\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.utils import shuffle\n",
        "param_hyperopt6= {\n",
        "    'QuadraticDiscriminantAnalysis':\n",
        "      {\n",
        "           \n",
        "      }      \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "def f_clf6(params):\n",
        "   model3 =Pipeline(steps=[('encoder',Accepted[5][1]()),('QuadraticDiscriminantAnalysis',QuadraticDiscriminantAnalysis())])\n",
        "   return model3 \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf6(params)\n",
        "    #clf = RandomForestClassifier(**params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf6 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                param_hyperopt6, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "\n",
        "clf6 = f_clf6(space_eval(param_hyperopt6, best_clf6)).fit(X_train2, y_train2)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf6_val_score = roc_auc_score(y_test, clf6.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.format(-trials.best_trial['result']['loss'], clf6_val_score))\n",
        "print('Best parameters:')\n",
        "print(space_eval(param_hyperopt6, best_clf6))\n",
        "# Calculating performance on validation set\n",
        "clf6_val_score = roc_auc_score(y_test, clf6.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf6_val_score))\n",
        "#print('Best parameters:')\n",
        "#print(\n",
        "a6=space_eval(param_hyperopt6, best_clf6)\n",
        "six_classifier={'classifier':list(a6.keys())[0],'Cross-val score':-trials.best_trial['result']['loss'],'validation score':clf6_val_score,'parameters':list(a6.values())[0]}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [02:06<00:00, 12.49s/it, best loss: -0.786171897798788]\n",
            "Cross-val score: 0.78617; validation score: 0.78724\n",
            "Best parameters:\n",
            "{'QuadraticDiscriminantAnalysis': {}}\n",
            "Cross-val score: 0.78617; validation score: 0.78724\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oUvZj4cvDNH",
        "colab_type": "code",
        "outputId": "1e3ebf97-c916-412e-e1a6-4aaf36265c24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "pipe1 = make_pipeline(Accepted[3][1](),XGBClassifier(**Four_classifier['parameters']))\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier(**Se_classifier['parameters']))  \n",
        "pipe3 = make_pipeline(Accepted[0][1](),ExtraTreesClassifier(**Fr_classifier['parameters'])) \n",
        "pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier(**fith_classifier['parameters'])) \n",
        "pipe4 = make_pipeline(Accepted[2][1](),LinearDiscriminantAnalysis())\n",
        "pipe5 = make_pipeline(Accepted[5][1](),QuadraticDiscriminantAnalysis())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe5,pipe6], meta_classifier=LogisticRegression()) \n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test2, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test2, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.82      0.77     94841\n",
            "           1       0.85      0.76      0.80    120517\n",
            "\n",
            "    accuracy                           0.79    215358\n",
            "   macro avg       0.79      0.79      0.79    215358\n",
            "weighted avg       0.79      0.79      0.79    215358\n",
            "\n",
            "StackingClassifier score: 0.789651\n",
            "Cross-val score: 0.85474\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZRFjh3jzf8x",
        "colab_type": "code",
        "outputId": "5be9aa9d-e324-48f6-b8fc-a5d2cfb4face",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier())  \n",
        "pipe3 = make_pipeline(Accepted[0][1](),ExtraTreesClassifier()) \n",
        "pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier()) \n",
        "pipe4 = make_pipeline(Accepted[2][1](),LinearDiscriminantAnalysis())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe2,pipe3,pipe4,pipe6], meta_classifier=LogisticRegression()) \n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test2, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test2, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.76      0.76     94841\n",
            "           1       0.81      0.81      0.81    120517\n",
            "\n",
            "    accuracy                           0.79    215358\n",
            "   macro avg       0.79      0.79      0.79    215358\n",
            "weighted avg       0.79      0.79      0.79    215358\n",
            "\n",
            "StackingClassifier score: 0.905730\n",
            "Cross-val score: 0.86727\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA4P3yftv5VC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Fr_classifier={'Cross-val score': 0.8755075622861487,\n",
        " 'classifier': 'ExtraTreesClassifier',\n",
        " 'parameters': {'criterion': 'entropy',\n",
        "  'max_depth': 15,\n",
        "  'max_features': 13,\n",
        "  'min_samples_leaf': 4,\n",
        "  'min_samples_split': 4,\n",
        "  'n_estimators': 13},\n",
        " 'validation score': 0.884938764808794}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72q7zxSvwQK6",
        "colab_type": "code",
        "outputId": "367c1766-0447-4db5-c048-a6e2f9b376a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "Fr_classifier['parameters']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'criterion': 'entropy',\n",
              " 'max_depth': 15,\n",
              " 'max_features': 13,\n",
              " 'min_samples_leaf': 4,\n",
              " 'min_samples_split': 4,\n",
              " 'n_estimators': 13}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT1x5xQ-vWFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Se_classifier={'Cross-val score': 0.8273926245659394,\n",
        " 'classifier': 'RandomForestClassifier',\n",
        " 'parameters': {'criterion': 'gini',\n",
        "  'max_depth': 7,\n",
        "  'max_features': 'auto',\n",
        "  'min_samples_leaf': 2,\n",
        "  'min_samples_split': 3,\n",
        "  'n_estimators': 23},\n",
        " 'validation score': 0.7973701565847332}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jb9jiievacj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Thrid_classifier={'Cross-val score': 0.8268064630182487,\n",
        " 'classifier': 'LinearDiscriminantAnalysis',\n",
        " 'parameters': {},\n",
        " 'validation score': 0.827194350739109}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWLcgpDlvecC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Four_classifier={'Cross-val score': 0.8761819849027198,\n",
        " 'classifier': 'XGBClassifier',\n",
        " 'parameters': {'max_depth': 7,\n",
        "  'min_child_weight': 1,\n",
        "  'n_estimators': 38,\n",
        "  'subsample': 0.8479101254812229},\n",
        " 'validation score': 0.8770024548105813}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT7GCH_evh_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fith_classifier={'Cross-val score': 0.8931760781045686,\n",
        " 'classifier': 'LGBMClassifier',\n",
        " 'parameters': {'colsample_bytree': 1.0,\n",
        "  'max_depth': 9,\n",
        "  'min_child_samples': 70,\n",
        "  'num_leave': 10,\n",
        "  'reg_alpha': 5,\n",
        "  'reg_lambda': 20,\n",
        "  'scale_pos_weight': 50.0,\n",
        "  'subsample': 1.0},\n",
        " 'validation score': 0.8944420319608248}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOeAy9aqvmT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "six_classifier={'Cross-val score': 0.786171897798788,\n",
        " 'classifier': 'QuadraticDiscriminantAnalysis',\n",
        " 'parameters': {},\n",
        " 'validation score': 0.787235957146868}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoY5YxNhSY0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a8={'ExtraTreesClassifier': {'criterion': 'gini',\n",
        "  'max_depth': 7,\n",
        "  'max_features': 7,\n",
        "  'min_samples_leaf': 4,\n",
        "  'min_samples_split': 4,\n",
        "  'n_estimators': 19},\n",
        " 'LGBMClassifier': {'colsample_bytree': 1.0,\n",
        "  'max_depth': 7,\n",
        "  'min_child_samples': 80,\n",
        "  'num_leave': 50,\n",
        "  'reg_alpha': 0,\n",
        "  'reg_lambda': 10,\n",
        "  'scale_pos_weight': 70.0,\n",
        "  'subsample': 1.0},\n",
        " 'RandomForestClassifier': {'criterion': 'entropy',\n",
        "  'max_depth': 5,\n",
        "  'max_features': 'sqrt',\n",
        "  'min_samples_leaf': 3,\n",
        "  'min_samples_split': 2,\n",
        "  'n_estimators': 30},\n",
        " 'XGBClassifier': {'max_depth': 13,\n",
        "  'min_child_weight': 3,\n",
        "  'subsample': 0.9406880083709813}}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56pPpNhbSxXu",
        "colab_type": "code",
        "outputId": "b063ae00-ef91-43ba-f31b-0f0b891d9107",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "list(a8.keys())[0]==type(ExtraTreesClassifier()).__name__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR-MiLGdTMB7",
        "colab_type": "code",
        "outputId": "476b9603-b415-4c6a-c498-86b99284806f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        " model3 =Pipeline(steps=[('encoder',Accepted[5][1]()),('QuadraticDiscriminantAnalysis',QuadraticDiscriminantAnalysis())])\n",
        "type(model3[0]).__name__ "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'WOEEncoder'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx6XQUooUNdL",
        "colab_type": "code",
        "outputId": "d975c48a-29a5-4bd8-8563-4c8e257f309c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "list(a8.keys())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ExtraTreesClassifier',\n",
              " 'LGBMClassifier',\n",
              " 'RandomForestClassifier',\n",
              " 'XGBClassifier']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQZrDLCO4tzY",
        "colab_type": "code",
        "outputId": "c582b68c-85a5-4e0b-8489-5f14779fcd65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "{'ExtraTreesClassifier': {'criterion': 'gini',\n",
        "  'max_depth': 7,\n",
        "  'max_features': 7,\n",
        "  'min_samples_leaf': 4,\n",
        "  'min_samples_split': 4,\n",
        "  'n_estimators': 19},\n",
        " 'LGBMClassifier': {'colsample_bytree': 1.0,\n",
        "  'max_depth': 7,\n",
        "  'min_child_samples': 80,\n",
        "  'num_leave': 50,\n",
        "  'reg_alpha': 0,\n",
        "  'reg_lambda': 10,\n",
        "  'scale_pos_weight': 70.0,\n",
        "  'subsample': 1.0},\n",
        " 'RandomForestClassifier': {'criterion': 'entropy',\n",
        "  'max_depth': 5,\n",
        "  'max_features': 'sqrt',\n",
        "  'min_samples_leaf': 3,\n",
        "  'min_samples_split': 2,\n",
        "  'n_estimators': 30},\n",
        " 'XGBClassifier': {'max_depth': 13,\n",
        "  'min_child_weight': 3,\n",
        "  'subsample': 0.9406880083709813}}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e735912700ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mFr_classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'Fr_classifier' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc6sc6QfbVhx",
        "colab_type": "code",
        "outputId": "29f521ad-9c86-49e6-8b04-a46f03df8b63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier())\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier())\n",
        "pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier())\n",
        "pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe6], meta_classifier=LogisticRegression())\n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test2, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test2, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.78      0.77     94841\n",
            "           1       0.82      0.80      0.81    120517\n",
            "\n",
            "    accuracy                           0.79    215358\n",
            "   macro avg       0.79      0.79      0.79    215358\n",
            "weighted avg       0.79      0.79      0.79    215358\n",
            "\n",
            "StackingClassifier score: 0.913023\n",
            "Cross-val score: 0.86650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxWwikwQVEhi",
        "colab_type": "code",
        "outputId": "b3d5b3a5-0fec-48c2-c0b2-1451a54da5ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier(**a8['RandomForestClassifier']))  \n",
        "pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier(**a8['ExtraTreesClassifier'])) \n",
        "pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier(**a8['LGBMClassifier'])) \n",
        "pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe2,pipe3,pipe4,pipe6], meta_classifier=LogisticRegression()) \n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test2, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.48      0.46     94433\n",
            "           1       0.56      0.52      0.54    120925\n",
            "\n",
            "    accuracy                           0.50    215358\n",
            "   macro avg       0.50      0.50      0.50    215358\n",
            "weighted avg       0.51      0.50      0.50    215358\n",
            "\n",
            "StackingClassifier score: 0.809074\n",
            "Cross-val score: 0.50016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_NjQ3a2n3HR",
        "colab_type": "code",
        "outputId": "6020a5fe-fbf9-484b-8fac-728a0220d551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "for clf, label in zip([list(sclf2.named_classifiers.keys()),sclf2],[list(a8.keys())]):\n",
        "  for i in range(len(list(a8.keys()))):\n",
        "    if list(a8.keys())[i]  in label:\n",
        "      if (type((sclf2.named_classifiers[clf[0]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe1 = make_pipeline(Accepted[0][1](),\n",
        "                                XGBClassifier(**a8[(type((sclf2.named_classifiers[clf[0]][1])).__name__)]))\n",
        "      if (type((sclf2.named_classifiers[clf[1]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe2 = make_pipeline(Accepted[1][1](),\n",
        "                                RandomForestClassifier(**a8[(type((sclf2.named_classifiers[clf[1]][1])).__name__)]))  \n",
        "      if (type((sclf2.named_classifiers[clf[2]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe3 = make_pipeline(Accepted[2][1](),\n",
        "                                ExtraTreesClassifier(**a8[(type((sclf2.named_classifiers[clf[2]][1])).__name__)])) \n",
        "      if (type((sclf2.named_classifiers[clf[5]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe6 = make_pipeline(Accepted[5][1](),\n",
        "                                lgb.LGBMClassifier(**a8[(type((sclf2.named_classifiers[clf[5]][1])).__name__)])) \n",
        "    else:  \n",
        "            pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "            pipe5 = make_pipeline(Accepted[4][1](),QuadraticDiscriminantAnalysis())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe5,pipe6], meta_classifier=LogisticRegression()) \n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test2, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test2, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.84      0.79     94841\n",
            "           1       0.86      0.78      0.82    120517\n",
            "\n",
            "    accuracy                           0.81    215358\n",
            "   macro avg       0.81      0.81      0.81    215358\n",
            "weighted avg       0.81      0.81      0.81    215358\n",
            "\n",
            "StackingClassifier score: 0.813905\n",
            "Cross-val score: 0.87589\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwMS_lRx-uAd",
        "colab_type": "code",
        "outputId": "8b757d76-a14b-410a-f446-2526c2827122",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sclf2.named_classifiers[]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pipeline-1': Pipeline(memory=None,\n",
              "          steps=[('jamessteinencoder',\n",
              "                  JamesSteinEncoder(cols=None, drop_invariant=False,\n",
              "                                    handle_missing='value',\n",
              "                                    handle_unknown='value', model='independent',\n",
              "                                    random_state=None, randomized=False,\n",
              "                                    return_df=True, sigma=0.05, verbose=0)),\n",
              "                 ('randomforestclassifier',\n",
              "                  RandomForestClassifier(bootstrap=True, class_weight=None,\n",
              "                                         criterion='gini', max_depth=None,\n",
              "                                         max_features='auto',\n",
              "                                         max_leaf_nodes=None,\n",
              "                                         min_impurity_decrease=0.0,\n",
              "                                         min_impurity_split=None,\n",
              "                                         min_samples_leaf=1, min_samples_split=2,\n",
              "                                         min_weight_fraction_leaf=0.0,\n",
              "                                         n_estimators='warn', n_jobs=None,\n",
              "                                         oob_score=False, random_state=None,\n",
              "                                         verbose=0, warm_start=False))],\n",
              "          verbose=False), 'pipeline-2': Pipeline(memory=None,\n",
              "          steps=[('catboostencoder',\n",
              "                  CatBoostEncoder(a=1, cols=None, drop_invariant=False,\n",
              "                                  handle_missing='value', handle_unknown='value',\n",
              "                                  random_state=None, return_df=True, sigma=None,\n",
              "                                  verbose=0)),\n",
              "                 ('extratreesclassifier',\n",
              "                  ExtraTreesClassifier(bootstrap=False, class_weight=None,\n",
              "                                       criterion='gini', max_depth=None,\n",
              "                                       max_features='auto', max_leaf_nodes=None,\n",
              "                                       min_impurity_decrease=0.0,\n",
              "                                       min_impurity_split=None,\n",
              "                                       min_samples_leaf=1, min_samples_split=2,\n",
              "                                       min_weight_fraction_leaf=0.0,\n",
              "                                       n_estimators='warn', n_jobs=None,\n",
              "                                       oob_score=False, random_state=None,\n",
              "                                       verbose=0, warm_start=False))],\n",
              "          verbose=False), 'pipeline-3': Pipeline(memory=None,\n",
              "          steps=[('mestimateencoder',\n",
              "                  MEstimateEncoder(cols=None, drop_invariant=False,\n",
              "                                   handle_missing='value',\n",
              "                                   handle_unknown='value', m=1.0,\n",
              "                                   random_state=None, randomized=False,\n",
              "                                   return_df=True, sigma=0.05, verbose=0)),\n",
              "                 ('lineardiscriminantanalysis',\n",
              "                  LinearDiscriminantAnalysis(n_components=None, priors=None,\n",
              "                                             shrinkage=None, solver='svd',\n",
              "                                             store_covariance=False,\n",
              "                                             tol=0.0001))],\n",
              "          verbose=False), 'pipeline-4': Pipeline(memory=None,\n",
              "          steps=[('woeencoder',\n",
              "                  WOEEncoder(cols=None, drop_invariant=False,\n",
              "                             handle_missing='value', handle_unknown='value',\n",
              "                             random_state=None, randomized=False,\n",
              "                             regularization=1.0, return_df=True, sigma=0.05,\n",
              "                             verbose=0)),\n",
              "                 ('lgbmclassifier',\n",
              "                  LGBMClassifier(boosting_type='gbdt', class_weight=None,\n",
              "                                 colsample_bytree=1.0, importance_type='split',\n",
              "                                 learning_rate=0.1, max_depth=-1,\n",
              "                                 min_child_samples=20, min_child_weight=0.001,\n",
              "                                 min_split_gain=0.0, n_estimators=100, n_jobs=-1,\n",
              "                                 num_leaves=31, objective=None,\n",
              "                                 random_state=None, reg_alpha=0.0,\n",
              "                                 reg_lambda=0.0, silent=True, subsample=1.0,\n",
              "                                 subsample_for_bin=200000, subsample_freq=0))],\n",
              "          verbose=False)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmXql2VSupkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import combinations\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from xgboost.sklearn import XGBClassifier  \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import lightgbm as lgb\n",
        "import category_encoders as ce\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score,classification_report\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier(**a8['XGBClassifier']))\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier(**a8['RandomForestClassifier']))  \n",
        "pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier(**a8['ExtraTreesClassifier'])) \n",
        "pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier(**a8['LGBMClassifier'])) \n",
        "pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "pipe5 = make_pipeline(Accepted[4][1](),QuadraticDiscriminantAnalysis())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe5,pipe6], meta_classifier=LogisticRegression()) \n",
        "model2=sclf2.fit(X_train2, y_train2)\n",
        "y_true3, y_pred3  =y_test, model2.predict(X_test2)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train2, y_train2))\n",
        "clf2_val_score = roc_auc_score(y_test, sclf2.predict_proba(X_test2)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVt5r7R6nsh3",
        "colab_type": "code",
        "outputId": "e6169280-bbd4-43fe-e1bc-db87afbce428",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "a8"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ExtraTreesClassifier': {'criterion': 'gini',\n",
              "  'max_depth': 7,\n",
              "  'max_features': 7,\n",
              "  'min_samples_leaf': 4,\n",
              "  'min_samples_split': 4,\n",
              "  'n_estimators': 19},\n",
              " 'LGBMClassifier': {'colsample_bytree': 1.0,\n",
              "  'max_depth': 7,\n",
              "  'min_child_samples': 80,\n",
              "  'num_leave': 50,\n",
              "  'reg_alpha': 0,\n",
              "  'reg_lambda': 10,\n",
              "  'scale_pos_weight': 70.0,\n",
              "  'subsample': 1.0},\n",
              " 'RandomForestClassifier': {'criterion': 'entropy',\n",
              "  'max_depth': 5,\n",
              "  'max_features': 'sqrt',\n",
              "  'min_samples_leaf': 3,\n",
              "  'min_samples_split': 2,\n",
              "  'n_estimators': 30},\n",
              " 'XGBClassifier': {'max_depth': 13,\n",
              "  'min_child_weight': 3,\n",
              "  'subsample': 0.9406880083709813}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tsh4A3eZtNDm",
        "colab_type": "code",
        "outputId": "ce2a2c8c-e6da-4b64-e765-8c1c6e23887b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "list(pipe1.named_steps.keys())[1],list(a8.keys())[0].lower()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('xgbclassifier', 'extratreesclassifier')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nG69v4Ox0gb",
        "colab_type": "code",
        "outputId": "63e30393-4758-4a64-a674-f16e53fe74c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for clf, label in zip([list(sclf2.named_classifiers.keys()),sclf2],[list(a8.keys())]):\n",
        "  print(label[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ExtraTreesClassifier\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWXLeaFQyxt4",
        "colab_type": "code",
        "outputId": "eae935de-ff35-4618-b642-82545e3d0fbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(type((sclf2.named_classifiers['pipeline-1'][1])).__name__)==list(a8.keys())[3]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVdjja7PwEPE",
        "colab_type": "code",
        "outputId": "795bd2eb-ac74-40a8-ea1e-6cfc96267d0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "for i in range(len(list(a8.keys()))):\n",
        " if (type((sclf2.named_classifiers['pipeline-1'][1])).__name__)==list(a8.keys())[i]:\n",
        "  pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier(**a8[(type((sclf2.named_classifiers['pipeline-1'][1])).__name__)]))\n",
        "  print(pipe1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pipeline(memory=None,\n",
            "         steps=[('catboostencoder',\n",
            "                 CatBoostEncoder(a=1, cols=None, drop_invariant=False,\n",
            "                                 handle_missing='value', handle_unknown='value',\n",
            "                                 random_state=None, return_df=True, sigma=None,\n",
            "                                 verbose=0)),\n",
            "                ('xgbclassifier',\n",
            "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
            "                               colsample_bylevel=1, colsample_bynode=1,\n",
            "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
            "                               max_delta_step=0, max_depth=13,\n",
            "                               min_child_weight=3, missing=None,\n",
            "                               n_estimators=100, n_jobs=1, nthread=None,\n",
            "                               objective='binary:logistic', random_state=0,\n",
            "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
            "                               seed=None, silent=None,\n",
            "                               subsample=0.9406880083709813, verbosity=1))],\n",
            "         verbose=False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyjoLr3s4OzU",
        "colab_type": "code",
        "outputId": "febab780-ac65-4655-aec1-4f82539af121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "a8"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ExtraTreesClassifier': {'criterion': 'gini',\n",
              "  'max_depth': 7,\n",
              "  'max_features': 7,\n",
              "  'min_samples_leaf': 4,\n",
              "  'min_samples_split': 4,\n",
              "  'n_estimators': 19},\n",
              " 'LGBMClassifier': {'colsample_bytree': 1.0,\n",
              "  'max_depth': 7,\n",
              "  'min_child_samples': 80,\n",
              "  'num_leave': 50,\n",
              "  'reg_alpha': 0,\n",
              "  'reg_lambda': 10,\n",
              "  'scale_pos_weight': 70.0,\n",
              "  'subsample': 1.0},\n",
              " 'RandomForestClassifier': {'criterion': 'entropy',\n",
              "  'max_depth': 5,\n",
              "  'max_features': 'sqrt',\n",
              "  'min_samples_leaf': 3,\n",
              "  'min_samples_split': 2,\n",
              "  'n_estimators': 30},\n",
              " 'XGBClassifier': {'max_depth': 13,\n",
              "  'min_child_weight': 3,\n",
              "  'subsample': 0.9406880083709813}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUpBLkPhbCLR",
        "colab_type": "code",
        "outputId": "f3e33a24-96e1-4621-dc1b-1f26ea2b0f45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "list(a8.keys())\n",
        "for clf, label in zip([list(sclf2.named_classifiers.keys()),sclf2],[list(a8.keys())]):\n",
        "   \n",
        "        print(clf)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['pipeline-1', 'pipeline-2', 'pipeline-3', 'pipeline-4', 'pipeline-5', 'pipeline-6']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yS_rBX750WcG",
        "colab_type": "code",
        "outputId": "684e1d1d-3603-4dcd-d668-e03588a3b3e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "for clf, label in zip([list(sclf2.named_classifiers.keys()),sclf2],[list(a8.keys())]):\n",
        "  for i in range(len(list(a8.keys()))):\n",
        "    if list(a8.keys())[i]  in label:\n",
        "      if (type((sclf2.named_classifiers[clf[0]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier(**a8[(type((sclf2.named_classifiers[clf[0]][1])).__name__)]))\n",
        "          pipe2\n",
        "      if (type((sclf2.named_classifiers[clf[1]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier(**a8[(type((sclf2.named_classifiers[clf[1]][1])).__name__)]))  \n",
        "          pipe2   \n",
        "      if (type((sclf2.named_classifiers[clf[2]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier(**a8[(type((sclf2.named_classifiers[clf[2]][1])).__name__)])) \n",
        "      if (type((sclf2.named_classifiers[clf[3]][1])).__name__)==list(a8.keys())[i]:\n",
        "          pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier(**a8[(type((sclf2.named_classifiers[clf[5]][1])).__name__)])) \n",
        "    else:  \n",
        "            pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "            pipe5 = make_pipeline(Accepted[4][1](),QuadraticDiscriminantAnalysis())\n",
        "sclf2 = StackingClassifier(classifiers=[pipe1,pipe2,pipe3,pipe4,pipe5,pipe6], meta_classifier=LogisticRegression())  \n",
        "model2=sclf2.fit(X_train, y_train)\n",
        "y_true3, y_pred3  =y_test, model2.predict(X_test)\n",
        "print(classification_report(y_true3, y_pred3))  \n",
        "print(\"StackingClassifier score: %f\" % model2.score(X_train, y_train))\n",
        "clf2_val_score = roc_auc_score(y_test, sclf2.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}'.format(clf2_val_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('catboostencoder',\n",
              "                 CatBoostEncoder(a=1, cols=None, drop_invariant=False,\n",
              "                                 handle_missing='value', handle_unknown='value',\n",
              "                                 random_state=None, return_df=True, sigma=None,\n",
              "                                 verbose=0)),\n",
              "                ('xgbclassifier',\n",
              "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
              "                               colsample_bylevel=1, colsample_bynode=1,\n",
              "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
              "                               max_delta_step=0, max_depth=13,\n",
              "                               min_child_weight=3, missing=None,\n",
              "                               n_estimators=100, n_jobs=1, nthread=None,\n",
              "                               objective='binary:logistic', random_state=0,\n",
              "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
              "                               seed=None, silent=None,\n",
              "                               subsample=0.9406880083709813, verbosity=1))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVFICfs6qliC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for clf, label in zip([list(sclf2.named_classifiers.keys()),sclf2],[list(a8.keys())]):\n",
        "  if label[0].lower()==list(pipe1.named_steps.keys())[1]:\n",
        "    print(label)\n",
        "\n",
        "pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier())\n",
        "\n",
        "pipe2 = make_pipeline(Accepted[1][1](),RandomForestClassifier())\n",
        "pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier())\n",
        "pipe4 = make_pipeline(Accepted[3][1](),LinearDiscriminantAnalysis())\n",
        "pipe5 = make_pipeline(Accepted[4][1](),QuadraticDiscriminantAnalysis())\n",
        "pipe6 = make_pipeline(Accepted[5][1](),lgb.LGBMClassifier())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_QG3jfWoLMr",
        "colab_type": "code",
        "outputId": "619dc299-79c4-4e32-ca6f-442e3dd47cc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier(**a8['XGBClassifier']))\n",
        "pipe1,a8['XGBClassifier']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Pipeline(memory=None,\n",
              "          steps=[('catboostencoder',\n",
              "                  CatBoostEncoder(a=1, cols=None, drop_invariant=False,\n",
              "                                  handle_missing='value', handle_unknown='value',\n",
              "                                  random_state=None, return_df=True, sigma=None,\n",
              "                                  verbose=0)),\n",
              "                 ('xgbclassifier',\n",
              "                  XGBClassifier(base_score=0.5, booster='gbtree',\n",
              "                                colsample_bylevel=1, colsample_bynode=1,\n",
              "                                colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
              "                                max_delta_step=0, max_depth=13,\n",
              "                                min_child_weight=3, missing=None,\n",
              "                                n_estimators=100, n_jobs=1, nthread=None,\n",
              "                                objective='binary:logistic', random_state=0,\n",
              "                                reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
              "                                seed=None, silent=None,\n",
              "                                subsample=0.9406880083709813, verbosity=1))],\n",
              "          verbose=False),\n",
              " {'max_depth': 13, 'min_child_weight': 3, 'subsample': 0.9406880083709813})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0IPp8evfQkz",
        "colab_type": "code",
        "outputId": "7ebcfbc6-2563-4608-f52b-0b7698809814",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "for i,k in enumerate(list(sclf2.named_classifiers.keys())):\n",
        "  for j,val in enumerate(list(a8.keys())):\n",
        "    if type((sclf2.named_classifiers[k][1])).__name__ ==list(a8.keys())[j]:\n",
        "      print(list(a8[val])[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_depth\n",
            "criterion\n",
            "criterion\n",
            "colsample_bytree\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oty2Q7TFlUNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sclf2.named_classifiers\n",
        "for i in range(len(list(a8.keys()))):\n",
        "   if list(pipe1.named_steps.keys())[1]==list(a8.keys())[i]:\n",
        "      print(a8.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiGbF94Mg6XY",
        "colab_type": "code",
        "outputId": "5aa1680c-f325-446f-a5be-4e03b7543f2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "sclf2.named_classifiers['pipeline-1'][1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.1, max_delta_step=0,\n",
              "              max_depth={'colsample_bytree': 1.0, 'max_depth': 7,\n",
              "                         'min_child_samples': 80, 'num_leave': 50,\n",
              "                         'reg_alpha': 0, 'reg_lambda': 10,\n",
              "                         'scale_pos_weight': 70.0, 'subsample': 1.0},\n",
              "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
              "              nthread=None, objective='binary:logistic', random_state=0,\n",
              "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "              silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaFe4_nXdj96",
        "colab_type": "code",
        "outputId": "a9616d42-3e4f-49b5-9a1d-07a9b904f5e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "pipe3 = make_pipeline(Accepted[2][1](),ExtraTreesClassifier(**a8['ExtraTreesClassifier'] )  )\n",
        "pipe3\n",
        "a8.keys(),list(sclf2.named_classifiers.keys())\n",
        "a8['XGBClassifier']\n",
        "pipe1 = make_pipeline(Accepted[0][1](),XGBClassifier(**a8['XGBClassifier']))\n",
        "pipe1,a8['XGBClassifier']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Pipeline(memory=None,\n",
              "          steps=[('catboostencoder',\n",
              "                  CatBoostEncoder(a=1, cols=None, drop_invariant=False,\n",
              "                                  handle_missing='value', handle_unknown='value',\n",
              "                                  random_state=None, return_df=True, sigma=None,\n",
              "                                  verbose=0)),\n",
              "                 ('xgbclassifier',\n",
              "                  XGBClassifier(base_score=0.5, booster='gbtree',\n",
              "                                colsample_bylevel=1, colsample_bynode=1,\n",
              "                                colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
              "                                max_delta_step=0, max_depth=13,\n",
              "                                min_child_weight=3, missing=None,\n",
              "                                n_estimators=100, n_jobs=1, nthread=None,\n",
              "                                objective='binary:logistic', random_state=0,\n",
              "                                reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
              "                                seed=None, silent=None,\n",
              "                                subsample=0.9406880083709813, verbosity=1))],\n",
              "          verbose=False),\n",
              " {'max_depth': 13, 'min_child_weight': 3, 'subsample': 0.9406880083709813})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHU7-dKPdJKv",
        "colab_type": "code",
        "outputId": "51ca7629-9338-4a86-add8-74e741515f96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "list(pipe1.named_steps.keys())[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'xgbclassifier'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVJ3yM5qcTeU",
        "colab_type": "code",
        "outputId": "312adf8d-3e45-426a-87d6-160f90293884",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "sclf2.named_classifiers['pipeline-1']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('catboostencoder',\n",
              "                 CatBoostEncoder(a=1, cols=None, drop_invariant=False,\n",
              "                                 handle_missing='value', handle_unknown='value',\n",
              "                                 random_state=None, return_df=True, sigma=None,\n",
              "                                 verbose=0)),\n",
              "                ('xgbclassifier',\n",
              "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
              "                               colsample_bylevel=1, colsample_bynode=1,\n",
              "                               colsample_bytree=1, gamma=0,\n",
              "                               learning_rate={'colsa...\n",
              "                               n_estimators={'criterion': 'entropy',\n",
              "                                             'max_depth': 5,\n",
              "                                             'max_features': 'sqrt',\n",
              "                                             'min_samples_leaf': 3,\n",
              "                                             'min_samples_split': 2,\n",
              "                                             'n_estimators': 30},\n",
              "                               n_jobs=1, nthread=None,\n",
              "                               objective='binary:logistic', random_state=0,\n",
              "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
              "                               seed=None, silent=None, subsample=1,\n",
              "                               verbosity={'max_depth': 13,\n",
              "                                          'min_child_weight': 3,\n",
              "                                          'subsample': 0.9406880083709813}))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yif1ZmBHWOHz",
        "colab_type": "code",
        "outputId": "c6ff0c91-1784-4fd6-b507-d136b2fc6f8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(sclf2.named_classifiers['pipeline-1'][1])\n",
        "type((sclf2.named_classifiers['pipeline-1'][1])).__name__ \n",
        "list(sclf2.named_classifiers.keys())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pipeline-1', 'pipeline-2']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5TMP6e7ab6X",
        "colab_type": "code",
        "outputId": "016882b2-f986-4157-8671-fc3dcbf786bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for i,k in enumerate(list(sclf2.named_classifiers.keys())):\n",
        "  for j,val in enumerate(list(a8.keys())):\n",
        "    if type((sclf2.named_classifiers[k][1])).__name__ ==list(a8.keys())[j]:\n",
        "        print(list(a8.values())[j])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'criterion': 'entropy', 'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 30}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQa1FPcy3ucp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "{'ExtraTreesClassifier': {'criterion': 'gini', 'max_depth': 7, 'max_features': 7, 'min_samples_leaf': 4, 'min_samples_split': 4, 'n_estimators': 19}, \n",
        "'LGBMClassifier': {'colsample_bytree': 1.0, 'max_depth': 7, 'min_child_samples': 80, 'num_leave': 50, 'reg_alpha': 0, 'reg_lambda': 10, 'scale_pos_weight': 70.0, 'subsample': 1.0},\n",
        "'RandomForestClassifier': {'criterion': 'entropy', 'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 30},\n",
        "'XGBClassifier': {'max_depth': 13, 'min_child_weight': 3, 'subsample': 0.9406880083709813}}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwGKr5nNdGcp",
        "colab_type": "code",
        "outputId": "a9f7cd5b-9183-42f2-8736-1a16defcbdbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from hyperopt import hp, tpe, fmin, Trials, STATUS_OK\n",
        "from sklearn import datasets\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble.forest import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline \n",
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "import numpy as np\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "param_hyperopt={\n",
        "  'RandomForestClassifier': \n",
        "            {\n",
        "            'max_depth': ho_scope.int(hp.quniform('c2_max_depth', 5, 7, 1)),\n",
        "            'n_estimators': ho_scope.int(hp.quniform('c2_n_estimators', 5, 30, 1)),\n",
        "            'max_features': hp.choice('c2_max_features', ['auto', 'sqrt']),\n",
        "            'criterion': hp.choice('c2_criterion', ['gini', 'entropy']),\n",
        "            'min_samples_split': ho_scope.int(hp.quniform('c2_min_samples_split', 2, 5,1)),\n",
        "            'min_samples_leaf': ho_scope.int(hp.quniform('c2_min_samples_leaf', 2, 5,1))\n",
        "                          \n",
        "         },\n",
        "  'ExtraTreesClassifier':\n",
        "            {\n",
        "                'max_depth': ho_scope.int(hp.quniform('c4_max_depth', 5, 20, 1)),\n",
        "                'n_estimators': ho_scope.int(hp.quniform('c4_n_estimators', 5, 30, 1)),\n",
        "                'max_features': ho_scope.int(hp.quniform('c4_max_features', 5, 20, 1)),\n",
        "                'criterion': hp.choice('c4_criterion', ['gini', 'entropy']),\n",
        "                'min_samples_split': ho_scope.int(hp.quniform('c4_min_samples_split', 2, 5,1)),\n",
        "                'min_samples_leaf': ho_scope.int(hp.quniform('c4_min_samples_leaf', 2, 5,1))\n",
        "       },\n",
        "  'XGBClassifier':\n",
        "      {\n",
        "           'max_depth': hp.choice(\"x_max_depth\", np.arange(5, 25, dtype=int)),\n",
        "           'min_child_weight': hp.choice ('x_min_child',np.arange(1, 10, dtype=int)),\n",
        "           'subsample': hp.uniform ('x_subsample', 0.8, 1)\n",
        "       },\n",
        "  'LGBMClassifier':\n",
        "       {\n",
        "        'max_depth': ho_scope.int(hp.quniform('c6_max_depth', 3, 10, 1)),\n",
        "        'num_leave': ho_scope.int(hp.quniform('c6_num_leaves', 5, 50, 5)),\n",
        "        'min_child_samples': ho_scope.int(hp.quniform('c6_min_child_samples', 50, 100, 10)),\n",
        "        'scale_pos_weight': ho_scope.float(hp.quniform('c6_scale_pos_weight', 50, 100, 10)),\n",
        "        'subsample': ho_scope.float(hp.quniform('c6_subsample', 0.6, 0.9, 1)),\n",
        "        'colsample_bytree': ho_scope.float(hp.quniform('c6_colsample_bytree', 0.6, 0.9, 1)),\n",
        "        'reg_lambda': hp.choice(\"c6_reg_lambda\",[0, 1e-1, 1, 5, 10, 20, 50]),\n",
        "        'reg_alpha': hp.choice(\"c6_reg_alpha\",[0, 1e-1, 1, 5, 10, 20, 50])\n",
        "       }         \n",
        "       \n",
        "}  \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "def f_clf8(params):\n",
        "   model1 =Pipeline(steps=[('encoder',Accepted[0][1]()),\n",
        "             ('c4',ExtraTreesClassifier(**f_unpack_dict(params['ExtraTreesClassifier'])))])\n",
        "   model2 =Pipeline(steps=[('encoder',Accepted[1][1]()),('LGBMClassifier', lgb.LGBMClassifier(**f_unpack_dict(params['LGBMClassifier'])))]) \n",
        "   model3 =Pipeline(steps=[('encoder',Accepted[2][1]()),('XGBClassifier', xgb.XGBClassifier(**f_unpack_dict(params['XGBClassifier'])))])   \n",
        "   model4 =Pipeline(steps=[('encoder',Accepted[3][1]()),('LinearDiscriminantAnalysis', LinearDiscriminantAnalysis())]) \n",
        "   model5 =Pipeline(steps=[('encoder',Accepted[4][1]()),\n",
        "             ('c2',RandomForestClassifier(**f_unpack_dict(params['RandomForestClassifier'])))]) \n",
        "   model6 =Pipeline(steps=[('encoder',Accepted[5][1]()),('QuadraticDiscriminantAnalysis', QuadraticDiscriminantAnalysis())])\n",
        "   sclf2 = StackingClassifier(classifiers=[model1,model2,model3,model4,model5,model6],meta_classifier=LogisticRegression())\n",
        "   return sclf2\n",
        "  \n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf8(params)\n",
        "    #clf = RandomForestClassifier(**params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf8 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                param_hyperopt, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "\n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval\n",
        "from hyperopt.pyll import scope as ho_scope\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.utils import shuffle\n",
        "clf8 = f_clf8(space_eval(param_hyperopt, best_clf8)).fit(X_train, y_train)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf8_val_score = roc_auc_score(y_test, clf8.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf8_val_score))\n",
        "print('Best parameters:')\n",
        "print(space_eval(param_hyperopt, best_clf8))\n",
        "oo=-trials.best_trial['result']['loss']\n",
        "oo"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 10%|█         | 1/10 [29:27<4:25:04, 1767.20s/it, best loss: -0.8759822037775029]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd05nQ3EojhD",
        "colab_type": "code",
        "outputId": "7a2da0a9-b32c-4347-b9e2-3466c7cbaff0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "from hyperopt import hp, tpe, fmin, Trials, STATUS_OK\n",
        "from sklearn import datasets\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble.forest import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline \n",
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from hyperopt.pyll import scope as ho_scope                                       \n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "import numpy as np\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from functools import partial\n",
        "\n",
        "param_hyperopt= {\n",
        "    'AdaBoostClassifier':\n",
        "     {\n",
        "          'learning_rate': hp.loguniform('c5_learning_rate', np.log(0.01), np.log(1)),\n",
        "           'n_estimators': ho_scope.int(hp.quniform('c5_n_estimators', 5, 30, 1)),\n",
        "           'algorithm': hp.choice('c5_algorithm',[\"SAMME\"])  \n",
        "       }    \n",
        "} \n",
        "   \n",
        "def f_unpack_dict(dct): \n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "            \n",
        "    return res\n",
        "\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.2, random_state = 0)\n",
        "\n",
        "def f_clf7(params):\n",
        "   model2 =Pipeline(steps=[('encoder',Accepted[5][1]()),\n",
        "             ('c5',AdaBoostClassifier(**f_unpack_dict(params['AdaBoostClassifier'])))]) \n",
        "   return model2\n",
        "        \n",
        "\n",
        "def objective_function(params,X_train2, y_train2):\n",
        "    model=f_clf7(params)\n",
        "    #clf = RandomForestClassifier(**params)\n",
        "    score = cross_val_score(model, X_train2, y_train2, cv=5,scoring='roc_auc', n_jobs=-1).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}  \n",
        "\n",
        "trials = Trials()\n",
        "best_clf7 = fmin(partial(objective_function, X_train2=X_train2, y_train2=y_train2),\n",
        "                 param_hyperopt, algo=tpe.suggest, max_evals=10,trials=trials, rstate=np.random.RandomState(1))\n",
        "\n",
        "from hyperopt import fmin, hp, tpe, Trials, space_eval\n",
        "from hyperopt.pyll import scope as ho_scope\n",
        "from hyperopt.pyll.stochastic import sample as ho_sample\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.utils import shuffle\n",
        "clf7 = f_clf7(space_eval(param_hyperopt, best_clf7)).fit(X_train2, y_train2)\n",
        "\n",
        "# Calculating performance on validation set\n",
        "clf7_val_score = roc_auc_score(y_test, clf7.predict_proba(X_test)[:, 1])\n",
        "print('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n",
        "      format(-trials.best_trial['result']['loss'], clf7_val_score))\n",
        "print('Best parameters:')\n",
        "print(space_eval(param_hyperopt, best_clf7))\n",
        "cc=-trials.best_trial['result']['loss']\n",
        "cc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [07:45<00:00, 43.23s/it, best loss: -0.7034403411046589]\n",
            "Cross-val score: 0.70344; validation score: 0.70367\n",
            "Best parameters:\n",
            "{'AdaBoostClassifier': {'algorithm': 'SAMME', 'learning_rate': 0.30044074314959096, 'n_estimators': 13}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7034403411046589"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTiYkemagMPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install hyperopt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfP43S4Ohohx",
        "colab_type": "code",
        "outputId": "2be333cf-8cd0-4328-c9f5-a54e3d3a5410",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Compare Algorithms\n",
        "import pandas\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis,QuadraticDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble.forest import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline \n",
        "from sklearn.preprocessing import scale, normalize\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\n",
        "import xgboost\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "# load dataset\n",
        "numeric_features = df6_pd.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df6_pd.select_dtypes(include=['object']).columns\n",
        "X2 = df6_pd.drop('Delay', axis=1)\n",
        "y2 = df6_pd['Delay']\n",
        "# Do the train test split\n",
        "#le = preprocessing.LabelEncoder()\n",
        "#label_encoder = le.fit(y)\n",
        "#y = label_encoder.transform(y)\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2)\n",
        "block_size=int(round(X_train2.shape[0]/2))\n",
        "n = block_size  #chunk row size\n",
        "list_X_train = [X_train2[i:i+n] for i in range(0,X_train2.shape[0],n)]\n",
        "list_y_train = [y_train2[i:i+n] for i in range(0,y_train2.shape[0],n)]\n",
        "list_X_test = [X_test2[i:i+n] for i in range(0,X_test2.shape[0],n)]\n",
        "list_y_test = [y_test2[i:i+n] for i in range(0,y_test2.shape[0],n)]\n",
        "# Build the encoder\n",
        "\n",
        "# prepare configuration for cross validation test harness\n",
        "seed = 7\n",
        "# prepare models\n",
        "models = []\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                     ('LDA', LinearDiscriminantAnalysis())])).fit(list_X_train[0], list_y_train[0]))\n",
        "models.append((Pipeline(steps=[('encoder',ce.james_stein.JamesSteinEncoder()),\n",
        "                     ('MLPClassifier',\n",
        "                      MLPClassifier(hidden_layer_sizes=(10,10), max_iter=5, alpha=0.0001,\n",
        "                     solver='sgd', verbose=10,  random_state=1))])).fit(list_X_train[1], list_y_train[1]))\n",
        "\n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "scoring = 'roc_auc'\n",
        "for name, model in enumerate(models):\n",
        "    #kfold = model_selection.StratifiedKFold(n_splits=5, random_state=1)\n",
        "    print('1')\n",
        "    cv_results = model_selection.cross_val_score(model, list_X_train[name],  list_y_train[name], cv=5, scoring=scoring)\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "    print(msg)\n",
        "# boxplot algorithm comparison\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Algorithm Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 112.60309919\n",
            "Iteration 3, loss = 112.60068321\n",
            "Iteration 4, loss = 112.59827204\n",
            "Iteration 5, loss = 112.59586177\n",
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0: 0.827382 (0.001427)\n",
            "1\n",
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 181.08164638\n",
            "Iteration 3, loss = 181.07853945\n",
            "Iteration 4, loss = 181.07543119\n",
            "Iteration 5, loss = 181.07232161\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 118.73212266\n",
            "Iteration 3, loss = 118.73009402\n",
            "Iteration 4, loss = 118.72805917\n",
            "Iteration 5, loss = 118.72602306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 153.40265149\n",
            "Iteration 3, loss = 153.40002376\n",
            "Iteration 4, loss = 153.39739347\n",
            "Iteration 5, loss = 153.39475892\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 120.69334431\n",
            "Iteration 3, loss = 120.69127561\n",
            "Iteration 4, loss = 120.68921110\n",
            "Iteration 5, loss = 120.68714254\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 190.89814426\n",
            "Iteration 3, loss = 190.89486761\n",
            "Iteration 4, loss = 190.89159044\n",
            "Iteration 5, loss = 190.88831180\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1: 0.500000 (0.000000)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEVCAYAAADuAi4fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}